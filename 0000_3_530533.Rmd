# 530_533 

https://www.ssc.wisc.edu/sscc/pubs/RegressionDiagnostics.html

http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/


## Definition of the general linear model 

$$Y=X\beta+\varepsilon$$

$$\begin{bmatrix}
Y_1 \\
Y_2  \\
...\\
Y_n \end{bmatrix}=\begin{bmatrix}
1 & x_{11} & x_{12} & x_{13} & ... & x_{1m}\\
1 & x_{21} & x_{22} & x_{23} & ... & x_{2m} \\
...\\
1 & x_{n1} & x_{n2} & x_{n3} & ... & x_{nm}
\end{bmatrix} \begin{bmatrix}
\beta_0 \\
\beta_1  \\
...\\
\beta_m \end{bmatrix}+\begin{bmatrix}
\varepsilon_1 \\ \varepsilon_2 \\
...\\
\varepsilon_n  \end{bmatrix}$$

Where,

$Y$: Response vector

$X$: Design matrix

$\beta$: parameter vector

$\varepsilon$: error vector

If $\varepsilon$ follows a mutivariate normal distribution then we will be under the General Linear Model (GLM) framework. 

$$\varepsilon \sim N(0, \sigma^2I_{x \times n})$$

If $X$ is continuous we have regression.

If $X$ is categorical we have ANOVA.

If $X$ is a mix of both, we have ANCOVA.




## Simple Linear Regression

Simple linear regression is a linear regression model with a single explanatory variable. In addition, we typically assume that this is under the GLM framework and thus we also assume that the residuals follow normal distribution. 


### Least squares and propoerties of the regression parameters 

#### Basic idea

__Of note__ The following method can always calculate the vector of $\beta$, not related to any specific methods.

$$Y_{n \times 1}=X_{n\times m}\beta_{m \times 1}$$

$\rightarrow$

$$[X^T]_{m \times n}Y_{n \times 1}=[X^T]_{m \times n}X_{n\times m}\beta_{m \times 1}$$
$\rightarrow$

$$[X^TX]^{-1}_{m\times m}[X^TY]_{m \times 1}=\beta_{m \times 1}$$

$\rightarrow$

$$\beta_{m \times 1}=[(X^TX)^{-1} \cdot (X^TY)]_{m \times 1}$$

#### Least Squares

Assume the following model:

$$Y=X\beta +\varepsilon$$

When $\beta$ only has a dimension of $2 \times 1$, we can write it as follows. 

$$Y_i=\beta_0+\beta_1X_i +\varepsilon$$




Thus,

$$Q=\sum_{i=1}^n (Y_i-\beta_0-\beta_1X_i)^2$$
We can calculate the partial derivates as follows.

$$\frac{\partial}{\partial \beta_0} Q \rightarrow \sum_{i=1}^n [Y_i-\beta_0-\beta_1X_i]=0$$

$$\frac{\partial}{\partial \beta_1} Q \rightarrow \sum_{i=1}^n [Y_i-\beta_0-\beta_1X_i]X_i=0$$

__Combining the two pieces of information, we can get__

$$b_1=\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}$$

$$b_0=\frac{1}{n}(\sum Y_i-b_1\sum X_i)=\bar{Y}-b_1\bar{X}$$

__Properties of Least Squares Estimators (Gauss- Markov theorem):__

_P.18_

Under the conditions of regression model shown above, the least squares estimator $b_0$ and $b_1$ are unbiased and have minimum variance among all unbiased linear estimators.

That is,

$$E(b_0)=\beta_0$$

$$E(b_1)=\beta_1$$

__Point Estimation of Mean Response:__

Given sample estimators $b_0$ and $b_1$ in the regression function 

$$E\{Y\}=\beta_0+\beta_1X$$
We estimate the regression function as follows:

$$\hat{Y}=b_0+b_1X$$

We call a value of the response variable a response and $E\{Y\}$ the _mean response_.


__Propoerties of fitted regression line__

_P.22_

The resitual:

$$e_i=Y_i-\hat{Y_i}$$
The sum of the residuals is zero

$$\sum_{i=1}^n e_i=0$$


#### Point estimator of $\sigma^2$

Note that, there are two $S^2$ below, and they have different formulus, even though the only difference is the degree of freedom and the logic of calculating df is the same across these two cases (i.e., __Single Population vs. Regression Model__ ).

__1. Single Pupulation__

__Sum of squares:__ 

_P.25_

$$\sum_{i=1}^ne_i^2=\sum_{i=1}^n (Y_i-\bar{Y})^2$$

The sum of squares is then divided by the degrees of freedom associated with it. 



$$S^2= \frac{\sum_{i=1}^n (Y_i-\bar{Y})^2}{n-1}$$

$S^2$ is an unbiased estimator of the variance $\sigma^2$. The sample variance $S^2$ is often called a mean square, because a sum of squares has been divided by the appropriate number of degrees of freedom.

$$E(S^2)=\sigma^2$$


__2. Regression model__


__Sum of Squares Error (SSE) (or, Sum of Squares Residual): __

$$SSE=\sum_{i=1}^n (Y_i-\hat{Y_i})^2=\sum_{i=1}^n e_i^2$$


__Mean Square Error (MSE) (or, Mean Square Residual):__

$$S^2=MSE=\frac{SSE}{n-2}=\frac{\sum(Y_i-\hat{Y_i})^2}{n-2}=\frac{\sum e_i^2}{n-2}$$
$MSE$ is an unbiased estimator of $\sigma^2$ for regression model.

$$E(MSE=S^2)=\sigma^2$$



### MLE

_P.30_

Note that the __Normal Error Regression Model__ is as follows.

$$Y_i=\beta_0+\beta_1X_i +\varepsilon$$
(This is a special case of the definition provided earlier, i.e., $Y=X\beta +\varepsilon$)

For this model, each $Y_i$ observation is normally distributed with mean $\beta_0+\beta_1x_i$ and a standard devision $\sigma$.

Givern $Y_i$ follows normal distribution, we can use $pdf$ of normal distributions and MLE to estimate parameters.

$$\begin{aligned} L(\beta_0,\beta_1,\sigma^2) &=\prod_{i=1}^n \frac{1}{(2\pi \sigma^2)^{1/2}}exp[-\frac{1}{2 \sigma^2}(Y_i-\beta_0-\beta_1X_i)^2] \\ &=\frac{1}{(2\pi \sigma^2)^{n/2}}exp[-\frac{1}{2 \sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2]  \end{aligned}$$
Since the variance $\sigma^2$is usually unknown, the likelihood fucntion is a function of three parameters, $\beta_0$, $\beta_1$, and $\sigma^2$. 

Thus, we can calculate $\beta_0$, $\beta_1$, and $\sigma^2$ analytically, and the results of $\beta_0$ and $\beta_1$ are the same as least squares estimators (see above).

The variance $\sigma^2$ is as follows.

$$\hat{\sigma}^2=\frac{\sum(Y_i-\hat{Y_i})^2}{n}$$

As noted in the least square estimation, we know that $\hat{\sigma}^2$ is biased. Thus, we know that:

$$S^2=MSE=\frac{n}{n-2}\hat{\sigma}^2$$

## Full rank, less than full rank 

http://www.biostat.jhsph.edu/~iruczins/teaching/140.751/notes/ch7.pdf

$$Y_{n \times 1}=X_{n\times m}\beta_{m \times 1}$$

If the rank $r$ of $X_{n \times m}$ is smaller than $m$, i.e., $r<m$, there is not a unique solution $\hat{\beta}$. We have three ways to find a solution $\hat{\beta}$ and the orthogonal projection $\hat{Y}$:

__1. Reducing the model to the full rank.__


Let $X_1$ consist of $r$ linear independent columns from $X$ and let $X_2$ consistn of the remaining columns. Then, $X_2=X_1F$ because the columns of $X_2$ are linearly dependent on the columns of $X_1$.

$$X=(X_1, X_2)=(X_1, X_1F)=X_1[I_{r\times r}, F]$$
This is a special case of the factorization $X=KL$, where rank $(K_{n \times r})=r$ and rank $(L_{r\times p})=r$. 

$$E[Y]=x\beta=KL\beta=k\alpha$$
Since $K$ has full rank, the Least Squares Estimate of $\alpha$ is $\hat{\alpha}=(K^T K)^{-1} \cdot K^TY$. 

The orthogonal project,

$$\hat{Y}=K\cdot \alpha=K \cdot(K^T K)^{-1} \cdot K^TY=X_1 \cdot(X_1^TX_1)^{-1}\cdot X_1^TY$$

$$\begin{bmatrix}
Y_{11} \\ ... \\ Y_{1n_1} \\
Y_{21}  \\ ... \\ Y_{2n_2} \end{bmatrix}=
\begin{bmatrix}
1 & 1 & 0\\
... & ... & ... \\
1 & 1 & 0 \\
1 & 0 & 1 \\ 
... & ... & ... \\
1 & 0 &1 \end{bmatrix} 
\begin{bmatrix}
\mu \\
\beta_1  \\
\beta_2\\
\end{bmatrix}+\begin{bmatrix}
\varepsilon_{11} \\ ... \\ \varepsilon_{1n_1} \\
\varepsilon_{21}  \\ ... \\ \varepsilon_{2n_2}  \end{bmatrix}$$


Let $X_1$ consist of the first 2 columns of $X$, then

$$X=X_1 \begin{bmatrix}
1 & 0 & 1 \\ 0 & 1 & -1 \end{bmatrix}$$


Thus,

$$X_1=K=\begin{bmatrix}
1 & 1 \\
... & ...  \\
1 & 1 \\
1 & 0  \\ 
... & ... \\
1 & 0  \end{bmatrix} $$


$$\hat{\alpha}=(K^T K)^{-1} \cdot K^TY=\begin{bmatrix}
n & n_1 \\
n_1 & n_1 \end{bmatrix}^{-1} \begin{bmatrix}
\sum Y_{1j}+\sum y_{2j} \\
\sum y_{1j} \end{bmatrix} =\begin{bmatrix}
\bar{Y_2}\\ \bar{Y_1}-\bar{Y_2} \end{bmatrix}$$

$$\hat{Y}=X_1 \hat{\alpha}=\begin{bmatrix}
1 & 1 \\
... & ...  \\
1 & 1 \\
1 & 0  \\ 
... & ... \\
1 & 0  \end{bmatrix} \begin{bmatrix}
\bar{Y_2}\\ \bar{Y_1}-\bar{Y_2} \end{bmatrix}=\begin{bmatrix}
\bar{Y_1}  \\
...   \\
\bar{Y_1} \\
\bar{Y_2}   \\ 
... \\
\bar{Y_2} \end{bmatrix}$$

__2. Find the generalized inverse $(X^TX)^{-1}$. __

As noted above, when $X^TX$ has a full rank, we can directly calculate the inverse of $X^TX$. That is,


$$\beta_{m \times 1}=[(X^TX)^{-1} \cdot (X^TY)]_{m \times 1}$$

We can just find __some columns__ within $X$ that are independent, and then calculate the inverse of it.

This is because if a matrix $W$ with a rank $r$ and can be partitioned as follows.

$$W=\begin{bmatrix}
A & B  \\
C & D \end{bmatrix}$$

Assume $A$ has rank $r$, then

$$W^{-1}=\begin{bmatrix}
A^{-1} &0  \\
0 & 0 \end{bmatrix}$$

Thus, let $X=(X_1, X_2)$, where $X_1$ consists of $r$ linearly independent columns from $X$. Then a generalized inverse of $X^TX$ is

$$[X^TX]^{-1}=\begin{bmatrix}
[X_1^TX_1]^{-1}&0  \\
0 & 0 \end{bmatrix}$$

Thus, all other steps are similar to the full rank case. 

__3. Impose identififiability constraints__

(Not very sure this one.)


## Assumptions, checking assumptions

__The following are some of the possible violations of assumptions:__

### Regression function is nonlinear__

Plot explanatory and response variables to see whether their relationship is linear, it is not, suggesting that a linear regression function is not appropriate. 

__Stat Methods:__

F-test for lack of fit. 

### Non-constant variance__

Plots of the residuals against the predictor variable (i.e., $X$) or against the fitted values (i.e., $\hat{Y}$), to study whether the linear model is appropriate and whether the __variance of the error terms__ is constant.  

__Stat Methods:__

__(1) Modified Levene's test (Brown-Forsythe test):__

To check and see if the variability for $Y$ for the smaller $X's$ are different than the vairability of $Y$ for the larger $X's$.

Thus, we can break up the data into two groups based $X's$ values. Then, we test to see if the vairability of these two groups significantly differ. 

__Note that__ Modefied Levene's test does not depend on normality of the error terms. That is, this test is robust against serious departures from normality. 


__(2) Breusch-Pagan test:__

Want to see if there is any relationship between $\sigma_i$ and the $X_i$'s. To do this, we can fit a regression of $log(\sigma_i)$ on $X_i$'s:

$$log(\sigma_i)=b_0+b_1X_i$$


### Error terms not independent__

We want a random pattern. Any type of pattern indicates time orderred problems indicates that the model is not appropriate. 

__Stat Methods:__

__Durbin Watson Test__ Test for presence of autocorrelation amongst observations. 

### Possibility of outliers__

Scatter plot the dependent variable and independent variables (each $X_i$ is plotted seprately.)


### Non normal distribution of error__

Use histogram to check whether the resitual follows normal distribution. 

__Stat Methods:__

_P.115_

Correlation between ordered residuals and their expected values under normality. 


_P.70 512 note_

You can use Shapiro-Wilk, Kolmogorov-Smirnov, Cramer-von Mises, Anderson-Darling in SAS to assess the normal errors. 

### Omission of some of the important predictors__

Plots should be made betweeen variables omitted from the model and the dependent variable. Such omitted variables may have important effects on the response. 



## Bootstrapping used in linear models

_P.458_

__Intro__

For standard fitted regression models, methods described earlier chapters are available for evaluating the precision of estimated regression coefficients, fitted values, and predictions of new observations.

However, in many nonstandard situations, such as when nonconstant error variance are estimated by iteratively reweighted least sqaure or when robust regression estimation is used, standard methods for evaluating the precision may not be available or may only approximately applicable when the sample size is large. 

Bootstrapping was developed by Efron to provide estimates of the precision of sample estimates for these complex cases.

__Conceptual__

Suppose that we have fitted a regression model (simple or multiple) by some procedure and get the coefficient $b_1$. We now wish to evaluate the precision of this esimtate by bootsrap method. 

(1) In essensce, the bootstrap method call for the selection from the observed sample data of a random sample of size $n$ with replacement. 

(2) Next, the bootstrap method calculates the estimated regression coefficient from the boostrap sample, using the same fitting procedure as employed for the original fitting. This leads to the first bootstrap estimate $b_1^*$.

This procedure repeated a large number of times, each time a bootstrap sample of size $n$ is selected with replacement from the original sample and the estimated coefficient is obtained.

(3) The estimated standard deviation of all the bootstrap estimates $b_1^*$ denoted by $S^*\{b_1^*\}$, is an estimate of the variability of the sampling distribution of $b_1$ and therefore a measure of the prevision of $b_1$.

__Some Math__

Bootstrapping sampling for regression can be done in two basic ways.

(1) Whne the regression fucntion being fitted is a good model for the data, the error terms have constant variance, and the predictor variables can be regarded as fixed, fixed X sampling is appropriate.

Here, the resitudals $e_i$ from the original fitting are regarded as the sample data to be sampled with replacement. After bootstrap sample of the resituals of size n have beeb obtained, denoted by $e_1^*, ..., e_n^*$. The bootstrap sample residuals are added to the fitted values from the original fitting to obtain nuew bootstrap Y values, denoted by $Y_1^*,...,Y_n^*$:

$$Y_i^*=\hat{Y_i}+e_i^*$$
These bootstrap $Y^*$ values are then regressed on the original $X$ variables by the same procedure used initially to obtain the bootstrap estimate $b_1^*$.


(2) When there is some doubt about the adequacy of the regressio nmodel, or the rror variances are not constant, or the predictor variables can not be regarded as fixed, random X sampling is appropriate.

For simple regression, the pair $X$ and $Y$ data in the oringal sample are considered to be the data to be sampled with replacedment. This this second procedure samples cases with replacement $n
 times, yelding a bootstrap sample of $n$ pairs of ($X^*,Y^*$) values. This bootstrap sample is then used for obtaining the bootstrap estimate $b_1$, as with fixed $X$ sampling.
 
 
 __Bootstrap confidence intervals__
 
 _P.460_
 

## Generalized linear models


### Definition, similarities and differences from general linear models

Generalized Linear Model (GLiM) loosens this assumption that $\varepsilon$ follows a mutivariate normal distribution, and allows for a variety of other distributions from the exponential family for the residuals.

Of note, the GLM is a special case of the GLiM in which the distribution of the residuals follow a conditionally normal distribution.


### Advantage and disadvantages

### logistic and poisson regression 

__Logistic regression__

Logistic regression is a special case of GLiM with binomial distribution on the $Y's$ and the link function


The basic idea of logistic regression:
$$p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}$$

Thus, $\beta_0+\beta_1x_1+...+\beta_nx_n$ can be from $-\infty$ to $+\infty$, and $p(y=1)$ will be always within the range of $(0,1)$.

```{R}
f<-function(x){exp(x)/(1+exp(x))}
data<-seq(-10,10,1)
plot(data,f(data),type = "b")
  
```

We can also write the function into another format as follows:
$$log \frac{p(y=1)}{1-p(y=1)}= \beta_0+\beta_1x_1+...+\beta_nx_n$$
Thus, we know that the regression coeficients of $\beta_i$ actually change the "log-odds" of the event. Of course, note that the magnitude of $\beta_i$ is dependent upon the units of $x_i$.
