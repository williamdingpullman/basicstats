# 530_533 

https://www.ssc.wisc.edu/sscc/pubs/RegressionDiagnostics.html

http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/


## Definition of the general linear model 

$$Y=X\beta+\varepsilon$$

$$\begin{bmatrix}
Y_1 \\
Y_2  \\
...\\
Y_n \end{bmatrix}=\begin{bmatrix}
1 & x_{11} & x_{12} & x_{13} & ... & x_{1m}\\
1 & x_{21} & x_{22} & x_{23} & ... & x_{2m} \\
...\\
1 & x_{n1} & x_{n2} & x_{n3} & ... & x_{nm}
\end{bmatrix} \begin{bmatrix}
\beta_0 \\
\beta_1  \\
...\\
\beta_m \end{bmatrix}+\begin{bmatrix}
\varepsilon_1 \\ \varepsilon_2 \\
...\\
\varepsilon_n  \end{bmatrix}$$

Where,

$Y$: Response vector

$X$: Design matrix

$\beta$: parameter vector

$\varepsilon$: error vector

If $\varepsilon$ follows a mutivariate normal distribution then we will be under the General Linear Model (GLM) framework. 

$$\varepsilon \sim N(0, \sigma^2I_{x \times n})$$

If $X$ is continuous we have regression.

If $X$ is categorical we have ANOVA.

If $X$ is a mix of both, we have ANCOVA.




## Simple Linear Regression

Simple linear regression is a linear regression model with a single explanatory variable. In addition, we typically assume that this is under the GLM framework and thus we also assume that the residuals follow normal distribution. 


### Least squares vs. MLE vs, propoerties of the regression parameters 

#### Basic idea

__Of note__

The following method can always calculate the vector of $\beta$, not related to any specific methods.

$$Y_{n \times 1}=X_{n\times m}\beta_{m \times 1}$$

$\rightarrow$

$$[X^T]_{m \times n}Y_{n \times 1}=[X^T]_{m \times n}X_{n\times m}\beta_{m \times 1}$$
$\rightarrow$

$$[X^TX]^{-1}_{m\times m}[X^TY]_{m \times 1}=\beta_{m \times 1}$$

$\rightarrow$

$$\beta_{m \times 1}=[(X^TX)^{-1} \cdot (X^TY)]_{m \times 1}$$
#### Least Squares

Assume the following model:

$$Y=X\beta +\varepsilon$$

When $\beta$ only has a dimension of $2 \times 1$, can can write it as follows. 

$$Y_i=\beta_0+\beta_1X_i +\varepsilon$$




Thus,

$$Q=\sum_{i=1}^n (Y_i-\beta_0-\beta_1X_i)^2$$
We can calculate the partial derivates as follows.

$$\frac{\partial}{\partial \beta_0} Q \rightarrow \sum_{i=1}^n [Y_i-\beta_0-\beta_1X_i]=0$$

$$\frac{\partial}{\partial \beta_1} Q \rightarrow \sum_{i=1}^n [Y_i-\beta_0-\beta_1X_i]X_i=0$$

__Combining the two pieces of information, we can get__

$$b_1=\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}$$

$$b_0=\frac{1}{n}(\sum Y_i-b_1\sum X_i)=\bar{Y}-b_1\bar{X}$$

__Properties of Least Squares Estimators (Gauss- Markov theorem)__

_P.18_

Under the conditions of regression model shown above, the least squares estimator $b_0$ and $b_1$ are unbiased and have minimum variance among all unbiased linear estimators.

That is,

$$E(b_0)=\beta_0$$

$$E(b_1)=\beta_1$$


#### MLE

_P.30_

Note that the __Normal Error Regression Model__ is as follows.

$$Y_i=\beta_0+\beta_1X_i +\varepsilon$$
(This is a specific eample of the definition provided earlier, i.e., $Y=X\beta +\varepsilon$)

For this model, each $Y_i$ observation is normally distributed with mean $\beta_0+\beta_1x_i$ and a standard devision $\sigma$.

Givern $Y_i$ follows normal distribution, we can use $pdf$ of normal distributions and MLE to estimate parameters.

$$\begin{aligned} L(\beta_0,\beta_1,\sigma^2) &=\prod_{i=1}^n \frac{1}{(2\pi \sigma^2)^{1/2}}exp[-\frac{1}{2 \sigma^2}(Y_i-\beta_0-\beta_1X_i)^2] \\ &=\frac{1}{(2\pi \sigma^2)^{n/2}}exp[-\frac{1}{2 \sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2]  \end{aligned}$$
Since the variance $\sigma^2$is usually unknown, the likelihood fucntion is a function of three parameters, $\beta_0$, $\beta_1$, and $\sigma^2$.

We can calculate $\beta_0$, $\beta_1$, and $\sigma^2$ analytically, and the results of $\beta_0$ and $\beta_1$ are the same as least squares estimators (see above).

The variance $\sigma^2$ is as follows.

$$\hat{\sigma}^2=\frac{\sum(Y_i-\hat{Y_i})^2}{n}$$

## full rank, less than full rank 

http://www.biostat.jhsph.edu/~iruczins/teaching/140.751/notes/ch7.pdf

$$Y_{n \times 1}=X_{n\times m}\beta_{m \times 1}$$

If the rank $r$ of $X_{n \times m}$ is smaller than $m$, i.e., $r<m$, there is not a unique solution $\hat{\beta}$. We have three ways to find a solution $\hat{\beta}$ and the orthogonal projection $\hat{Y}$:

__1. Reducing the model to the full rank.__


Let $X_1$ consist of $r$ linear independent columns from $X$ and let $X_2$ consistn of the remaining columns. Then, $X_2=X_1F$ because the columns of $X_2$ are linearly dependent on the columns of $X_1$.

$$X=(X_1, X_2)=(X_1, X_1F)=X_1[I_{r\times r}, F]$$
This is a special case of the factorization $X=KL$, where rank $(K_{n \times r})=r$ and rank $(L_{r\times p})=r$. 

$$E[Y]=x\beta=KL\beta=k\alpha$$
Since $K$ has full rank, the Least Squares Estimate of $\alpha$ is $\hat{\alpha}=(K^T K)^{-1} \cdot K^TY$. 

The orthogonal project,

$$\hat{Y}=K\cdot \alpha=K \cdot(K^T K)^{-1} \cdot K^TY=X_1 \cdot(X_1^TX_1)^{-1}\cdot X_1^TY$$

$$\begin{bmatrix}
Y_{11} \\ ... \\ Y_{1n_1} \\
Y_{21}  \\ ... \\ Y_{2n_2} \end{bmatrix}=
\begin{bmatrix}
1 & 1 & 0\\
... & ... & ... \\
1 & 1 & 0 \\
1 & 0 & 1 \\ 
... & ... & ... \\
1 & 0 &1 \end{bmatrix} 
\begin{bmatrix}
\mu \\
\beta_1  \\
\beta_2\\
\end{bmatrix}+\begin{bmatrix}
\varepsilon_{11} \\ ... \\ \varepsilon_{1n_1} \\
\varepsilon_{21}  \\ ... \\ \varepsilon_{2n_2}  \end{bmatrix}$$


Let $X_1$ consist of the first 2 columns of $X$, then

$$X=X_1 \begin{bmatrix}
1 & 0 & 1 \\ 0 & 1 & -1 \end{bmatrix}$$


Thus,

$$X_1=K=\begin{bmatrix}
1 & 1 \\
... & ...  \\
1 & 1 \\
1 & 0  \\ 
... & ... \\
1 & 0  \end{bmatrix} $$


$$\hat{\alpha}=(K^T K)^{-1} \cdot K^TY=\begin{bmatrix}
n & n_1 \\
n_1 & n_1 \end{bmatrix}^{-1} \begin{bmatrix}
\sum Y_{1j}+\sum y_{2j} \\
\sum y_{1j} \end{bmatrix} =\begin{bmatrix}
\bar{Y_2}\\ \bar{Y_1}-\bar{Y_2} \end{bmatrix}$$

$$\hat{Y}=X_1 \hat{\alpha}=\begin{bmatrix}
1 & 1 \\
... & ...  \\
1 & 1 \\
1 & 0  \\ 
... & ... \\
1 & 0  \end{bmatrix} \begin{bmatrix}
\bar{Y_2}\\ \bar{Y_1}-\bar{Y_2} \end{bmatrix}=\begin{bmatrix}
\bar{Y_1}  \\
...   \\
\bar{Y_1} \\
\bar{Y_2}   \\ 
... \\
\bar{Y_2} \end{bmatrix}$$

__2. Find the generalized inverse $(X^TX)^{-}$. __

As noted above, when $X^TX$ has a full rank, we can directly calculate the inverse of $X^TX$. That is,


$$\beta_{m \times 1}=[(X^TX)^{-1} \cdot (X^TY)]_{m \times 1}$$

We can just find __some columns__ within $X$ that are independent, and then calculate the inverse of it.

This is because if a matrix $W$ with a rank $r$ and can be partitioned as follows.

$$W=\begin{bmatrix}
A & B  \\
C & D \end{bmatrix}$$

Assume $A$ has rank $r$, then

$$W^{-1}=\begin{bmatrix}
A^{-1} &0  \\
0 & 0 \end{bmatrix}$$

Thus, let $X=(X_1, X_2)$, where $X_1$ consists of $r$ linearly independent columns from $X$. Then a generalized inverse of $X^TX$ is

$$[X^TX]^{-1}=\begin{bmatrix}
[X_1^TX_1]^{-1}&0  \\
0 & 0 \end{bmatrix}$$

Thus, all other steps are similar to the full rank case. 

__3. Impose identififiability constraints__

(Not very sure this one.)


### Assumptions, checking assumptions

## Bootstrapping used in linear models


## Generalized linear models


### Definition, similarities and differences from general linear models

Generalized Linear Model (GLiM) loosens this assumption that $\varepsilon$ follows a mutivariate normal distribution, and allows for a variety of other distributions from the exponential family for the residuals.

Of note, the GLM is a special case of the GLiM in which the distribution of the residuals follow a conditionally normal distribution.


### Advantage and disadvantages

### logistic and poisson regression 
