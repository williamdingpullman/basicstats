# 530_533 

https://www.ssc.wisc.edu/sscc/pubs/RegressionDiagnostics.html

http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/


## Definition of the general linear model 

$$Y=X\beta+\varepsilon$$

$$\begin{bmatrix}
Y_1 \\
Y_2  \\
...\\
Y_n \end{bmatrix}=\begin{bmatrix}
1 & x_{11} & x_{12} & x_{13} & ... & x_{1m}\\
1 & x_{21} & x_{22} & x_{23} & ... & x_{2m} \\
...\\
1 & x_{n1} & x_{n2} & x_{n3} & ... & x_{nm}
\end{bmatrix} \begin{bmatrix}
\beta_0 \\
\beta_1  \\
...\\
\beta_m \end{bmatrix}+\begin{bmatrix}
\varepsilon_1 \\ \varepsilon_2 \\
...\\
\varepsilon_n  \end{bmatrix}$$

Where,

$Y$: Response vector

$X$: Design matrix

$\beta$: parameter vector

$\varepsilon$: error vector

If $\varepsilon$ follows a mutivariate normal distribution then we will be under the General Linear Model (GLM) framework. 

$$\varepsilon \sim N(0, \sigma^2I_{x \times n})$$

If $X$ is continuous we have regression.

If $X$ is categorical we have ANOVA.

If $X$ is a mix of both, we have ANCOVA.




## Simple Linear Regression

Simple linear regression is a linear regression model with a single explanatory variable. In addition, we typically assume that this is under the GLM framework and thus we also assume that the residuals follow normal distribution. 


### Least squares vs. MLE vs, propoerties of the regression parameters 

#### Least sqaure method

__Basic Idea:__

Assume the following model:

$$Y=X\beta +\varepsilon$$
Thus,

$$\varepsilon=Y-X\beta$$

Since this is always going to be zero, we need to minimize:

$$\sum \varepsilon^2=\sum [Y-X\beta]^2$$

__Calculation:__

$$Y_{n \times 1}=X_{n\times m}\beta_{m \times 1}$$

$\rightarrow$

$$[X^T]_{m \times n}Y_{n \times 1}=[X^T]_{m \times n}X_{n\times m}\beta_{m \times 1}$$
$\rightarrow$

$$[X^TX]_{m\times m}[X^TY]_{m \times 1}=\beta_{m \times 1}$$

$\rightarrow$

$$\beta_{m \times 1}=[X^TX \cdot X^TY]_{m \times 1}$$

## full rank, less than full rank 

http://www.biostat.jhsph.edu/~iruczins/teaching/140.751/notes/ch7.pdf

$$Y_{n \times 1}=X_{n\times m}\beta_{m \times 1}$$

If the rank $r$ of $X_{n \times m}$ is smaller than $m$, i.e., $r<m$, there is not a unique solution $\hat{\beta}$. We have three ways to find a solution $\hat{\beta}$ and the orthogonal projection $\hat{Y}$:

(1) Reducing the model to the full rank.

(2) Find the generalized inverse $(X^TX)^{-}$.

(3) Impose identififiability constraints.

Let $X_1$ consist of $r$ linear independent columns from $X$ and let $X_2$ consistn of the remaining columns. Then, $X_2=X_1F$ because the columns of $X_2$ are linearly dependent on the columns of $X_1$.

$$X=(X_1, X_2)=(X_1, X_1F)=X_1[I_{r\times r}, F]$$
This is a special case of the factorization $X=KL$, where rank $(K_{n \times r})=r$ and rank $(L_{r\times p})=r$. 

$$E[Y]=x\beta=KL\beta=k\alpha$$
Since $K$ has full rank, the Least Squares Estimate of $\alpha$ is $\hat{\alpha}=(K^T K)^{-1} \cdot K^TY$. 

The orthogonal project,

$$\hat{Y}=K\cdot \alpha=K \cdot(K^T K)^{-1} \cdot K^TY=X_1 \cdot(X_1^TX_1)^{-1}\cdot X_1^TY$$

### Assumptions, checking assumptions

## Bootstrapping used in linear models


## Generalized linear models


### Definition, similarities and differences from general linear models

Generalized Linear Model (GLiM) loosens this assumption that $\varepsilon$ follows a mutivariate normal distribution, and allows for a variety of other distributions from the exponential family for the residuals.

Of note, the GLM is a special case of the GLiM in which the distribution of the residuals follow a conditionally normal distribution.


### Advantage and disadvantages

### logistic and poisson regression 
