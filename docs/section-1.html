<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 556 | Basic Stats</title>
  <meta name="description" content="The webpages are mainly about Bayesian." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 556 | Basic Stats" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The webpages are mainly about Bayesian." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 556 | Basic Stats" />
  
  <meta name="twitter:description" content="The webpages are mainly about Bayesian." />
  

<meta name="author" content="Bill Last Updated:" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="section.html"/>
<link rel="next" href="logit-and-probit.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://williamdingpullman.github.io/" target="blank">Bill's Stats Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface: Motivation</a></li>
<li class="chapter" data-level="1" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>1</b> 443</a><ul>
<li class="chapter" data-level="1.1" data-path="section.html"><a href="section.html#some-basic-concepts"><i class="fa fa-check"></i><b>1.1</b> Some basic concepts</a><ul>
<li class="chapter" data-level="1.1.1" data-path="section.html"><a href="section.html#permutation"><i class="fa fa-check"></i><b>1.1.1</b> Permutation</a></li>
<li class="chapter" data-level="1.1.2" data-path="section.html"><a href="section.html#combinations"><i class="fa fa-check"></i><b>1.1.2</b> Combinations</a></li>
<li class="chapter" data-level="1.1.3" data-path="section.html"><a href="section.html#partitioning"><i class="fa fa-check"></i><b>1.1.3</b> Partitioning</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="section.html"><a href="section.html#discrete-random-variables"><i class="fa fa-check"></i><b>1.2</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="1.2.1" data-path="section.html"><a href="section.html#binomial"><i class="fa fa-check"></i><b>1.2.1</b> Binomial</a></li>
<li class="chapter" data-level="1.2.2" data-path="section.html"><a href="section.html#poisson"><i class="fa fa-check"></i><b>1.2.2</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="section.html"><a href="section.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.3</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="1.3.1" data-path="section.html"><a href="section.html#uniform"><i class="fa fa-check"></i><b>1.3.1</b> Uniform</a></li>
<li class="chapter" data-level="1.3.2" data-path="section.html"><a href="section.html#exponential"><i class="fa fa-check"></i><b>1.3.2</b> Exponential</a></li>
<li class="chapter" data-level="1.3.3" data-path="section.html"><a href="section.html#normal"><i class="fa fa-check"></i><b>1.3.3</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="section.html"><a href="section.html#large-sample-theory"><i class="fa fa-check"></i><b>1.4</b> Large Sample Theory</a><ul>
<li class="chapter" data-level="1.4.1" data-path="section.html"><a href="section.html#convergence-in-distribution"><i class="fa fa-check"></i><b>1.4.1</b> Convergence in distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="section.html"><a href="section.html#weak-law"><i class="fa fa-check"></i><b>1.4.2</b> Weak law</a></li>
<li class="chapter" data-level="1.4.3" data-path="section.html"><a href="section.html#strong-law"><i class="fa fa-check"></i><b>1.4.3</b> Strong law</a></li>
<li class="chapter" data-level="1.4.4" data-path="section.html"><a href="section.html#central-limit-theorem"><i class="fa fa-check"></i><b>1.4.4</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.4.5" data-path="section.html"><a href="section.html#poisson-approximation-to-binomial"><i class="fa fa-check"></i><b>1.4.5</b> Poisson approximation to binomial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>2</b> 556</a><ul>
<li class="chapter" data-level="2.1" data-path="section-1.html"><a href="section-1.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>2.1</b> Statistics and Sampling Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="section-1.html"><a href="section-1.html#statistics"><i class="fa fa-check"></i><b>2.1.1</b> Statistics</a></li>
<li class="chapter" data-level="2.1.2" data-path="section-1.html"><a href="section-1.html#chi2-t-f-beta"><i class="fa fa-check"></i><b>2.1.2</b> <span class="math inline">\(\chi^2, t, F, beta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="section-1.html"><a href="section-1.html#large-sample-approximations"><i class="fa fa-check"></i><b>2.1.3</b> Large-sample approximations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="section-1.html"><a href="section-1.html#point-estimation"><i class="fa fa-check"></i><b>2.2</b> Point Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="section-1.html"><a href="section-1.html#method-of-moments-estimators"><i class="fa fa-check"></i><b>2.2.1</b> Method of moments estimators</a></li>
<li class="chapter" data-level="2.2.2" data-path="section-1.html"><a href="section-1.html#least-squares-estimators"><i class="fa fa-check"></i><b>2.2.2</b> least squares estimators</a></li>
<li class="chapter" data-level="2.2.3" data-path="section-1.html"><a href="section-1.html#likelihood-function-and-maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.2.3</b> likelihood function and maximum likelihood estimators</a></li>
<li class="chapter" data-level="2.2.4" data-path="section-1.html"><a href="section-1.html#invariance-property-of-mles"><i class="fa fa-check"></i><b>2.2.4</b> Invariance property of MLEs</a></li>
<li class="chapter" data-level="2.2.5" data-path="section-1.html"><a href="section-1.html#unbiased-estimators"><i class="fa fa-check"></i><b>2.2.5</b> Unbiased estimators</a></li>
<li class="chapter" data-level="2.2.6" data-path="section-1.html"><a href="section-1.html#unbiased-estimators-vs.-invariance-property-of-mles"><i class="fa fa-check"></i><b>2.2.6</b> Unbiased estimators vs. Invariance property of MLEs</a></li>
<li class="chapter" data-level="2.2.7" data-path="section-1.html"><a href="section-1.html#umvue-and-cramer-rao-lower-bound"><i class="fa fa-check"></i><b>2.2.7</b> UMVUE and Cramer-Rao lower bound</a></li>
<li class="chapter" data-level="2.2.8" data-path="section-1.html"><a href="section-1.html#best-linear-unbiased-estimation-blue-or-mvlue"><i class="fa fa-check"></i><b>2.2.8</b> Best linear unbiased estimation (BLUE or MVLUE)</a></li>
<li class="chapter" data-level="2.2.9" data-path="section-1.html"><a href="section-1.html#rao-blackwell-theorem"><i class="fa fa-check"></i><b>2.2.9</b> Rao-Blackwell theorem</a></li>
<li class="chapter" data-level="2.2.10" data-path="section-1.html"><a href="section-1.html#consistency-asymptotic-unbiasedness"><i class="fa fa-check"></i><b>2.2.10</b> Consistency, asymptotic unbiasedness</a></li>
<li class="chapter" data-level="2.2.11" data-path="section-1.html"><a href="section-1.html#efficiency-asymptotic-efficiency"><i class="fa fa-check"></i><b>2.2.11</b> Efficiency, asymptotic efficiency</a></li>
<li class="chapter" data-level="2.2.12" data-path="section-1.html"><a href="section-1.html#asymptotic-properties-of-mles"><i class="fa fa-check"></i><b>2.2.12</b> Asymptotic properties of MLEs</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="section-1.html"><a href="section-1.html#sufficient-and-completeness"><i class="fa fa-check"></i><b>2.3</b> Sufficient and completeness</a><ul>
<li class="chapter" data-level="2.3.1" data-path="section-1.html"><a href="section-1.html#sufficiency-and-minimal-sufficiency"><i class="fa fa-check"></i><b>2.3.1</b> Sufficiency and minimal sufficiency</a></li>
<li class="chapter" data-level="2.3.2" data-path="section-1.html"><a href="section-1.html#neyman-factorization-theorem-minimal-sufficiency-of-mles"><i class="fa fa-check"></i><b>2.3.2</b> Neyman factorization theorem, minimal sufficiency of MLEs</a></li>
<li class="chapter" data-level="2.3.3" data-path="section-1.html"><a href="section-1.html#completeness-lehmann-scheffe-completeness-theorem"><i class="fa fa-check"></i><b>2.3.3</b> completeness, Lehmann-Scheffe completeness theorem</a></li>
<li class="chapter" data-level="2.3.4" data-path="section-1.html"><a href="section-1.html#exponential-class-complete-sufficient-statistics"><i class="fa fa-check"></i><b>2.3.4</b> Exponential class, complete sufficient statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="logit-and-probit.html"><a href="logit-and-probit.html"><i class="fa fa-check"></i><b>3</b> Logit and Probit</a><ul>
<li class="chapter" data-level="3.1" data-path="logit-and-probit.html"><a href="logit-and-probit.html#logit"><i class="fa fa-check"></i><b>3.1</b> Logit</a></li>
<li class="chapter" data-level="3.2" data-path="logit-and-probit.html"><a href="logit-and-probit.html#probit"><i class="fa fa-check"></i><b>3.2</b> Probit</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="normal-distribution.html"><a href="normal-distribution.html"><i class="fa fa-check"></i><b>4</b> Normal distribution</a><ul>
<li class="chapter" data-level="4.1" data-path="normal-distribution.html"><a href="normal-distribution.html#basics"><i class="fa fa-check"></i><b>4.1</b> Basics</a></li>
<li class="chapter" data-level="4.2" data-path="normal-distribution.html"><a href="normal-distribution.html#confidence-intervals-for-normal-distributions"><i class="fa fa-check"></i><b>4.2</b> Confidence intervals for normal distributions</a></li>
<li class="chapter" data-level="4.3" data-path="normal-distribution.html"><a href="normal-distribution.html#percentile"><i class="fa fa-check"></i><b>4.3</b> Percentile</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>5</b> MLE</a><ul>
<li class="chapter" data-level="5.1" data-path="intro.html"><a href="intro.html#basic-idea-of-mle"><i class="fa fa-check"></i><b>5.1</b> Basic idea of MLE</a></li>
<li class="chapter" data-level="5.2" data-path="intro.html"><a href="intro.html#coin-flip-example-probit-and-logit"><i class="fa fa-check"></i><b>5.2</b> Coin flip example, probit, and logit</a><ul>
<li class="chapter" data-level="5.2.1" data-path="intro.html"><a href="intro.html#probit-1"><i class="fa fa-check"></i><b>5.2.1</b> Probit</a></li>
<li class="chapter" data-level="5.2.2" data-path="intro.html"><a href="intro.html#logit-1"><i class="fa fa-check"></i><b>5.2.2</b> Logit</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="intro.html"><a href="intro.html#further-on-logit"><i class="fa fa-check"></i><b>5.3</b> Further on logit</a></li>
<li class="chapter" data-level="5.4" data-path="intro.html"><a href="intro.html#references"><i class="fa fa-check"></i><b>5.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html"><i class="fa fa-check"></i><b>6</b> Score, Gradient and Jacobian</a><ul>
<li class="chapter" data-level="6.1" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#score"><i class="fa fa-check"></i><b>6.1</b> Score</a></li>
<li class="chapter" data-level="6.2" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#fisher-scoring"><i class="fa fa-check"></i><b>6.2</b> Fisher scoring</a></li>
<li class="chapter" data-level="6.3" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#gradient-and-jacobian"><i class="fa fa-check"></i><b>6.3</b> Gradient and Jacobian</a></li>
<li class="chapter" data-level="6.4" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#hessian-and-fisher-information"><i class="fa fa-check"></i><b>6.4</b> Hessian and Fisher Information</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="canonical-link-function.html"><a href="canonical-link-function.html"><i class="fa fa-check"></i><b>7</b> Canonical link function</a></li>
<li class="chapter" data-level="8" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>8</b> Ordinary Least Squares (OLS)</a><ul>
<li class="chapter" data-level="8.1" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#taylor-series"><i class="fa fa-check"></i><b>8.1</b> Taylor series</a></li>
<li class="chapter" data-level="8.2" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#references-1"><i class="fa fa-check"></i><b>8.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html"><i class="fa fa-check"></i><b>9</b> Cholesky decomposition</a><ul>
<li class="chapter" data-level="9.1" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-1"><i class="fa fa-check"></i><b>9.1</b> Example 1</a></li>
<li class="chapter" data-level="9.2" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-2"><i class="fa fa-check"></i><b>9.2</b> Example 2</a></li>
<li class="chapter" data-level="9.3" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-3"><i class="fa fa-check"></i><b>9.3</b> Example 3</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://www.williamsding.com/" target="blank">Bill's website</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basic Stats</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-1" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> 556</h1>
<div id="statistics-and-sampling-distributions" class="section level2">
<h2><span class="header-section-number">2.1</span> Statistics and Sampling Distributions</h2>
<div id="statistics" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Statistics</h3>
<div id="definition-of-statistic" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Definition of Statistic</h4>
<p><em>P.264</em></p>
<p>A fucntion of observable random variables, <span class="math inline">\(T=t(X_1, ... X_n)\)</span>, which does not depend on any unknown parameters is called statistic.</p>
<p>For example, let <span class="math inline">\(X_1, ..., X_n\)</span> represent a random sample from a population with <span class="math inline">\(pdf \;f(x)\)</span>. The sample mean provides an example of a statistic with the function</p>
<p><span class="math display">\[t(x_1,...,x_n)=(x_1+...+x_n)/n\]</span></p>
<p>This statistic usually is denoted by</p>
<p><span class="math display">\[\bar{X}=\sum_{i=1}^n \frac{X_i}{n}\]</span></p>
<p>When a random sample is observed, the value of <span class="math inline">\(\bar{X}\)</span>, computed from the data, usually is denoted by lower case <span class="math inline">\(\bar{x}\)</span>.</p>
<p><span class="math display">\[\bar{x}=\sum_{i=1}^n \frac{x_i}{n}\]</span></p>
</div>
<div id="sample-and-parameters" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> Sample and parameters</h4>
<p><em>P.265</em></p>
<p>If <span class="math inline">\(X_1,..., X_n\)</span> denotes a random sample from <span class="math inline">\(f(x)\)</span> with <span class="math inline">\(E(X)=\mu\)</span> and <span class="math inline">\(var(X)=\sigma^2\)</span>, then</p>
<p><span class="math display">\[E(\bar{X})=\mu\]</span></p>
<p><span class="math display">\[Var(\bar{X})=\frac{\sigma^2}{n}\]</span></p>
<p>For example, a random sample of size <span class="math inline">\(n\)</span> from a Bernoulli distribution <span class="math inline">\(X_i \sim BIN(1,p)\)</span>. We know Bernoulli has <span class="math inline">\(\mu=p\)</span> and <span class="math inline">\(\sigma^2 =pq\)</span>. In this case, the sample mean is</p>
<p><span class="math display">\[\bar{X}=Y/n=\hat{p}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E(\hat{p})=p\]</span></p>
<p><span class="math display">\[Var (\hat{p})=\frac{pq}{n}\]</span></p>
<p>Thus, sample mean is the unbiased estimate for the population mean. However, you can not use sample mean’s variance to estimate pupulation variance. That lead to definition of sample variance.</p>
<p><em>P.266</em></p>
<p>Sample variance:</p>
<p><span class="math display">\[S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2\]</span></p>
<p><span class="math display">\[E(S^2)=\sigma^2\]</span></p>
</div>
</div>
<div id="chi2-t-f-beta" class="section level3">
<h3><span class="header-section-number">2.1.2</span> <span class="math inline">\(\chi^2, t, F, beta\)</span></h3>
<div id="chi2" class="section level4">
<h4><span class="header-section-number">2.1.2.1</span> <span class="math inline">\(\chi^2\)</span></h4>
<p>Always squre from standard normal, and the standardization can be using <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\bar{X}\)</span>.</p>
<p><span class="math display">\[\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2(n-1)\]</span>
(Thus, we can see this is a bit weired, as the numerator is <span class="math inline">\(\bar{X}\)</span> is from the sample, whereas <span class="math inline">\(\sigma^2\)</span> is from the population.)</p>
<p>Thus, we can</p>
<p><span class="math display">\[\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sigma^2}\sim \chi^2(n-1)\]</span>
(You can compare <span class="math inline">\(\bar{X}\)</span> with <span class="math inline">\(\mu\)</span>, we can see the only difference is that the <span class="math inline">\(\chi^2\)</span> has one less degree of freedom because we use this degree of freedom to calculate the mean.)</p>
<p>For the mean and variance of <span class="math inline">\(\chi^2\)</span>:</p>
<p>Assume that</p>
<p><span class="math display">\[X \sim x^2(v)\]</span></p>
<p><span class="math display">\[mean:v\]</span>
<span class="math display">\[variance: 2v\]</span></p>
</div>
<div id="t" class="section level4">
<h4><span class="header-section-number">2.1.2.2</span> <span class="math inline">\(t\)</span></h4>
<p>Definition</p>
<p><span class="math display">\[t(k)=\frac{N(0,1)}{\sqrt{\frac{\chi^2(k)}{k}}}\]</span></p>
<p><strong>Property 1</strong></p>
<p><strong>t</strong> distribution is symmetrical</p>
<p>Given that <em>t</em> distribution is symmetrical, we can get</p>
<p><span class="math display">\[H(-c)=1-H(c)\]</span></p>
<p><strong>Property 2</strong></p>
<p><em>t</em> distribution has heavier tails than the normal.</p>
<p>My note: <em>t</em> distribution only has a parameter of <span class="math inline">\(k\)</span>, which is determined by the <span class="math inline">\(\chi^2\)</span>’s degree of freedom. Of course, <span class="math inline">\(\chi^2\)</span> also only has one parameter, namely the degree of freedom.</p>
</div>
<div id="f" class="section level4">
<h4><span class="header-section-number">2.1.2.3</span> <span class="math inline">\(F\)</span></h4>
<p>If <span class="math inline">\(V_1 \sim \chi^2(v_1)\)</span> and <span class="math inline">\(V_2 \sim \chi^2(v_2)\)</span> are independent, then the random variable</p>
<p><span class="math display">\[\frac{V_1/v_1}{V_2/v_2}\sim F(v_1,v_2)\]</span></p>
</div>
<div id="beta" class="section level4">
<h4><span class="header-section-number">2.1.2.4</span> Beta</h4>
<p>If <span class="math inline">\(X \sim F(v_1, v_2)\)</span></p>
<p><span class="math display">\[Y=\frac{(v_1/v_2)X}{1+(v_1/v_2)X} \sim Beta(\alpha, \beta)\]</span></p>
</div>
</div>
<div id="large-sample-approximations" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Large-sample approximations</h3>
<p><em>P.280</em></p>
<p>If <span class="math inline">\(Y_v \sim x^2(x)\)</span>, then</p>
<p><span class="math display">\[Z_v =\frac{Y_v-v}{\sqrt{2v}} \xrightarrow{d} Z \sim N(0,1)\]</span></p>
<p>(The proof is based on CLT.)</p>
</div>
</div>
<div id="point-estimation" class="section level2">
<h2><span class="header-section-number">2.2</span> Point Estimation</h2>
<div id="method-of-moments-estimators" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Method of moments estimators</h3>
<div id="definition-about-moments-chapter-2" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> Definition about moments (chapter 2)</h4>
<p><em>P.73</em></p>
<p>THe <strong>kth moment about the origin</strong> of a random variable <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[\mu^{&#39;}_k=E(X^k)\]</span></p>
<p>and the <strong>kth moment about the mean</strong> is</p>
<p><span class="math display">\[\mu_k=E[X-E(X)]^k=E(X-\mu)^k\]</span></p>
<p>Thus,<span class="math inline">\(k=E(X^k)\)</span> may be considered as the <span class="math inline">\(k\)</span>th moment of <span class="math inline">\(X\)</span> or the first moment of <span class="math inline">\(X^k\)</span>.</p>
<p>The first moment about the mean is zero,</p>
<p><span class="math display">\[\mu_1=E[X-E(X)]=E(X)-E(X)=0\]</span></p>
<p>The second moment about the mean is the variance,</p>
<p><span class="math display">\[\mu_2=E[(X-\mu)^2]=\sigma^2\]</span></p>
<p>Note that the definition of variance:</p>
<p><em>P.73</em></p>
<p><span class="math display">\[Var(X)=E[(X-\mu)^2]\]</span></p>
</div>
<div id="definition" class="section level4">
<h4><span class="header-section-number">2.2.1.2</span> Definition</h4>
<p>Based on the last chapater (i.e., Chapter 8), sample mean <span class="math inline">\(\bar{X}\)</span> is an estimator of the population mean <span class="math inline">\(\mu\)</span>. A more general approach, which produced estimators known as the <strong>method of moments estimators(MMEs)</strong> , can be developed.</p>
<p>If <span class="math inline">\(X_1,...,X_n\)</span> is a random sample from <span class="math inline">\(f(x; \theta_1,...,\theta_k)\)</span>, the first <span class="math inline">\(k\)</span> sample moments are given by</p>
<p><span class="math display">\[M_j^{&#39;}=\frac{\sum_{i=1}^n X_i^j}{n}\]</span></p>
<p>where,</p>
<p><span class="math display">\[j=1,2,...k\]</span></p>
<p><strong>Example 1</strong></p>
<p><em>P.291</em></p>
<p>Consider a random sample from a distribution with two unknown parameters, the mean <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span>. We know from earlier considerations that <span class="math inline">\(\mu=\mu^{&#39;}_1\)</span> and <span class="math inline">\(\sigma^2=E(X^2)-\mu^2=\mu_2^{&#39;}-(\mu^{&#39;}_1)^2\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[\hat{\sigma}^2=\mu_2^{&#39;}-(\mu^{&#39;}_1)^2=\frac{\sum_{i=1}^n X_i^2}{n}-\frac{\sum_{i=1}^n X_i}{n}=\frac{\sum_{i=1}^n X_i^2}{n}-\bar{X}=\sum_{i=1}^n \frac{(X_i-\bar{X})^2}{n}\]</span></p>
<p>(Thus, we can see that the estimation of <span class="math inline">\(\sigma^2\)</span> is not the same as the definition of sample variance <span class="math inline">\(S^2\)</span>. <span class="math inline">\(\hat{\sigma}^2=\frac{n-1}{n}S^2\)</span>.)</p>
<p><strong>Example 2</strong></p>
<p><em>P.292</em></p>
<p>If a sample is from a Gamma distribution <span class="math inline">\(X_i \sim GAM(\theta,k)\)</span>, and we want to estimate the <span class="math inline">\(\theta\)</span> and <span class="math inline">\(k\)</span>.</p>
<p>We know that for Gamma distribution, the mean is <span class="math inline">\(k\theta\)</span>, and the variance is <span class="math inline">\(k\theta^2\)</span>.</p>
<p>We also know that <span class="math inline">\(\mu_1^{&#39;}=\mu=k\theta\)</span> and <span class="math inline">\(\mu_2^{&#39;}= \sigma^2+\mu^2= k\theta^2+k^2\theta^2=k\theta^2(1+k)\)</span>.</p>
<p>Thus, we can get</p>
<p><span class="math display">\[\bar{X}=k\theta\]</span></p>
<p><span class="math display">\[\sum \frac{X_i^2}{n}=k\theta^2(1+k)\]</span></p>
<p>Thus, we can get</p>
<p><span class="math display">\[\hat{\theta}=\sum_{i=1}^n \frac{(X_i-\bar{X})^2}{n \bar{X}}=\frac{(n-1)/n S^2}{\bar{X}}\]</span></p>
<p><span class="math display">\[\hat{k}=\frac{\bar{X}}{\hat{\theta}}\]</span></p>
</div>
<div id="property" class="section level4">
<h4><span class="header-section-number">2.2.1.3</span> Property</h4>
<p>The joint MGF of (<span class="math inline">\(X_1, ..., X_n\)</span>) is defined as <span class="math inline">\(M(t_1,...,t_n)=E(e^{\sum_{i=1}^nt_iX_i})\)</span></p>
<p>When <span class="math inline">\(X_1, ..., X_n\)</span> are independent if and only if</p>
<p><span class="math display">\[M(t_1,...,t_n)=\prod_{i=1}^n M_{X_i}(t_i)\]</span>
where <span class="math inline">\(M_{X_i}(t_i)\)</span> is the MGF of <span class="math inline">\(X_i\)</span></p>
</div>
<div id="well-known-mgf" class="section level4">
<h4><span class="header-section-number">2.2.1.4</span> Well-known MGF</h4>
<ol style="list-style-type: decimal">
<li><p>Bernoulli with success probability p: <span class="math inline">\(1-p+pe^t\)</span></p></li>
<li><p>Binomial Bin(n,p): <span class="math inline">\((1-p+pe^t)^n\)</span></p></li>
<li><p>Poisson <span class="math inline">\(POI(\lambda)\)</span>: <span class="math inline">\(e^{\lambda(e^t-1)}\)</span></p></li>
<li><p>Normal <span class="math inline">\(N(\mu,\sigma^2)\)</span>: <span class="math inline">\(e^{\mu t+\frac{1}{2}\sigma^2t^2}\)</span></p></li>
<li><p>Gamma <span class="math inline">\(GAM(\theta,k)\)</span>: <span class="math inline">\((1-\theta t)^{-k}\)</span></p></li>
</ol>
<p>Two special cases:</p>
<ol start="6" style="list-style-type: decimal">
<li><p>Chi-square <span class="math inline">\(\chi^2(v) =GAM(2,\frac{v}{2})\)</span>: <span class="math inline">\((1-2t)^{-\frac{v}{2}}\)</span></p></li>
<li><p>Exponential <span class="math inline">\(EXP(\theta)=GAM(\theta,1)\)</span>: <span class="math inline">\((1-\theta t)^{-1}\)</span></p></li>
</ol>
</div>
</div>
<div id="least-squares-estimators" class="section level3">
<h3><span class="header-section-number">2.2.2</span> least squares estimators</h3>
</div>
<div id="likelihood-function-and-maximum-likelihood-estimators" class="section level3">
<h3><span class="header-section-number">2.2.3</span> likelihood function and maximum likelihood estimators</h3>
<div id="likelihood-function" class="section level4">
<h4><span class="header-section-number">2.2.3.1</span> Likelihood function</h4>
<p><em>P.293</em></p>
<p>The joint density function of <span class="math inline">\(n\)</span> random variables <span class="math inline">\(X_1,...X_n\)</span> evaluated at <span class="math inline">\(x_1, ...x_n\)</span>, say <span class="math inline">\(f(x_1,...,x_n; \theta)\)</span>, is referered to as a <em>likelihood function</em>.</p>
</div>
<div id="maximum-likelihood-estimators" class="section level4">
<h4><span class="header-section-number">2.2.3.2</span> Maximum likelihood estimators</h4>
<p><em>P.294</em></p>
<p>Let <span class="math inline">\(L(\theta)=f(x_1,...x_n; \theta), \theta \in \Omega\)</span>, be the joint pdf of <span class="math inline">\(X_1, ..., X_n\)</span>. For a given set of observations, <span class="math inline">\((x_1,...x_n)\)</span>, a value <span class="math inline">\(\hat{\theta}\)</span> in <span class="math inline">\(\Omega\)</span> at which <span class="math inline">\(L(\theta)\)</span> is a maximum is called a <em>maximum likelihood estimate (MLE)</em> of <span class="math inline">\(\theta\)</span>. That is <span class="math inline">\(\hat{\theta}\)</span> is a value of <span class="math inline">\(\theta\)</span> that satisfies</p>
<p><span class="math display">\[f(x_1,...x_n; \theta)=MAX_{\theta \in \Omega} f(x_1, ..., x_n; \theta)\]</span></p>
</div>
</div>
<div id="invariance-property-of-mles" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Invariance property of MLEs</h3>
<p><em>P.296</em></p>
<p>If <span class="math inline">\(\hat{\theta}\)</span> is the MLE of <span class="math inline">\(\theta\)</span> and if <span class="math inline">\(u(\theta)\)</span> is a function of <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(u(\hat{\theta})\)</span> is an MLE of <span class="math inline">\(u(\theta)\)</span>.</p>
<p><strong>Example 1</strong></p>
<p>We know that the <span class="math inline">\(pdf\)</span> of exponential distribution (<span class="math inline">\(X \sim EXP (\theta)\)</span>) is as follows:</p>
<p><span class="math display">\[\frac{1}{\theta} e^{-\frac{X}{\theta}}\]</span></p>
<p>Thus, its likelihood function is as follows</p>
<p><span class="math display">\[L(\theta)=\frac{1}{\theta^n}e^{-\frac{\sum X_i}{\theta}}\]</span>
Thus, log-likelihood is as follows.</p>
<p><span class="math display">\[lnL(\theta)=-n ln(\theta)-\frac{\sum X_i}{\theta}\]</span>
Thus,</p>
<p><span class="math display">\[\frac{d}{d\theta} lnL(\theta)=-n \frac{1}{\theta}+\frac{\sum X_i}{\theta^2}\]</span></p>
<p>Thus, we can get the <span class="math inline">\(MLE\)</span> for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta}=\bar{x}\)</span>.</p>
<p>If we want to estimate <span class="math inline">\(\tau(\theta)=P(X \geq 1)\)</span>:</p>
<p><span class="math display">\[\tau(\theta)=P(X\geq 1)=\int_1^{\infty} \frac{1}{\theta} e^{-\frac{X}{\theta}} dx=-\int_1^{\infty}  e^{-\frac{X}{\theta}} d(-\frac{x}{\theta})=-[e^{-\frac{X}{\theta}}]_1^{\infty}=-[0-e^{-\frac{1}{\theta}}]=e^{-\frac{1}{\theta}}\]</span></p>
<p>Thus, based on the invariance property, we know that the <span class="math inline">\(MLE\)</span> for <span class="math inline">\(\tau(\theta)\)</span> is as follows.</p>
<p><span class="math display">\[e^{-\frac{1}{\bar{x}}}\]</span></p>
<p><strong>Example 2: MLE vs. MME</strong></p>
<p><em>P.296</em></p>
<p>Assume a random sample from a two-parameter exponential distribution, <span class="math inline">\(X_i \sim EXP(1)\)</span>. Thus, the <span class="math inline">\(pdf\)</span> is <span class="math inline">\(e^{-(x-\eta)}\)</span>. Thus, the likelihood function is</p>
<p><span class="math display">\[L(\eta)=e^{-\sum(x_i-\eta)}\]</span></p>
<p>Thus, the log likelihood,</p>
<p><span class="math display">\[lnL(\eta)=-\sum(x_i-\eta)=n\eta-n\bar{X}\]</span>
Thus, we know that as <span class="math inline">\(\eta\)</span> increases, the log likelihood increases accordingly. Thus, we want to find the maximum <span class="math inline">\(\eta\)</span>. Note that a two-parameter exponential distribution has the support of <span class="math inline">\(x_i \geq \eta\)</span>. Thus, all <span class="math inline">\(\eta\)</span> are smaller than any <span class="math inline">\(X_i\)</span>. Thus, we can get the ML estimator is the first order statistic</p>
<p><span class="math display">\[\hat{\eta}=X_{1:n}\]</span></p>
<p><strong>Note that</strong> the estimators above is based on ML. What would be the answer if using MME?</p>
<p>We know that for a two-parameter exponential distribution, its mean is <span class="math inline">\(\mu=1+\eta\)</span>. And, we know that based on MME, <span class="math inline">\(\mu=\bar{X}\)</span>. Thus, we can get the following,</p>
<p><span class="math display">\[\hat{\eta}=\bar{X}-1\]</span></p>
<p><strong>Conclusion</strong></p>
<p>We can see that ML and MME have didferent estimators for the same <span class="math inline">\(\eta\)</span> for a two-parameter exponential distribution.</p>
</div>
<div id="unbiased-estimators" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Unbiased estimators</h3>
<p>An estimator T is said to be an unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span> if</p>
<p><span class="math display">\[E(T)=\tau(\theta)\]</span></p>
<p>for all <span class="math inline">\(\theta \in \Omega\)</span>. Otherwise, we said that <span class="math inline">\(T\)</span> is biased stimator of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p>For instance, if we want to estimate a percentile, say the 95th percentile of <span class="math inline">\(N(\mu,9)\)</span>. Note that the percentiles that we know are about standardized noraml (i.e., <span class="math inline">\(N(0,1)\)</span>). Thus, we need to have some calculation to get the non-standard one.</p>
<p><span class="math display">\[\frac{X_{95 \; percentile}-\mu}{\sigma}=1.645\]</span>
Thus, we can get</p>
<p><span class="math display">\[X_{95 \; percentile}=1.645 \times \sigma +\mu\]</span></p>
<p>We know that <span class="math inline">\(\bar{X}\)</span> is the unbiased estimate for <span class="math inline">\(\mu\)</span>. Thus, we can get</p>
<p><span class="math display">\[X_{95 \; percentile}=1.645 \times \sigma +\mu=4.94+\mu\]</span></p>
<p>We know that</p>
<p><span class="math display">\[E(T)=E(\bar{X}+4.94)=\mu+4.94\]</span>
Thus, <span class="math inline">\(T=\bar{X}+4.94\)</span> is the unbiased estimator of <span class="math inline">\(\tau(\mu)=\mu+4.94\)</span>.</p>
</div>
<div id="unbiased-estimators-vs.-invariance-property-of-mles" class="section level3">
<h3><span class="header-section-number">2.2.6</span> Unbiased estimators vs. Invariance property of MLEs</h3>
<p><strong>Do not apply “Invariance property of MLEs” to the Unbiased estimators.</strong></p>
<p><em>P.303</em></p>
<p>For example, consider a random sample of size <span class="math inline">\(n\)</span> from an exponential distribution, <span class="math inline">\(X_i \sim EXP(\theta)\)</span>, with parameter <span class="math inline">\(\theta\)</span>. We know that, <span class="math inline">\(\bar{X}\)</span> is unbiased for <span class="math inline">\(\theta\)</span> (which is the mean of an exponential distribution).</p>
<p>If we want to estimate <span class="math inline">\(\tau(\theta)=\frac{1}{\theta}\)</span>, then by the invariance property of MLE is <span class="math inline">\(T_1=\frac{1}{\bar{X}}\)</span>.</p>
<p>However, <span class="math inline">\(T_1\)</span> is a biased estimators of <span class="math inline">\(\frac{1}{\theta}\)</span>. Specifically,</p>
<p>We know that if <span class="math inline">\(X \sim Gam(\theta,k)\)</span>, then <span class="math inline">\(Y=\frac{2X}{\theta} \sim x^2(2k)\)</span>. We know that exponential distributions are a specicial case of Gamma distribution, <span class="math inline">\(EXP(\theta)=Gam(\theta,1)\)</span>, thus we get the following,</p>
<p><span class="math display">\[Y=\frac{2n\bar{X}}{\theta}=\frac{2n}{\theta} \frac{\sum_{i=1}^n X_i}{n}=\frac{\sum_{i=1}^n 2X_i}{\theta}\sim\sum_{i=1}^n x^2(2\cdot1)=\sum_{i=1}^n x^2(2)=x^2(2n)\]</span></p>
<p>We further know that if <span class="math inline">\(Y \sim x^2(v)\)</span>, <span class="math inline">\(E(Y^r)=2^r\frac{\Gamma(v/2+r)}{\Gamma(v/2)}\)</span>. Thus, we know that,</p>
<p><span class="math display">\[E(Y^{-1})=2^{-1} \frac{\Gamma(2n/2-1)}{\Gamma(2n/2)}=\frac{1}{2}\cdot \frac{1}{n-1}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E(Y^{-1})=E(\frac{\theta}{2n \bar{X}})=\frac{\theta}{2n}E(\frac{1}{\bar{X}})=\frac{1}{2}\cdot \frac{1}{n-1}\]</span></p>
<p>Thus.</p>
<p><span class="math display">\[E(\frac{1}{\bar{X}})=\frac{1}{n-1}\frac{n}{\theta}=\frac{n}{n-1}\frac{1}{\theta}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E(\frac{n-1}{n} \frac{1}{\bar{X}})=\frac{1}{\theta}\]</span></p>
<p><strong>Conclusion</strong></p>
<p><span class="math inline">\(\frac{1}{\bar{X}}\)</span> is not the unbiased estimator for <span class="math inline">\(\frac{1}{\theta}\)</span>. However, we can adjust it to <span class="math inline">\(\frac{n-1}{n} \frac{1}{\bar{X}}\)</span>, which is the unbiased estimator for <span class="math inline">\(\frac{1}{\theta}\)</span>. When the sample size is big enough, we know that <span class="math inline">\(\frac{n-1}{n}\)</span> will be close to 1.</p>
</div>
<div id="umvue-and-cramer-rao-lower-bound" class="section level3">
<h3><span class="header-section-number">2.2.7</span> UMVUE and Cramer-Rao lower bound</h3>
<div id="umvue" class="section level4">
<h4><span class="header-section-number">2.2.7.1</span> UMVUE</h4>
<p>Let <span class="math inline">\(X_1, X_2,...,X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(f(x; \theta)\)</span>. An estimator <span class="math inline">\(T^*\)</span> of <span class="math inline">\(\tau (\theta)\)</span> is called a <em>uniformly minimum variance ubiased estimator</em> (UMVUE) of <span class="math inline">\(\tau(\theta)\)</span> if</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(T^*\)</span> is unbiased for <span class="math inline">\(\tau{\theta}\)</span></p></li>
<li><p>For any other unbiased estimator <span class="math inline">\(T\)</span> of <span class="math inline">\(\tau(\theta)\)</span>, <span class="math inline">\(Var(T^*) \leq Var(T)\)</span> for all <span class="math inline">\(\theta \in \Omega\)</span>.</p></li>
</ol>
</div>
<div id="cramer-rao-lower-bound" class="section level4">
<h4><span class="header-section-number">2.2.7.2</span> Cramer-Rao lower bound</h4>
<p>If <span class="math inline">\(T\)</span> is an unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>, then the Cramer-Rao lower bound (CRLB), based on a random sample, is</p>
<p><span class="math display">\[Var(T)=\frac{[\tau^{&#39;}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}\]</span></p>
<p><strong>Example:</strong></p>
<p>Consider a random sample from an exponential distribution, <span class="math inline">\(X_i \sim Exp(\theta)\)</span>. Because</p>
<p><span class="math display">\[f(x; \theta)=\frac{1}{\theta} e^{-\frac{x}{\theta}}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[ln(f(x;\theta))=-\frac{x}{\theta}-ln \theta\]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial \theta} ln(f(X; \theta))=\frac{X}{\theta^2}-\frac{1}{\theta}=\frac{X-\theta}{\theta^2}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E[\frac{\partial}{\partial \theta} ln(f(X; \theta))]^2 =E[\frac{(X-\theta)^2}{\theta^4}]=\frac{1}{\theta^4}E(X-\theta)^2=\frac{1}{\theta^4} Var(X)=\frac{\theta^2}{\theta^4}=\frac{1}{\theta^2}\]</span></p>
<p>Thus, the CRLB for <span class="math inline">\(\tau(\theta)=\theta\)</span> is as follows.</p>
<p><span class="math display">\[Var(T) \geq \frac{[\tau^{&#39;}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}=\frac{[\frac{\partial }{\partial \theta}\theta]^2}{n \frac{1}{\theta^2}}=\frac{1^2}{\frac{n}{\theta^2}}=\frac{\theta^2}{n}\]</span></p>
<p>We know that the variance for exponential distribution</p>
<p><span class="math display">\[Var(\bar{X})=\frac{Var(X)}{n}=\frac{\theta^2}{n}\]</span></p>
<p>We know that <span class="math inline">\(\theta\)</span> is the mean for exponential distribution. We also know that sample mean <span class="math inline">\(\bar{X}\)</span> is the unbiased estimator for population mean.</p>
<p>Thus,</p>
<p><span class="math inline">\(\bar{X}\)</span> is the UMVUE of <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<div id="best-linear-unbiased-estimation-blue-or-mvlue" class="section level3">
<h3><span class="header-section-number">2.2.8</span> Best linear unbiased estimation (BLUE or MVLUE)</h3>
</div>
<div id="rao-blackwell-theorem" class="section level3">
<h3><span class="header-section-number">2.2.9</span> Rao-Blackwell theorem</h3>
</div>
<div id="consistency-asymptotic-unbiasedness" class="section level3">
<h3><span class="header-section-number">2.2.10</span> Consistency, asymptotic unbiasedness</h3>
</div>
<div id="efficiency-asymptotic-efficiency" class="section level3">
<h3><span class="header-section-number">2.2.11</span> Efficiency, asymptotic efficiency</h3>
<p><em>P.308</em></p>
<p>The relative efficiency of an unbiased estimator <span class="math inline">\(T\)</span> of <span class="math inline">\(\tau(\theta)\)</span> to another unbiased estimator <span class="math inline">\(T^*\)</span> of <span class="math inline">\(\tau(\theta)\)</span> is given by</p>
<p><span class="math display">\[re(T, T^*)=\frac{Var(T^*)}{Var(T)}\]</span></p>
<p>An unbiased estimator <span class="math inline">\(T^*\)</span> of <span class="math inline">\(\tau(\theta)\)</span> is said to be efficient if <span class="math inline">\(re(T, T^*) \leq 1\)</span> for all unbiased estimators <span class="math inline">\(T\)</span> of <span class="math inline">\(\tau(\theta)\)</span>, and all <span class="math inline">\(\theta \in \Omega\)</span>. The efficiency of an unbiased estimator <span class="math inline">\(T\)</span> of <span class="math inline">\(\tau(\theta)\)</span> is given by</p>
<p><span class="math display">\[e(T)=re(T, T^*)\]</span></p>
<p>if <span class="math inline">\(T^*\)</span> is an efficient estimator of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p>Thus, <strong>an effecient estimator is just a UMVUE.</strong></p>
<p><strong>Example:</strong></p>
<p><em>P.233 Example 7.2.2</em>
<em>P.303 Example 9.3.2</em>
<em>P.309 Example 9.3.7</em></p>
<p>Let <span class="math inline">\(X_1,X_2,...,X_n\)</span> sample from <span class="math inline">\(X_i \sim EXP(\theta)\)</span>. We know that</p>
<p><span class="math display">\[X_{1:n}=EXP(\theta/n)\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E(nX_{1:n})=nE(X_{1:n})=n\frac{\theta}{n}=\theta\]</span>
Thus,</p>
<p><span class="math display">\[nX_{1:n}\]</span>
is the unbiased estimator for <span class="math inline">\(\theta\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[re(T,T^*)=\frac{Var(T^*)}{Var(T)}=\frac{Var(\bar{X})}{Var(nX_{1:n})}=\frac{\theta^2/n}{n^2Var(X_{1:n})}=\frac{\theta^2/n}{n^2(\theta/n)^2}=\frac{\theta^2/n}{\theta^2}=\frac{1}{n}\]</span>
Thus, <span class="math inline">\(T^*=\bar{X}\)</span> is a more efficient estimator for <span class="math inline">\(\theta\)</span> than <span class="math inline">\(T=nX_{1:n}\)</span>.</p>
</div>
<div id="asymptotic-properties-of-mles" class="section level3">
<h3><span class="header-section-number">2.2.12</span> Asymptotic properties of MLEs</h3>
</div>
</div>
<div id="sufficient-and-completeness" class="section level2">
<h2><span class="header-section-number">2.3</span> Sufficient and completeness</h2>
<div id="sufficiency-and-minimal-sufficiency" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Sufficiency and minimal sufficiency</h3>
</div>
<div id="neyman-factorization-theorem-minimal-sufficiency-of-mles" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Neyman factorization theorem, minimal sufficiency of MLEs</h3>
</div>
<div id="completeness-lehmann-scheffe-completeness-theorem" class="section level3">
<h3><span class="header-section-number">2.3.3</span> completeness, Lehmann-Scheffe completeness theorem</h3>
</div>
<div id="exponential-class-complete-sufficient-statistics" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Exponential class, complete sufficient statistics</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logit-and-probit.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/0000_2_556.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
