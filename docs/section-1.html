<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 556 | Basic Stats</title>
  <meta name="description" content="The webpages are mainly about Bayesian." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 556 | Basic Stats" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The webpages are mainly about Bayesian." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 556 | Basic Stats" />
  
  <meta name="twitter:description" content="The webpages are mainly about Bayesian." />
  

<meta name="author" content="Bill Last Updated:" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="section.html"/>
<link rel="next" href="section-2.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://williamdingpullman.github.io/" target="blank">Bill's Stats Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface: Motivation</a></li>
<li class="chapter" data-level="1" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>1</b> 443</a><ul>
<li class="chapter" data-level="1.1" data-path="section.html"><a href="section.html#some-basic-concepts"><i class="fa fa-check"></i><b>1.1</b> Some basic concepts</a><ul>
<li class="chapter" data-level="1.1.1" data-path="section.html"><a href="section.html#random-variable"><i class="fa fa-check"></i><b>1.1.1</b> Random variable</a></li>
<li class="chapter" data-level="1.1.2" data-path="section.html"><a href="section.html#permutation"><i class="fa fa-check"></i><b>1.1.2</b> Permutation</a></li>
<li class="chapter" data-level="1.1.3" data-path="section.html"><a href="section.html#combinations"><i class="fa fa-check"></i><b>1.1.3</b> Combinations</a></li>
<li class="chapter" data-level="1.1.4" data-path="section.html"><a href="section.html#partitioning"><i class="fa fa-check"></i><b>1.1.4</b> Partitioning</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="section.html"><a href="section.html#discrete-random-variables"><i class="fa fa-check"></i><b>1.2</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="1.2.1" data-path="section.html"><a href="section.html#binomial"><i class="fa fa-check"></i><b>1.2.1</b> Binomial</a></li>
<li class="chapter" data-level="1.2.2" data-path="section.html"><a href="section.html#poisson"><i class="fa fa-check"></i><b>1.2.2</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="section.html"><a href="section.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.3</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="1.3.1" data-path="section.html"><a href="section.html#uniform"><i class="fa fa-check"></i><b>1.3.1</b> Uniform</a></li>
<li class="chapter" data-level="1.3.2" data-path="section.html"><a href="section.html#exponential"><i class="fa fa-check"></i><b>1.3.2</b> Exponential</a></li>
<li class="chapter" data-level="1.3.3" data-path="section.html"><a href="section.html#normal"><i class="fa fa-check"></i><b>1.3.3</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="section.html"><a href="section.html#large-sample-theory"><i class="fa fa-check"></i><b>1.4</b> Large Sample Theory</a><ul>
<li class="chapter" data-level="1.4.1" data-path="section.html"><a href="section.html#convergence-in-distribution"><i class="fa fa-check"></i><b>1.4.1</b> Convergence in distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="section.html"><a href="section.html#weak-law"><i class="fa fa-check"></i><b>1.4.2</b> Weak law</a></li>
<li class="chapter" data-level="1.4.3" data-path="section.html"><a href="section.html#strong-law"><i class="fa fa-check"></i><b>1.4.3</b> Strong law</a></li>
<li class="chapter" data-level="1.4.4" data-path="section.html"><a href="section.html#central-limit-theorem"><i class="fa fa-check"></i><b>1.4.4</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.4.5" data-path="section.html"><a href="section.html#poisson-approximation-to-binomial"><i class="fa fa-check"></i><b>1.4.5</b> Poisson approximation to binomial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>2</b> 556</a><ul>
<li class="chapter" data-level="2.1" data-path="section-1.html"><a href="section-1.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>2.1</b> Statistics and Sampling Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="section-1.html"><a href="section-1.html#statistics"><i class="fa fa-check"></i><b>2.1.1</b> Statistics</a></li>
<li class="chapter" data-level="2.1.2" data-path="section-1.html"><a href="section-1.html#chi2-t-f-beta"><i class="fa fa-check"></i><b>2.1.2</b> <span class="math inline">\(\chi^2, t, F, beta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="section-1.html"><a href="section-1.html#large-sample-approximations"><i class="fa fa-check"></i><b>2.1.3</b> Large-sample approximations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="section-1.html"><a href="section-1.html#point-estimation"><i class="fa fa-check"></i><b>2.2</b> Point Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="section-1.html"><a href="section-1.html#method-of-moments-estimators"><i class="fa fa-check"></i><b>2.2.1</b> Method of moments estimators</a></li>
<li class="chapter" data-level="2.2.2" data-path="section-1.html"><a href="section-1.html#least-squares-estimators"><i class="fa fa-check"></i><b>2.2.2</b> least squares estimators</a></li>
<li class="chapter" data-level="2.2.3" data-path="section-1.html"><a href="section-1.html#likelihood-function-and-maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.2.3</b> likelihood function and maximum likelihood estimators</a></li>
<li class="chapter" data-level="2.2.4" data-path="section-1.html"><a href="section-1.html#invariance-property-of-mles"><i class="fa fa-check"></i><b>2.2.4</b> Invariance property of MLEs</a></li>
<li class="chapter" data-level="2.2.5" data-path="section-1.html"><a href="section-1.html#unbiased-estimators"><i class="fa fa-check"></i><b>2.2.5</b> Unbiased estimators</a></li>
<li class="chapter" data-level="2.2.6" data-path="section-1.html"><a href="section-1.html#unbiased-estimators-vs.-invariance-property-of-mles"><i class="fa fa-check"></i><b>2.2.6</b> Unbiased estimators vs. Invariance property of MLEs</a></li>
<li class="chapter" data-level="2.2.7" data-path="section-1.html"><a href="section-1.html#umvue-and-cramer-rao-lower-bound"><i class="fa fa-check"></i><b>2.2.7</b> UMVUE and Cramer-Rao lower bound</a></li>
<li class="chapter" data-level="2.2.8" data-path="section-1.html"><a href="section-1.html#best-linear-unbiased-estimation-blue-or-mvlue"><i class="fa fa-check"></i><b>2.2.8</b> Best linear unbiased estimation (BLUE or MVLUE)</a></li>
<li class="chapter" data-level="2.2.9" data-path="section-1.html"><a href="section-1.html#consistency-asymptotic-unbiasedness"><i class="fa fa-check"></i><b>2.2.9</b> Consistency, asymptotic unbiasedness</a></li>
<li class="chapter" data-level="2.2.10" data-path="section-1.html"><a href="section-1.html#efficiency"><i class="fa fa-check"></i><b>2.2.10</b> Efficiency</a></li>
<li class="chapter" data-level="2.2.11" data-path="section-1.html"><a href="section-1.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>2.2.11</b> Asymptotic efficiency</a></li>
<li class="chapter" data-level="2.2.12" data-path="section-1.html"><a href="section-1.html#asymptotic-properties-of-mles"><i class="fa fa-check"></i><b>2.2.12</b> Asymptotic properties of MLEs</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="section-1.html"><a href="section-1.html#sufficient-and-completeness"><i class="fa fa-check"></i><b>2.3</b> Sufficient and completeness</a><ul>
<li class="chapter" data-level="2.3.1" data-path="section-1.html"><a href="section-1.html#sufficiency-and-minimal-sufficiency"><i class="fa fa-check"></i><b>2.3.1</b> Sufficiency and minimal sufficiency</a></li>
<li class="chapter" data-level="2.3.2" data-path="section-1.html"><a href="section-1.html#neyman-factorization-theorem-minimal-sufficiency-of-mles"><i class="fa fa-check"></i><b>2.3.2</b> Neyman factorization theorem, minimal sufficiency of MLEs</a></li>
<li class="chapter" data-level="2.3.3" data-path="section-1.html"><a href="section-1.html#rao-blackwell-theorem"><i class="fa fa-check"></i><b>2.3.3</b> Rao-Blackwell theorem</a></li>
<li class="chapter" data-level="2.3.4" data-path="section-1.html"><a href="section-1.html#completeness"><i class="fa fa-check"></i><b>2.3.4</b> completeness</a></li>
<li class="chapter" data-level="2.3.5" data-path="section-1.html"><a href="section-1.html#lehmann-scheffe-completeness-theorem"><i class="fa fa-check"></i><b>2.3.5</b> Lehmann-Scheffe completeness theorem</a></li>
<li class="chapter" data-level="2.3.6" data-path="section-1.html"><a href="section-1.html#exponential-class-complete-sufficient-statistics"><i class="fa fa-check"></i><b>2.3.6</b> Exponential class, complete sufficient statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>3</b> 530_533</a><ul>
<li class="chapter" data-level="3.1" data-path="section-2.html"><a href="section-2.html#definition-of-the-general-linear-model"><i class="fa fa-check"></i><b>3.1</b> Definition of the general linear model</a></li>
<li class="chapter" data-level="3.2" data-path="section-2.html"><a href="section-2.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="section-2.html"><a href="section-2.html#least-squares-vs.-mle-vs-propoerties-of-the-regression-parameters"><i class="fa fa-check"></i><b>3.2.1</b> Least squares vs. MLE vs, propoerties of the regression parameters</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="section-2.html"><a href="section-2.html#full-rank-less-than-full-rank"><i class="fa fa-check"></i><b>3.3</b> full rank, less than full rank</a><ul>
<li class="chapter" data-level="3.3.1" data-path="section-2.html"><a href="section-2.html#assumptions-checking-assumptions"><i class="fa fa-check"></i><b>3.3.1</b> Assumptions, checking assumptions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="section-2.html"><a href="section-2.html#bootstrapping-used-in-linear-models"><i class="fa fa-check"></i><b>3.4</b> Bootstrapping used in linear models</a></li>
<li class="chapter" data-level="3.5" data-path="section-2.html"><a href="section-2.html#generalized-linear-models"><i class="fa fa-check"></i><b>3.5</b> Generalized linear models</a><ul>
<li class="chapter" data-level="3.5.1" data-path="section-2.html"><a href="section-2.html#definition-similarities-and-differences-from-general-linear-models"><i class="fa fa-check"></i><b>3.5.1</b> Definition, similarities and differences from general linear models</a></li>
<li class="chapter" data-level="3.5.2" data-path="section-2.html"><a href="section-2.html#advantage-and-disadvantages"><i class="fa fa-check"></i><b>3.5.2</b> Advantage and disadvantages</a></li>
<li class="chapter" data-level="3.5.3" data-path="section-2.html"><a href="section-2.html#logistic-and-poisson-regression"><i class="fa fa-check"></i><b>3.5.3</b> logistic and poisson regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>4</b> 512</a><ul>
<li class="chapter" data-level="4.1" data-path="section-3.html"><a href="section-3.html#completely-randomized-designs"><i class="fa fa-check"></i><b>4.1</b> Completely randomized designs</a></li>
<li class="chapter" data-level="4.2" data-path="section-3.html"><a href="section-3.html#randomized-complete-block-designs"><i class="fa fa-check"></i><b>4.2</b> Randomized complete block designs</a></li>
<li class="chapter" data-level="4.3" data-path="section-3.html"><a href="section-3.html#incomplete-block-designs"><i class="fa fa-check"></i><b>4.3</b> incomplete block designs</a></li>
<li class="chapter" data-level="4.4" data-path="section-3.html"><a href="section-3.html#split-plot-designs"><i class="fa fa-check"></i><b>4.4</b> Split-plot designs</a></li>
<li class="chapter" data-level="4.5" data-path="section-3.html"><a href="section-3.html#row-column-design-latin-square-designs"><i class="fa fa-check"></i><b>4.5</b> Row-column design (Latin square designs)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logit-and-probit.html"><a href="logit-and-probit.html"><i class="fa fa-check"></i><b>5</b> Logit and Probit</a><ul>
<li class="chapter" data-level="5.1" data-path="logit-and-probit.html"><a href="logit-and-probit.html#logit"><i class="fa fa-check"></i><b>5.1</b> Logit</a></li>
<li class="chapter" data-level="5.2" data-path="logit-and-probit.html"><a href="logit-and-probit.html#probit"><i class="fa fa-check"></i><b>5.2</b> Probit</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="normal-distribution.html"><a href="normal-distribution.html"><i class="fa fa-check"></i><b>6</b> Normal distribution</a><ul>
<li class="chapter" data-level="6.1" data-path="normal-distribution.html"><a href="normal-distribution.html#basics"><i class="fa fa-check"></i><b>6.1</b> Basics</a></li>
<li class="chapter" data-level="6.2" data-path="normal-distribution.html"><a href="normal-distribution.html#confidence-intervals-for-normal-distributions"><i class="fa fa-check"></i><b>6.2</b> Confidence intervals for normal distributions</a></li>
<li class="chapter" data-level="6.3" data-path="normal-distribution.html"><a href="normal-distribution.html#percentile"><i class="fa fa-check"></i><b>6.3</b> Percentile</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>7</b> MLE</a><ul>
<li class="chapter" data-level="7.1" data-path="intro.html"><a href="intro.html#basic-idea-of-mle"><i class="fa fa-check"></i><b>7.1</b> Basic idea of MLE</a></li>
<li class="chapter" data-level="7.2" data-path="intro.html"><a href="intro.html#coin-flip-example-probit-and-logit"><i class="fa fa-check"></i><b>7.2</b> Coin flip example, probit, and logit</a><ul>
<li class="chapter" data-level="7.2.1" data-path="intro.html"><a href="intro.html#probit-1"><i class="fa fa-check"></i><b>7.2.1</b> Probit</a></li>
<li class="chapter" data-level="7.2.2" data-path="intro.html"><a href="intro.html#logit-1"><i class="fa fa-check"></i><b>7.2.2</b> Logit</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="intro.html"><a href="intro.html#further-on-logit"><i class="fa fa-check"></i><b>7.3</b> Further on logit</a></li>
<li class="chapter" data-level="7.4" data-path="intro.html"><a href="intro.html#references"><i class="fa fa-check"></i><b>7.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html"><i class="fa fa-check"></i><b>8</b> Score, Gradient and Jacobian</a><ul>
<li class="chapter" data-level="8.1" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#score"><i class="fa fa-check"></i><b>8.1</b> Score</a></li>
<li class="chapter" data-level="8.2" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#fisher-scoring"><i class="fa fa-check"></i><b>8.2</b> Fisher scoring</a></li>
<li class="chapter" data-level="8.3" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#gradient-and-jacobian"><i class="fa fa-check"></i><b>8.3</b> Gradient and Jacobian</a></li>
<li class="chapter" data-level="8.4" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#hessian-and-fisher-information"><i class="fa fa-check"></i><b>8.4</b> Hessian and Fisher Information</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="canonical-link-function.html"><a href="canonical-link-function.html"><i class="fa fa-check"></i><b>9</b> Canonical link function</a></li>
<li class="chapter" data-level="10" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>10</b> Ordinary Least Squares (OLS)</a><ul>
<li class="chapter" data-level="10.1" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#taylor-series"><i class="fa fa-check"></i><b>10.1</b> Taylor series</a></li>
<li class="chapter" data-level="10.2" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#references-1"><i class="fa fa-check"></i><b>10.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html"><i class="fa fa-check"></i><b>11</b> Cholesky decomposition</a><ul>
<li class="chapter" data-level="11.1" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-1"><i class="fa fa-check"></i><b>11.1</b> Example 1</a></li>
<li class="chapter" data-level="11.2" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-2"><i class="fa fa-check"></i><b>11.2</b> Example 2</a></li>
<li class="chapter" data-level="11.3" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-3"><i class="fa fa-check"></i><b>11.3</b> Example 3</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://www.williamsding.com/" target="blank">Bill's website</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basic Stats</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-1" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> 556</h1>
<div id="statistics-and-sampling-distributions" class="section level2">
<h2><span class="header-section-number">2.1</span> Statistics and Sampling Distributions</h2>
<div id="statistics" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Statistics</h3>
<div id="definition-of-statistic" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Definition of Statistic</h4>
<p><em>P.264</em></p>
<p>A fucntion of observable random variables, <span class="math inline">\(T=t(X_1, ... X_n)\)</span>, which does not depend on any unknown parameters is called statistic.</p>
<p>For example, let <span class="math inline">\(X_1, ..., X_n\)</span> represent a random sample from a population with <span class="math inline">\(pdf \;f(x)\)</span>. The sample mean provides an example of a statistic with the function</p>
<p><span class="math display">\[t(x_1,...,x_n)=(x_1+...+x_n)/n\]</span></p>
<p>This statistic usually is denoted by</p>
<p><span class="math display">\[\bar{X}=\sum_{i=1}^n \frac{X_i}{n}\]</span></p>
<p>When a random sample is observed, the value of <span class="math inline">\(\bar{X}\)</span>, computed from the data, usually is denoted by lower case <span class="math inline">\(\bar{x}\)</span>.</p>
<p><span class="math display">\[\bar{x}=\sum_{i=1}^n \frac{x_i}{n}\]</span></p>
</div>
<div id="sample-and-parameters" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> Sample and parameters</h4>
<p><em>P.265</em></p>
<p>If <span class="math inline">\(X_1,..., X_n\)</span> denotes a random sample from <span class="math inline">\(f(x)\)</span> with <span class="math inline">\(E(X)=\mu\)</span> and <span class="math inline">\(var(X)=\sigma^2\)</span>, then</p>
<p><span class="math display">\[E(\bar{X})=\mu\]</span></p>
<p><span class="math display">\[Var(\bar{X})=\frac{\sigma^2}{n}\]</span></p>
<p><strong>Example:</strong></p>
<p>A random sample of size <span class="math inline">\(n\)</span> from a Bernoulli distribution <span class="math inline">\(X_i \sim BIN(1,p)\)</span>. We know Bernoulli has <span class="math inline">\(\mu=p\)</span> and <span class="math inline">\(\sigma^2 =pq\)</span>. In this case, the sample mean is</p>
<p><span class="math display">\[\bar{X}=Y/n=\hat{p}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E(\hat{p})=p\]</span></p>
<p><span class="math display">\[Var (\hat{p})=\frac{pq}{n}\]</span></p>
<p>Thus, sample mean is the unbiased estimate for the population mean. However, the variance of the mean is not equal to population variance. That lead to definition of sample variance.</p>
<p><em>P.266</em></p>
<p>Sample variance:</p>
<p><span class="math display">\[S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2\]</span></p>
<p><span class="math display">\[E(S^2)=\sigma^2\]</span></p>
<p>(Question: does it mean that sample varaince is an unbiased estimator of population variance?)</p>
</div>
</div>
<div id="chi2-t-f-beta" class="section level3">
<h3><span class="header-section-number">2.1.2</span> <span class="math inline">\(\chi^2, t, F, beta\)</span></h3>
<div id="chi2" class="section level4">
<h4><span class="header-section-number">2.1.2.1</span> <span class="math inline">\(\chi^2\)</span></h4>
<p><em>P.271</em></p>
<p>If <span class="math inline">\(X_1,...,X_n\)</span> denotes a random sample from <span class="math inline">\(N(\mu,\sigma^2)\)</span>, then</p>
<p><span class="math display">\[\sum_{i=1}^n \frac{(X_i-\mu)^2}{\sigma^2} \sim x^2(n)\]</span>
<span class="math display">\[\frac{n(\bar{X}-\mu)^2}{\sigma^2}\sim x^2(1)\]</span></p>
<p><span class="math display">\[\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2(n-1)\]</span>
(Thus, we can see this is a bit weired, as the numerator is <span class="math inline">\(\bar{X}\)</span> is from the sample, whereas <span class="math inline">\(\sigma^2\)</span> is from the population. Thus, assume we know <span class="math inline">\(\sigma^2\)</span>?)</p>
<p>Thus, we can</p>
<p><span class="math display">\[\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sigma^2}\sim \chi^2(n-1)\]</span>
(You can compare <span class="math inline">\(\bar{X}\)</span> with <span class="math inline">\(\mu\)</span>, we can see the only difference is that the <span class="math inline">\(\chi^2\)</span> has one less degree of freedom because we use this degree of freedom to calculate the mean.)</p>
<p>For the mean and variance of <span class="math inline">\(\chi^2\)</span>:</p>
<p>Assume that</p>
<p><span class="math display">\[X \sim x^2(v)\]</span></p>
<p><span class="math display">\[mean:v\]</span>
<span class="math display">\[variance: 2v\]</span></p>
</div>
<div id="t" class="section level4">
<h4><span class="header-section-number">2.1.2.2</span> <span class="math inline">\(t\)</span></h4>
<p>Definition</p>
<p><span class="math display">\[t(k)=\frac{N(0,1)}{\sqrt{\frac{\chi^2(k)}{k}}}\]</span></p>
<p><strong>Property 1</strong></p>
<p><em>t</em> distribution is symmetrical.</p>
<p>Given that <em>t</em> distribution is symmetrical, we can get</p>
<p><span class="math display">\[H(-c)=1-H(c)\]</span></p>
<p><strong>Property 2</strong></p>
<p><em>t</em> distribution has heavier tails than the normal.</p>
<p>My note: <em>t</em> distribution only has a parameter of <span class="math inline">\(k\)</span>, which is determined by the <span class="math inline">\(\chi^2\)</span>’s degree of freedom. Of course, <span class="math inline">\(\chi^2\)</span> also only has one parameter, namely the degree of freedom.</p>
</div>
<div id="f" class="section level4">
<h4><span class="header-section-number">2.1.2.3</span> <span class="math inline">\(F\)</span></h4>
<p>If <span class="math inline">\(V_1 \sim \chi^2(v_1)\)</span> and <span class="math inline">\(V_2 \sim \chi^2(v_2)\)</span> are independent, then the random variable</p>
<p><span class="math display">\[\frac{V_1/v_1}{V_2/v_2}\sim F(v_1,v_2)\]</span></p>
</div>
<div id="beta" class="section level4">
<h4><span class="header-section-number">2.1.2.4</span> Beta</h4>
<p>If <span class="math inline">\(X \sim F(v_1, v_2)\)</span></p>
<p><span class="math display">\[Y=\frac{(v_1/v_2)X}{1+(v_1/v_2)X} \sim Beta(\alpha, \beta)\]</span></p>
</div>
</div>
<div id="large-sample-approximations" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Large-sample approximations</h3>
<p><em>P.280</em></p>
<p>If <span class="math inline">\(Y_v \sim x^2(x)\)</span>, then</p>
<p><span class="math display">\[Z_v =\frac{Y_v-v}{\sqrt{2v}} \xrightarrow{d} Z \sim N(0,1)\]</span></p>
<p>(The proof is based on CLT. In addition, as disscused above, we can get chi-square from normal distributions.)</p>
</div>
</div>
<div id="point-estimation" class="section level2">
<h2><span class="header-section-number">2.2</span> Point Estimation</h2>
<div id="method-of-moments-estimators" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Method of moments estimators</h3>
<div id="definition-about-moments-chapter-2" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> Definition about moments (chapter 2)</h4>
<p><em>P.73</em></p>
<p>THe <strong>kth moment about the origin</strong> of a random variable <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[\mu^{&#39;}_k=E(X^k)\]</span></p>
<p>and the <strong>kth moment about the mean</strong> is</p>
<p><span class="math display">\[\mu_k=E[X-E(X)]^k=E(X-\mu)^k\]</span></p>
<p>Thus,<span class="math inline">\(k=E(X^k)\)</span> may be considered as the <span class="math inline">\(k\)</span>th moment of <span class="math inline">\(X\)</span> or the first moment of <span class="math inline">\(X^k\)</span>.</p>
<p>The first moment about the mean is zero,</p>
<p><span class="math display">\[\mu_1=E[X-E(X)]=E(X)-E(X)=0\]</span></p>
<p>The second moment about the mean is the variance,</p>
<p><span class="math display">\[\mu_2=E[(X-\mu)^2]=\sigma^2\]</span></p>
<p>Note that the definition of variance:</p>
<p><em>P.73</em></p>
<p><span class="math display">\[Var(X)=E[(X-\mu)^2]\]</span></p>
<p>( <strong>Note: Based on the information above, it seems important to understand the difference between the moment of the origin and the moment of the mean.</strong> )</p>
</div>
<div id="definition" class="section level4">
<h4><span class="header-section-number">2.2.1.2</span> Definition</h4>
<p>Based on the last chapater (i.e., Chapter 8), sample mean <span class="math inline">\(\bar{X}\)</span> is an estimator of the population mean <span class="math inline">\(\mu\)</span>. A more general approach, which produced estimators known as the <strong>method of moments estimators(MMEs)</strong> , can be developed.</p>
<p>If <span class="math inline">\(X_1,...,X_n\)</span> is a random sample from <span class="math inline">\(f(x; \theta_1,...,\theta_k)\)</span>, the first <span class="math inline">\(k\)</span> sample moments are given by</p>
<p><span class="math display">\[M_j^{&#39;}=\frac{\sum_{i=1}^n X_i^j}{n}\]</span></p>
<p>where,</p>
<p><span class="math display">\[j=1,2,...k\]</span></p>
<p><strong>Example 1:</strong></p>
<p><em>P.291</em></p>
<p>Consider a random sample from a distribution with two unknown parameters, the mean <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span>. We know from earlier considerations that <span class="math inline">\(\mu=\mu^{&#39;}_1\)</span> and <span class="math inline">\(\sigma^2=E(X^2)-\mu^2=\mu_2^{&#39;}-(\mu^{&#39;}_1)^2\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[\hat{\sigma}^2=\mu_2^{&#39;}-(\mu^{&#39;}_1)^2=\frac{\sum_{i=1}^n X_i^2}{n}-\bar{X}^2=\sum_{i=1}^n \frac{(X_i-\bar{X})^2}{n}\]</span></p>
<p>(Thus, we can see that the MME estimation of <span class="math inline">\(\sigma^2\)</span> is not the same as the definition of sample variance <span class="math inline">\(S^2\)</span>. <span class="math inline">\(\hat{\sigma}^2=\frac{n-1}{n}S^2\)</span>. So, the MME estimator is not an unbiased one?)</p>
<p><strong>Example 2:</strong></p>
<p><em>P.292</em></p>
<p>If a sample is from a Gamma distribution <span class="math inline">\(X_i \sim GAM(\theta,k)\)</span>, and we want to estimate the <span class="math inline">\(\theta\)</span> and <span class="math inline">\(k\)</span>.</p>
<p>We know that for Gamma distribution, the mean is <span class="math inline">\(k\theta\)</span>, and the variance is <span class="math inline">\(k\theta^2\)</span>.</p>
<p>We also know that <span class="math inline">\(\mu_1^{&#39;}=\mu=k\theta\)</span> and <span class="math inline">\(\mu_2^{&#39;}= \sigma^2+\mu^2= k\theta^2+k^2\theta^2=k\theta^2(1+k)\)</span>.</p>
<p>Thus, we can get</p>
<p><span class="math display">\[\bar{X}=k\theta\]</span></p>
<p><span class="math display">\[\sum \frac{X_i^2}{n}=k\theta^2(1+k)\]</span></p>
<p>Thus, we can get</p>
<p><span class="math display">\[\hat{\theta}=\sum_{i=1}^n \frac{(X_i-\bar{X})^2}{n \bar{X}}=\frac{(n-1)/n S^2}{\bar{X}}\]</span></p>
<p><span class="math display">\[\hat{k}=\frac{\bar{X}}{\hat{\theta}}\]</span></p>
<p>( <strong>Note: To sum up, (1) we know how to calculate moments based on sample. And, (2) we know that connection between moment and parameter of mean and variance. Thus, combining (1) and (2), we can get the estimators for the parameters.</strong> )</p>
</div>
<div id="property" class="section level4">
<h4><span class="header-section-number">2.2.1.3</span> Property</h4>
<p>The joint MGF of (<span class="math inline">\(X_1, ..., X_n\)</span>) is defined as <span class="math inline">\(M(t_1,...,t_n)=E(e^{\sum_{i=1}^nt_iX_i})\)</span></p>
<p>When <span class="math inline">\(X_1, ..., X_n\)</span> are independent if and only if</p>
<p><span class="math display">\[M(t_1,...,t_n)=\prod_{i=1}^n M_{X_i}(t_i)\]</span>
where <span class="math inline">\(M_{X_i}(t_i)\)</span> is the MGF of <span class="math inline">\(X_i\)</span></p>
</div>
<div id="well-known-mgf" class="section level4">
<h4><span class="header-section-number">2.2.1.4</span> Well-known MGF</h4>
<ol style="list-style-type: decimal">
<li><p>Bernoulli with success probability <span class="math inline">\(p\)</span>: <span class="math inline">\(1-p+pe^t\)</span></p></li>
<li><p>Binomial <span class="math inline">\(Bin(n,p)\)</span>: <span class="math inline">\((1-p+pe^t)^n\)</span></p></li>
<li><p>Poisson <span class="math inline">\(POI(\lambda)\)</span>: <span class="math inline">\(e^{\lambda(e^t-1)}\)</span></p></li>
<li><p>Normal <span class="math inline">\(N(\mu,\sigma^2)\)</span>: <span class="math inline">\(e^{\mu t+\frac{1}{2}\sigma^2t^2}\)</span></p></li>
<li><p>Gamma <span class="math inline">\(GAM(\theta,k)\)</span>: <span class="math inline">\((1-\theta t)^{-k}\)</span></p></li>
</ol>
<p><strong>Two special cases:</strong></p>
<ol start="6" style="list-style-type: decimal">
<li><p>Chi-square <span class="math inline">\(\chi^2(v) =GAM(2,\frac{v}{2})\)</span>: <span class="math inline">\((1-2t)^{-\frac{v}{2}}\)</span></p></li>
<li><p>Exponential <span class="math inline">\(EXP(\theta)=GAM(\theta,1)\)</span>: <span class="math inline">\((1-\theta t)^{-1}\)</span></p></li>
</ol>
</div>
</div>
<div id="least-squares-estimators" class="section level3">
<h3><span class="header-section-number">2.2.2</span> least squares estimators</h3>
</div>
<div id="likelihood-function-and-maximum-likelihood-estimators" class="section level3">
<h3><span class="header-section-number">2.2.3</span> likelihood function and maximum likelihood estimators</h3>
<div id="likelihood-function" class="section level4">
<h4><span class="header-section-number">2.2.3.1</span> Likelihood function</h4>
<p><em>P.293</em></p>
<p>The joint density function of <span class="math inline">\(n\)</span> random variables <span class="math inline">\(X_1,...X_n\)</span> evaluated at <span class="math inline">\(x_1, ...x_n\)</span>, say <span class="math inline">\(f(x_1,...,x_n; \theta)\)</span>, is referered to as a <em>likelihood function</em>.</p>
</div>
<div id="maximum-likelihood-estimators" class="section level4">
<h4><span class="header-section-number">2.2.3.2</span> Maximum likelihood estimators</h4>
<p><em>P.294</em></p>
<p>Let <span class="math inline">\(L(\theta)=f(x_1,...x_n; \theta), \theta \in \Omega\)</span>, be the joint pdf of <span class="math inline">\(X_1, ..., X_n\)</span>. For a given set of observations, <span class="math inline">\((x_1,...x_n)\)</span>, a value <span class="math inline">\(\hat{\theta}\)</span> in <span class="math inline">\(\Omega\)</span> at which <span class="math inline">\(L(\theta)\)</span> is a maximum is called a <em>maximum likelihood estimate (MLE)</em> of <span class="math inline">\(\theta\)</span>. That is <span class="math inline">\(\hat{\theta}\)</span> is a value of <span class="math inline">\(\theta\)</span> that satisfies</p>
<p><span class="math display">\[f(x_1,...x_n; \theta)=MAX_{\theta \in \Omega} f(x_1, ..., x_n; \theta)\]</span></p>
</div>
</div>
<div id="invariance-property-of-mles" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Invariance property of MLEs</h3>
<p><em>P.296</em></p>
<p>If <span class="math inline">\(\hat{\theta}\)</span> is the MLE of <span class="math inline">\(\theta\)</span> and if <span class="math inline">\(u(\theta)\)</span> is a function of <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(u(\hat{\theta})\)</span> is an MLE of <span class="math inline">\(u(\theta)\)</span>.</p>
<p><strong>Example 1:</strong></p>
<p>We know that the <span class="math inline">\(pdf\)</span> of exponential distribution (<span class="math inline">\(X \sim EXP (\theta)\)</span>) is as follows:</p>
<p><span class="math display">\[\frac{1}{\theta} e^{-\frac{X}{\theta}}\]</span></p>
<p>Thus, its likelihood function is as follows</p>
<p><span class="math display">\[L(\theta)=\frac{1}{\theta^n}e^{-\frac{\sum X_i}{\theta}}\]</span>
Thus, log-likelihood is as follows.</p>
<p><span class="math display">\[lnL(\theta)=-n ln(\theta)-\frac{\sum X_i}{\theta}\]</span>
Thus,</p>
<p><span class="math display">\[\frac{d}{d\theta} lnL(\theta)=-n \frac{1}{\theta}+\frac{\sum X_i}{\theta^2}\]</span></p>
<p>Thus, we can get the <span class="math inline">\(MLE\)</span> for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta}=\bar{x}\)</span>.</p>
<p>If we want to estimate <span class="math inline">\(\tau(\theta)=P(X \geq 1)\)</span>:</p>
<p><span class="math display">\[\tau(\theta)=P(X\geq 1)=\int_1^{\infty} \frac{1}{\theta} e^{-\frac{X}{\theta}} dx=-\int_1^{\infty}  e^{-\frac{X}{\theta}} d(-\frac{x}{\theta})=-[e^{-\frac{X}{\theta}}]_1^{\infty}=-[0-e^{-\frac{1}{\theta}}]=e^{-\frac{1}{\theta}}\]</span></p>
<p>Thus, based on the invariance property, we know that the <span class="math inline">\(MLE\)</span> for <span class="math inline">\(\tau(\theta)\)</span> is as follows.</p>
<p><span class="math display">\[e^{-\frac{1}{\bar{x}}}\]</span></p>
<p><strong>Example 2: MLE vs. MME</strong></p>
<p><em>P.296</em></p>
<p>Assume a random sample from a two-parameter exponential distribution, <span class="math inline">\(X_i \sim EXP(1,\eta)\)</span>. Thus, the <span class="math inline">\(pdf\)</span> is <span class="math inline">\(e^{-(x-\eta)}\)</span>. Thus, the likelihood function is</p>
<p><span class="math display">\[L(\eta)=e^{-\sum(x_i-\eta)}\]</span></p>
<p>Thus, the log likelihood,</p>
<p><span class="math display">\[lnL(\eta)=-\sum(x_i-\eta)=n\eta-n\bar{X}\]</span>
Thus, we know that as <span class="math inline">\(\eta\)</span> increases, the log likelihood increases accordingly. Thus, we want to find the maximum <span class="math inline">\(\eta\)</span>. Note that a two-parameter exponential distribution has the support of <span class="math inline">\(x_i \geq \eta\)</span>. Thus, all <span class="math inline">\(\eta\)</span> are smaller than any <span class="math inline">\(X_i\)</span>. Thus, we can get the ML estimator is the first order statistic</p>
<p><span class="math display">\[\hat{\eta}=X_{1:n}\]</span></p>
<p><strong>Note that</strong> the estimators above is based on ML. What would be the answer if using MME?</p>
<p>We know that for a two-parameter exponential distribution, its mean is <span class="math inline">\(\mu=1+\eta\)</span>. And, we know that based on MME, <span class="math inline">\(\mu=\bar{X}\)</span>. Thus, we can get the following,</p>
<p><span class="math display">\[\hat{\eta}=\bar{X}-1\]</span></p>
<p><strong>Conclusion</strong></p>
<p>We can see that ML and MME have didferent estimators for the same <span class="math inline">\(\eta\)</span> for a two-parameter exponential distribution.</p>
</div>
<div id="unbiased-estimators" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Unbiased estimators</h3>
<p>An estimator <span class="math inline">\(T\)</span> is said to be an unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span> if</p>
<p><span class="math display">\[E(T)=\tau(\theta)\]</span></p>
<p>for all <span class="math inline">\(\theta \in \Omega\)</span>. Otherwise, we said that <span class="math inline">\(T\)</span> is biased stimator of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p>For instance, if we want to estimate a percentile, say the 95th percentile of <span class="math inline">\(N(\mu,9)\)</span>. Note that the percentiles that we know are about standardized noraml (i.e., <span class="math inline">\(N(0,1)\)</span>). Thus, we need to have some calculation to get the non-standard one.</p>
<p><span class="math display">\[\frac{X_{95 \; percentile}-\mu}{\sigma}=1.645\]</span>
Thus, we can get</p>
<p><span class="math display">\[X_{95 \; percentile}=1.645 \times \sigma +\mu\]</span></p>
<p>We know that <span class="math inline">\(\bar{X}\)</span> is the unbiased estimate for <span class="math inline">\(\mu\)</span>. Thus, we can get</p>
<p><span class="math display">\[X_{95 \; percentile}=1.645 \times \sigma +\mu=4.94+\mu\]</span></p>
<p>We know that</p>
<p><span class="math display">\[E(T)=E(\bar{X}+4.94)=\mu+4.94\]</span>
Thus, <span class="math inline">\(T=\bar{X}+4.94\)</span> is the unbiased estimator of <span class="math inline">\(\tau(\mu)=\mu+4.94\)</span>.</p>
</div>
<div id="unbiased-estimators-vs.-invariance-property-of-mles" class="section level3">
<h3><span class="header-section-number">2.2.6</span> Unbiased estimators vs. Invariance property of MLEs</h3>
<p><strong>Do not apply “Invariance property of MLEs” to the Unbiased estimators.</strong></p>
<p>( <strong>Note that:</strong> You can apply the Invariance property to “Unbiased estimators” when it is a linear combination. In that case, <span class="math inline">\(E(a\theta+b)=aE(\theta)+b\)</span>. Thus, if you find a <span class="math inline">\(T\)</span> that is a unbiased estimator for <span class="math inline">\(\theta\)</span>, it should be unbiased estimator for <span class="math inline">\(a\theta+b\)</span> as well. Thus, note that <span class="math inline">\(\frac{1}{\theta}\)</span> is not a linear combination of <span class="math inline">\(\theta\)</span>, thus <span class="math inline">\(\frac{1}{\theta}\)</span> has a very different estimator, compared to <span class="math inline">\(\theta\)</span>.)</p>
<p><em>P.303</em></p>
<p>For example, consider a random sample of size <span class="math inline">\(n\)</span> from an exponential distribution, <span class="math inline">\(X_i \sim EXP(\theta)\)</span>, with parameter <span class="math inline">\(\theta\)</span>. We know that, <span class="math inline">\(\bar{X}\)</span> is unbiased for <span class="math inline">\(\theta\)</span> (which is the mean of an exponential distribution).</p>
<p>If we want to estimate <span class="math inline">\(\tau(\theta)=\frac{1}{\theta}\)</span>, then by the invariance property of MLE is <span class="math inline">\(T_1=\frac{1}{\bar{X}}\)</span>.</p>
<p>However, <span class="math inline">\(T_1\)</span> is a biased estimators of <span class="math inline">\(\frac{1}{\theta}\)</span>. Specifically,</p>
<p>We know that if <span class="math inline">\(X \sim Gam(\theta,k)\)</span>, then <span class="math inline">\(Y=\frac{2X}{\theta} \sim x^2(2k)\)</span>. We know that exponential distributions are a specicial case of Gamma distribution, <span class="math inline">\(EXP(\theta)=Gam(\theta,1)\)</span>, thus we get the following,</p>
<p><span class="math display">\[Y=\frac{2n\bar{X}}{\theta}=\frac{2n}{\theta} \frac{\sum_{i=1}^n X_i}{n}=\frac{\sum_{i=1}^n 2X_i}{\theta}\sim\sum_{i=1}^n x^2(2\cdot1)=\sum_{i=1}^n x^2(2)=x^2(2n)\]</span></p>
<p>We further know that if <span class="math inline">\(Y \sim x^2(v)\)</span>, <span class="math inline">\(E(Y^r)=2^r\frac{\Gamma(v/2+r)}{\Gamma(v/2)}\)</span>. Thus, we know that,</p>
<p><span class="math display">\[E(Y^{-1})=2^{-1} \frac{\Gamma(2n/2-1)}{\Gamma(2n/2)}=\frac{1}{2}\cdot \frac{1}{n-1}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E(Y^{-1})=E(\frac{\theta}{2n \bar{X}})=\frac{\theta}{2n}E(\frac{1}{\bar{X}})=\frac{1}{2}\cdot \frac{1}{n-1}\]</span></p>
<p>Thus.</p>
<p><span class="math display">\[E(\frac{1}{\bar{X}})=\frac{1}{n-1}\frac{n}{\theta}=\frac{n}{n-1}\frac{1}{\theta}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E(\frac{n-1}{n} \frac{1}{\bar{X}})=\frac{1}{\theta}\]</span></p>
<p><strong>Conclusion:</strong></p>
<p><span class="math inline">\(\frac{1}{\bar{X}}\)</span> is not the unbiased estimator for <span class="math inline">\(\frac{1}{\theta}\)</span>. However, we can adjust it to <span class="math inline">\(\frac{n-1}{n} \frac{1}{\bar{X}}\)</span>, which is the unbiased estimator for <span class="math inline">\(\frac{1}{\theta}\)</span>. When the sample size is big enough, we know that <span class="math inline">\(\frac{n-1}{n}\)</span> will be close to 1.</p>
</div>
<div id="umvue-and-cramer-rao-lower-bound" class="section level3">
<h3><span class="header-section-number">2.2.7</span> UMVUE and Cramer-Rao lower bound</h3>
<div id="umvue" class="section level4">
<h4><span class="header-section-number">2.2.7.1</span> UMVUE</h4>
<p>Let <span class="math inline">\(X_1, X_2,...,X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(f(x; \theta)\)</span>. An estimator <span class="math inline">\(T^*\)</span> of <span class="math inline">\(\tau (\theta)\)</span> is called a <em>uniformly minimum variance ubiased estimator</em> (UMVUE) of <span class="math inline">\(\tau(\theta)\)</span> if</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(T^*\)</span> is unbiased for <span class="math inline">\(\tau (\theta)\)</span>.</p></li>
<li><p>For any other unbiased estimator <span class="math inline">\(T\)</span> of <span class="math inline">\(\tau(\theta)\)</span>, <span class="math inline">\(Var(T^*) \leq Var(T)\)</span> for all <span class="math inline">\(\theta \in \Omega\)</span>.</p></li>
</ol>
</div>
<div id="cramer-rao-lower-bound" class="section level4">
<h4><span class="header-section-number">2.2.7.2</span> Cramer-Rao lower bound</h4>
<p>If <span class="math inline">\(T\)</span> is an unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>, then the Cramer-Rao lower bound (CRLB), based on a random sample, is</p>
<p><span class="math display">\[Var(T)=\frac{[\tau^{&#39;}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}\]</span></p>
<p><strong>Example:</strong></p>
<p>Consider a random sample from an exponential distribution, <span class="math inline">\(X_i \sim Exp(\theta)\)</span>. Because</p>
<p><span class="math display">\[f(x; \theta)=\frac{1}{\theta} e^{-\frac{x}{\theta}}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[ln(f(x;\theta))=-\frac{x}{\theta}-ln \theta\]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial \theta} ln(f(X; \theta))=\frac{X}{\theta^2}-\frac{1}{\theta}=\frac{X-\theta}{\theta^2}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E[\frac{\partial}{\partial \theta} ln(f(X; \theta))]^2 =E[\frac{(X-\theta)^2}{\theta^4}]=\frac{1}{\theta^4}E(X-\theta)^2=\frac{1}{\theta^4} Var(X)=\frac{\theta^2}{\theta^4}=\frac{1}{\theta^2}\]</span></p>
<p>Thus, the CRLB for <span class="math inline">\(\tau(\theta)=\theta\)</span> is as follows.</p>
<p><span class="math display">\[Var(T) \geq \frac{[\tau^{&#39;}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}=\frac{[\frac{\partial }{\partial \theta}\theta]^2}{n \frac{1}{\theta^2}}=\frac{1^2}{\frac{n}{\theta^2}}=\frac{\theta^2}{n}\]</span></p>
<p>We know that the variance for the sample mean of the exponential distribution:</p>
<p><span class="math display">\[Var(\bar{X})=\frac{Var(X)}{n}=\frac{\theta^2}{n}\]</span></p>
<p>In addition, we also know that sample mean <span class="math inline">\(\bar{X}\)</span> is the unbiased estimator for population mean.</p>
<p>Thus,</p>
<p><span class="math inline">\(\bar{X}\)</span> is the UMVUE of <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<div id="best-linear-unbiased-estimation-blue-or-mvlue" class="section level3">
<h3><span class="header-section-number">2.2.8</span> Best linear unbiased estimation (BLUE or MVLUE)</h3>
</div>
<div id="consistency-asymptotic-unbiasedness" class="section level3">
<h3><span class="header-section-number">2.2.9</span> Consistency, asymptotic unbiasedness</h3>
<p><strong>Simple consistency:</strong></p>
<p><em>P.311</em></p>
<p>Let <span class="math inline">\(\{T_n\}\)</span> be a sequence of estimators of <span class="math inline">\(\tau(\theta)\)</span>. These estimators are said to be <strong>consistent</strong> estimators of <span class="math inline">\(\tau(\theta)\)</span> if for every <span class="math inline">\(\varepsilon &gt;0\)</span>,</p>
<p><span class="math display">\[lim_{n\rightarrow \infty}P[|T_n-\tau(\theta)|&lt;\varepsilon]=1\]</span></p>
<p>for every <span class="math inline">\(\theta \in \Omega\)</span>.</p>
<p>In the terminology of Chapter 7, <span class="math inline">\(T_n\)</span> converges <strong>stochastically</strong> to <span class="math inline">\(\tau(\theta)\)</span>, <span class="math inline">\(T_n \xrightarrow{P} \tau(\theta)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. Sometimes this also is referred to as <strong>simple</strong> consistency.</p>
<p>One interpretation of consistency is that for large sample size the estimator tends to be more concentrated about <span class="math inline">\(\tau(\theta)\)</span>, and by making <span class="math inline">\(n\)</span> sufficiently large <span class="math inline">\(T_n\)</span> can be made as concentrated as desired.</p>
<p><strong>MSE consistency:</strong></p>
<p>If <span class="math inline">\(\{ T_n \}\)</span> is a sequence of estimator of <span class="math inline">\(\tau(\theta)\)</span>, then they are called <strong>mean squared error consistent</strong> if</p>
<p><span class="math display">\[lim_{n\rightarrow \infty} E[T_n - \tau(\theta)]^2=0\]</span></p>
<p>for every <span class="math inline">\(\theta \in \Omega\)</span>.</p>
<p><strong>Asymptotic Unbiased</strong></p>
<p>A sequence <span class="math inline">\(\{ T_n \}\)</span> is said to be <strong>asymptotically unbiased</strong> for <span class="math inline">\(\tau(\theta)\)</span> if</p>
<p><span class="math display">\[lim_{n\rightarrow \infty} E(T_n)=\tau(\theta)\]</span></p>
<p>for every <span class="math inline">\(\theta \in \Omega\)</span>.</p>
<p><strong>Example:</strong></p>
<p><em>P.313</em></p>
<p>For a sample from <span class="math inline">\(X_i \sim EXP(\theta)\)</span>, we know that <span class="math inline">\(T_n=1/ \bar{X}\)</span> is an MLE estimator for <span class="math inline">\(\tau(\theta)=1/\theta\)</span>. Howwever, <span class="math inline">\(T_n\)</span> is not a unbiased estimator for <span class="math inline">\(\tau(\theta)\)</span>, as</p>
<p><span class="math display">\[E(T_n)=\frac{n}{n-1}\cdot \frac{1}{\theta}\]</span></p>
<p>We also know that</p>
<p><span class="math display">\[Var(T_n)=\frac{(\frac{n}{n-1})^2}{(n-2)\theta^2}\]</span></p>
<p>Thus, while <span class="math inline">\(T_n\)</span> is not unbiased, it is asymptotically unbiased and MSE consistent for <span class="math inline">\(\tau(\theta)=\frac{1}{\theta}\)</span>.</p>
<p><strong>Note that</strong>, MSE consistency is a stronger property than simple consistency. Thus, if a sequenc <span class="math inline">\(\{T_n\}\)</span> is mean squared error consistent, it is also simply consistent.</p>
</div>
<div id="efficiency" class="section level3">
<h3><span class="header-section-number">2.2.10</span> Efficiency</h3>
<p><em>P.308</em></p>
<p>The relative efficiency of an <strong>unbiased</strong> estimator <span class="math inline">\(T\)</span> of <span class="math inline">\(\tau(\theta)\)</span> to another <strong>unbiased</strong> estimator <span class="math inline">\(T^*\)</span> of <span class="math inline">\(\tau(\theta)\)</span> is given by</p>
<p><span class="math display">\[re(T, T^*)=\frac{Var(T^*)}{Var(T)}\]</span></p>
<p>An unbiased estimator <span class="math inline">\(T^*\)</span> of <span class="math inline">\(\tau(\theta)\)</span> is said to be efficient if <span class="math inline">\(re(T, T^*) \leq 1\)</span> for all unbiased estimators <span class="math inline">\(T\)</span> of <span class="math inline">\(\tau(\theta)\)</span>, and all <span class="math inline">\(\theta \in \Omega\)</span>. The efficiency of an unbiased estimator <span class="math inline">\(T\)</span> of <span class="math inline">\(\tau(\theta)\)</span> is given by</p>
<p><span class="math display">\[e(T)=re(T, T^*)\]</span></p>
<p>if <span class="math inline">\(T^*\)</span> is an efficient estimator of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p>Thus, <strong>an effecient estimator is just a UMVUE.</strong></p>
<p><strong>Example:</strong></p>
<p><em>P.233 Example 7.2.2</em></p>
<p><em>P.303 Example 9.3.2</em></p>
<p><em>P.309 Example 9.3.7</em></p>
<p>Let <span class="math inline">\(X_1,X_2,...,X_n\)</span> sample from <span class="math inline">\(X_i \sim EXP(\theta)\)</span>. We know that</p>
<p><span class="math display">\[X_{1:n}=EXP(\theta/n)\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E(nX_{1:n})=nE(X_{1:n})=n\frac{\theta}{n}=\theta\]</span>
Thus,</p>
<p><span class="math display">\[nX_{1:n}\]</span>
is the unbiased estimator for <span class="math inline">\(\theta\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[re(T,T^*)=\frac{Var(T^*)}{Var(T)}=\frac{Var(\bar{X})}{Var(nX_{1:n})}=\frac{\theta^2/n}{n^2Var(X_{1:n})}=\frac{\theta^2/n}{n^2(\theta/n)^2}=\frac{\theta^2/n}{\theta^2}=\frac{1}{n}\]</span>
Thus, <span class="math inline">\(T^*=\bar{X}\)</span> is a more efficient estimator for <span class="math inline">\(\theta\)</span> than <span class="math inline">\(T=nX_{1:n}\)</span>.</p>
</div>
<div id="asymptotic-efficiency" class="section level3">
<h3><span class="header-section-number">2.2.11</span> Asymptotic efficiency</h3>
<p>Let <span class="math inline">\(\{T_n\}\)</span> and <span class="math inline">\(\{T_n^*\}\)</span> be the two asymptotically unbiased sequences of estimators for <span class="math inline">\(\tau(\theta)\)</span>. The <strong>asymptotic relative efficiency</strong> of <span class="math inline">\(T_n\)</span>relative to <span class="math inline">\(T_n^*\)</span> is given by</p>
<p><span class="math display">\[are(T_n,T_n^*)=lim_{n \rightarrow \infty} \frac{Var(T_n^*)}{Var(T_n)}\]</span>
The sequence <span class="math inline">\(\{T_n^*\}\)</span> is said to asymptotically efficient if <span class="math inline">\(are\{T_n, T_n^*\} \leq 1\)</span> for all other asymptotically unbiased sequences <span class="math inline">\(\{T_n\}\)</span>, and all <span class="math inline">\(\theta \in \Omega\)</span>.</p>
</div>
<div id="asymptotic-properties-of-mles" class="section level3">
<h3><span class="header-section-number">2.2.12</span> Asymptotic properties of MLEs</h3>
<p><em>P.316</em></p>
<p>If certain regularity conditions are satisfied, then the solutions, <span class="math inline">\(\hat{\theta}\)</span>, of the MLE have the following properties:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\hat{\theta_n}\)</span> exists and is unique.</p></li>
<li><p><span class="math inline">\(\hat{\theta_n}\)</span> is a consistent estimator of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{\theta_n}\)</span> is asymptotically normal with asymptotic mean <span class="math inline">\(\theta\)</span> and variance</p></li>
</ol>
<p><span class="math display">\[\frac{1}{2}E[\frac{\partial}{\partial\theta}ln \; f(X;\theta)]^2\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><span class="math inline">\(\hat{\theta}\)</span> is asymptotically efficient.</li>
</ol>
<p>Note that the asymptotic efficiency of <span class="math inline">\(\hat{\theta}\)</span> follows from the fact that the asymptotic variance is the same as the <strong>CRLB</strong> for unbiased estimators of <span class="math inline">\(\theta\)</span>. Thus, for large <span class="math inline">\(n\)</span>, approximately</p>
<p><span class="math display">\[\hat{\theta_n} \sim N(\theta, CRLB)\]</span></p>
<p><strong>Example</strong></p>
<p><em>P.317</em></p>
<p>From Example 9.2.7, we know the MLE of the mean <span class="math inline">\(\theta\)</span> of an exponential distribution is the sample mean, <span class="math inline">\(\hat{\theta_n}=\bar{X}\)</span>.</p>
<p>We can infer the same asymptotic properties either from the discussion above or from the Central Limit Theorem. That is, <span class="math inline">\(\hat{\theta_n}\)</span> is asymptotically normal with asymptotic mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(\theta^2/n\)</span>. From example 9.3.4, we know that <span class="math inline">\(CRLB=\theta^2/n\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[\sqrt{n}\frac{\hat{\theta_n}-\theta}{\theta}\sim N(0,1)\]</span></p>
</div>
</div>
<div id="sufficient-and-completeness" class="section level2">
<h2><span class="header-section-number">2.3</span> Sufficient and completeness</h2>
<div id="sufficiency-and-minimal-sufficiency" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Sufficiency and minimal sufficiency</h3>
<p><em>P.335</em></p>
<p><strong>Sufficient statistic:</strong></p>
<p>A statistic <span class="math inline">\(S\)</span> will be consiered a “sufficient” statistic for a parameter <span class="math inline">\(\theta\)</span> if the conditional distribution of any other statistic <span class="math inline">\(T\)</span> given the value of <span class="math inline">\(S\)</span> does not involve <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Jointly sufficient statistics:</strong></p>
<p>Let <span class="math inline">\(X=(X_1,...,X_n)\)</span> have joint pdf <span class="math inline">\(f(x,\theta)\)</span>, and let <span class="math inline">\(S=(S_1,...,S_k)\)</span> be a k-dimensional statistic.</p>
<p>Then, <span class="math inline">\(S_1,...,S_k\)</span> is a set of <strong>jointly sufficient statistics</strong> for <span class="math inline">\(\theta\)</span> if for any other vector of statistics, <span class="math inline">\(T\)</span>, the conditional pdf of <span class="math inline">\(T\)</span> given <span class="math inline">\(S=s\)</span>, denoted by <span class="math inline">\(f_{T|s}(t)\)</span>, does not depend on <span class="math inline">\(\theta\)</span>.</p>
<p>In the one-dimension case, we simply say that <span class="math inline">\(S\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p>If <span class="math inline">\(k\)</span> unknown parameters are present in the model, then quite often there will exist a set of <span class="math inline">\(k\)</span> sufficient statistics. In some cases, the number of sufficient statistics will exceed the number of parameters.</p>
<p><strong>Minimal sufficient:</strong></p>
<p>A set of statistics is called a <strong>minimal sufficient</strong> set if the members of the set are jointly sufficient for the parameters and if they are a function of every other set of jointly sufficient statistics.</p>
<p><strong>In other words, once the value of a sufficient statistic is known, the observed value of any other statistic does not contain any further inforamtion about the parameter.</strong></p>
<p><strong>Example:</strong></p>
<p><em>P.339</em></p>
<p>Assume a random sample from an exponential distribution, <span class="math inline">\(X_i \sim EXP(\theta)\)</span>. It follows that</p>
<p><span class="math display">\[f(x_1,...,x_n; \theta)=\frac{1}{\theta^n} e^{-\frac{\sum X_i}{\theta}}\]</span>
which suggests checking the stastic <span class="math inline">\(S=\sum X_i\)</span>. We know that <span class="math inline">\(S \sim GAM(\theta,n)\)</span>, thus</p>
<p><span class="math display">\[f_S(s; \theta)=\frac{1}{\theta^n \Gamma(n)}s^{n-1}e^{-\frac{s}{n}}\]</span></p>
<p>If <span class="math inline">\(s=\sum x_i\)</span>, then,</p>
<p><span class="math display">\[\frac{f(x_1,...,x_n; \theta)}{f_S(s;\theta)}=\frac{\Gamma(n)}{s^{n-1}}\]</span></p>
<p>which is free of <span class="math inline">\(\theta\)</span>, and thus by definition S is sufficient for <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="neyman-factorization-theorem-minimal-sufficiency-of-mles" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Neyman factorization theorem, minimal sufficiency of MLEs</h3>
<p><strong>Factorization criterion:</strong></p>
<p>If <span class="math inline">\(X_1,...,X_n\)</span> have joint pdf <span class="math inline">\(f(x_1,...,x_n; \theta)\)</span>, and if <span class="math inline">\(S=(S_1,...,S_n)\)</span>, then <span class="math inline">\(S_1,...,S_k\)</span> are joinly sufficient for <span class="math inline">\(\theta\)</span> if and only if</p>
<p><span class="math display">\[f(x_1,...x_n; \theta)=g(s;\theta)h(x_1,...,x_n)\]</span>
where <span class="math inline">\(g(s;\theta)\)</span> does not depend on <span class="math inline">\(x_1,...,x_n\)</span>, except through <span class="math inline">\(s\)</span>, and <span class="math inline">\(h(x_1,...,x_n)\)</span> does not involve <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Example 1:</strong></p>
<p><em>P.340</em></p>
<p><span class="math inline">\(X_i \sim Bin(1,\theta)\)</span>. We can use factorization criterion to check its sufficient statistic.</p>
<p><span class="math display">\[f(x_1,...,x_n;\theta)=\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}=\theta^s(1-\theta)^{n-s}=g(s;\theta)h(x_1,...,x_n)\]</span></p>
<p>where <span class="math inline">\(s=\sum X_i\)</span> and <span class="math inline">\(h(x_1,...x_n)=1\)</span>. Thus, <span class="math inline">\(s\)</span> is sufficient for <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Example 2:</strong></p>
<p><strong>It is important to specify the regions of zero probability. The following shows that care must be exercised in this matter.</strong></p>
<p><em>P.340</em></p>
<p><span class="math inline">\(X_i \sim UNIF(0,\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is unknown. We get the joint <span class="math inline">\(pdf\)</span> of <span class="math inline">\(X_1,...,X_n\)</span> is</p>
<p><span class="math display">\[f(x_1,...x_n; \theta)=\frac{1}{\theta^n}\]</span>
where,</p>
<p><span class="math display">\[0&lt;x_i&lt;\theta\]</span></p>
<p>It is easier to specify this <span class="math inline">\(pdf\)</span> in terms of the minimum, <span class="math inline">\(x_{1:n}\)</span>, and maximum, <span class="math inline">\(x_{n:n}\)</span>, of <span class="math inline">\(x_1,...,x_n\)</span>. In particular,</p>
<p><span class="math display">\[0&lt; x_{1:n}  \; \; x_{n:n} &lt;\theta\]</span></p>
<p>Thus, <span class="math inline">\(x_{n:n}\)</span> is sufficient for <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Example 3:</strong></p>
<p><em>P.341</em></p>
<p>Consider a random sample from a normal distribution, <span class="math inline">\(X_i \sim N(\mu, \sigma^2)\)</span>.</p>
<p>We know the <span class="math inline">\(pdf\)</span> for normal is</p>
<p><span class="math display">\[f=\frac{1}{\sqrt{2\pi \sigma^2 }} e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\]</span></p>
<p>Thus, the joint <span class="math inline">\(pdf\)</span> is as follows.</p>
<p><span class="math display">\[\begin{aligned} f(x_1,...,x_n;\mu,\sigma^2) &amp;=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}\sum (x_i-\mu)^2} \\ &amp;=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}\sum (x_i^2+\mu^2-2x_i\mu)} \\ &amp;=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}(\sum x_i^2+n\mu^2-2\mu\sum x_i)} \end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(S_1=\sum X_i\)</span> and <span class="math inline">\(S_2=\sum X_i^2\)</span> are jointly sufficient for <span class="math inline">\(\theta={\mu, \sigma^2}\)</span>.</p>
<p><strong>Note that,</strong></p>
<p><em>P.341</em></p>
<p>based on <em>P.298</em> , Example 9.2.10, ML estimation results in</p>
<p><span class="math display">\[\hat{\mu}=\bar{x}\]</span>
<span class="math display">\[\hat{\sigma^2}=\frac{\sum(x_i-\bar{x})^2}{n}\]</span></p>
<p>Further, note that</p>
<p><span class="math display">\[\hat{\mu}=\bar{x}=\frac{S_1}{n}\]</span></p>
<p><span class="math display">\[\begin{aligned} \hat{\sigma^2}&amp;=\frac{\sum(x_i-\bar{x})^2}{n}=\frac{\sum (x_i^2+\bar{x}^2-2x_i\bar{x})}{n} \\ &amp;=\frac{\sum x_i^2+n\bar{x}^2-2\bar{x} \sum x_i}{n} \\ &amp;=\frac{\sum x_i^2+n\bar{x}^2-2n\bar{x}^2}{n} \\&amp;=\frac{\sum x_i^2-n\bar{x}^2}{n} \\&amp;=\frac{\sum x_i^2}{n}-\bar{x}^2 \\ &amp;=\frac{\sum x_i^2}{n}- (\frac{\sum x_i}{n})^2 \\ &amp;=\frac{S_2}{n}-(\frac{S_1}{n})^2 \end{aligned}\]</span></p>
<p>Thus, we can see that <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma^2}\)</span> correspond to a one-to-one stransformation of <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>, thus <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma^2}\)</span> also are jontly sufficient for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="rao-blackwell-theorem" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Rao-Blackwell theorem</h3>
<p>Let <span class="math inline">\(X_1,...,X_n\)</span> have joint <span class="math inline">\(pdf \; f(x_1,...,x_n; \theta)\)</span>, and let <span class="math inline">\(S=(S_1,...,S_k)\)</span> be a vector of jointly sufficient statistics for <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(T\)</span> is any unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>, and if <span class="math inline">\(T^*=E(T|S)\)</span>, then</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(T^*\)</span> is an unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>,</p></li>
<li><p><span class="math inline">\(T^*\)</span> is a function of <span class="math inline">\(S\)</span>, and</p></li>
<li><p><span class="math inline">\(Var(T^*) \leq Var(T)\)</span> for every <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(Var(T^*) &lt; Var(T)\)</span> for some <span class="math inline">\(\theta\)</span> unless <span class="math inline">\(T^*=T\)</span> with probability 1.</p></li>
</ol>
<p><strong>Discussion:</strong></p>
<p><em>P.345</em></p>
<p>"It is clear from the Rao-Blackwell theorem that if we are searching for an unbiased estimator with small variance, we may as well restrict attention to function of sufficient statistics.</p>
<p>If any ubiased estimator exists, then there will be one that is a function of sufficient statistics, namely <span class="math inline">\(E(T|S)\)</span>, which also is unbiased and has variance at least as small or smaller. In particular, we still are interested in knowing how to find a UMVUE for a parameter, and the above theorem narrows our problem down somewhat.</p>
<p>For instance, consider a one-parameter model <span class="math inline">\(f(x;\theta)\)</span>, and assume that a single sufficient statistic, <span class="math inline">\(S\)</span>, exists. We know that we must consider only unbiased functions of <span class="math inline">\(S\)</span> in searching for a UMVUE. In some cases it may be possible to show that only one function of <span class="math inline">\(S\)</span> is unbiased, and in that case we would know that it is a UMVUE.</p>
<p>The concept of “completeness” is helpful in determinning unique unbiased estimators, and this concept is defined in the next section."</p>
</div>
<div id="completeness" class="section level3">
<h3><span class="header-section-number">2.3.4</span> completeness</h3>
<p><strong>Completeness:</strong></p>
<p><em>P.345</em></p>
<p>A family of density functions <span class="math inline">\(\{ f_T(t,\theta); \theta \in \Omega\}\)</span>, is called <strong>complete</strong> if <span class="math inline">\(E[u(T)]=0\)</span> for all <span class="math inline">\(\theta \in \Omega\)</span> implies <span class="math inline">\(u(T)=0\)</span> with probability 1 for all <span class="math inline">\(\theta \in \Omega\)</span>.</p>
<p><em>P.345</em></p>
<p>“This sometimes is expressed by saying that there are no nontrivial unbiased estimators of zero. In particular, it means that two different functions of <span class="math inline">\(T\)</span> cannot have the same expected value.”</p>
<p>For example, if <span class="math inline">\(E[u_1(T)]=\tau(\theta)\)</span> and <span class="math inline">\(E[u_2(T)]=\tau(\theta)\)</span>, which implies <span class="math inline">\(u_1(T)-u_2(T)=0\)</span>, or <span class="math inline">\(u_1(T)=u_2(T)\)</span> with probablity 1, if the family of density fucntions is complete. That is, any unbiased estimator is unique in this case.</p>
<p>We mainly are interested in knowing that the family of density functions of a sufficient statistic is complete, because in that case an unbiased function of the sufficient statistic will be unique, and it must be a UMVUE by the Rao-Blackwell theorem.</p>
</div>
<div id="lehmann-scheffe-completeness-theorem" class="section level3">
<h3><span class="header-section-number">2.3.5</span> Lehmann-Scheffe completeness theorem</h3>
<p>Let <span class="math inline">\(X_1,...,X_n\)</span> have joint <span class="math inline">\(pdf \; f(x_1,...,x_n; \theta)\)</span> and let <span class="math inline">\(S\)</span> be a vector of jointly complete sufficient statistics for <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(T^*=t^*(S)\)</span> is a statistic that is unbiased for <span class="math inline">\(\tau(\theta)\)</span> and a function of <span class="math inline">\(S\)</span>, then <span class="math inline">\(T^*\)</span> is a UMVUE of <span class="math inline">\(tau(\theta)\)</span>.</p>
<p><strong>Example:</strong></p>
<p><em>P.346</em></p>
</div>
<div id="exponential-class-complete-sufficient-statistics" class="section level3">
<h3><span class="header-section-number">2.3.6</span> Exponential class, complete sufficient statistics</h3>
<p>A density fucntion is said to be a member of the <strong>regular exponential class</strong> if it can be expressed in the form</p>
<p><span class="math display">\[f(x; \theta)=c(\theta)h(x)e^{\sum_{j=1}^k q_j(\theta)t_j(x)}\]</span>
where,</p>
<p><span class="math display">\[x \in A\]</span>
And zero otherwise, where <span class="math inline">\(\theta=(\theta_1,...,\theta_k)\)</span> is a vector of <span class="math inline">\(k\)</span> unknown parameters, if the parameter space has the form</p>
<p><span class="math display">\[\Omega=\{\theta | a_i \leq \theta_i \leq b_i, i=1,...k\}\]</span></p>
<p>(note that <span class="math inline">\(a_i=-\infty\)</span> and <span class="math inline">\(b_i=\infty\)</span> are permissible values), and if it satisfies regularity conditions 1, 2, and 3a or 3b given by:</p>
<ol style="list-style-type: decimal">
<li><p>The set <span class="math inline">\(A=\{x:f(x; \theta)&gt;0\}\)</span> does not depend on <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>The functions <span class="math inline">\(q_j(\theta)\)</span> are nontrivial, functionally independent, continuous fucntion of the <span class="math inline">\(\theta_i\)</span>.</p></li>
</ol>
<p>3a. For a continuous random variable, the derivatives <span class="math inline">\(t_k^{&#39;}(x)\)</span> are linearly independent continous functions of <span class="math inline">\(x\)</span> over <span class="math inline">\(A\)</span>.</p>
<p>3b. For a discrete random variable, the <span class="math inline">\(t_k(x)\)</span> are nontrivial functions of <span class="math inline">\(x\)</span> on <span class="math inline">\(A\)</span>, and none is a linear function of the others.</p>
<p><strong>Example</strong></p>
<p><em>P.348</em></p>
<p>Consider Bernoulli distribution <span class="math inline">\(X \sim Bin(1,p)\)</span>. It follows that</p>
<p><span class="math display">\[\begin{aligned} f(x; p) &amp;=p^x(1-p)^{1-x} \\ &amp;=P^x(1-p)(1-p)^{-x} \\ &amp;=(1-p)(1-p)^{-x}P^x \\&amp;=(1-p)e^{ln(1-p)^{-x}P^x} \\ &amp;=(1-p)e^{xln(\frac{p}{1-p})} \end{aligned}\]</span></p>
<p>Compared to the definition of <strong>Exponential Class</strong> defined above, we can get the following.
<span class="math display">\[c(\theta)=(1-p)\]</span>
<span class="math display">\[h(x)=1\]</span>
<span class="math display">\[q_1(\theta)=ln(\frac{p}{1-p})\]</span>
<span class="math display">\[t_1(x)=x\]</span></p>
<p><strong>Theorem 10.4.2</strong></p>
<p><em>P.348</em></p>
<p>If <span class="math inline">\(X_1,...,X_n\)</span> is a random sample from a member of the regular exponential class <span class="math inline">\(REC(q_1,...,q_k)\)</span>, then the statistics</p>
<p><span class="math display">\[S_1=\sum_{i=1}^n t_1(X_i),...,S_k=\sum_{i=1}^n t_k(X_i)\]</span></p>
<p>are a minimal set of complete sufficient statistics for <span class="math inline">\(\theta_1,...\theta_k\)</span>.</p>
<p><strong>Example 1:</strong></p>
<p><em>P.348</em></p>
<p>Again, consider Bernoulli distribution <span class="math inline">\(X \sim Bin(1,p)\)</span>. For a random sample of size <span class="math inline">\(n\)</span>, <span class="math inline">\(t(x_i)=x_i\)</span> and thus based on <strong>Theorem 10.4.2 (see above)</strong> <span class="math inline">\(S=\sum_{i=1}^n X_i\)</span> is a complete sufficient statistic for <span class="math inline">\(p\)</span>.</p>
<p><strong>How about we want to find UMVUE for <span class="math inline">\(Var(X)=p(1-p)\)</span>?</strong></p>
<p>We might try <span class="math inline">\(\bar{X}(1-\bar{X})\)</span>.</p>
<p><span class="math display">\[\begin{aligned} E[\bar{X}(1-\bar{X})] &amp;=E(\bar{X})-E(\bar{X}^2) \\ &amp;=p-(p^2+var(\bar{X})) \\ &amp;=p-p^2-\frac{p(1-p)}{n} \\ &amp;=p(1-p)(1-\frac{1}{n}) \end{aligned}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[E[\frac{1}{1-\frac{1}{n}}\bar{X}(1-\bar{X})]=p(1-p)\]</span></p>
<p>Thus, <span class="math inline">\(\frac{1}{1-\frac{1}{n}}\bar{X}(1-\bar{X})\)</span> is the UMVUE of the <span class="math inline">\(p(1-p)\)</span>.</p>
<p>but <strong>why?</strong></p>
<p><strong>Example 2:</strong></p>
<p><em>P.349</em></p>
<p>If <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, then</p>
<p><span class="math display">\[\begin{aligned} f(x;\mu,\sigma)&amp;=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\ &amp;=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{1}{2\sigma^2}(x^2+\mu^2-2x\mu)} \\ &amp;=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{x^2}{2\sigma^2}-\frac{\mu^2}{2\sigma^2}+\frac{x\mu}{\sigma^2}} \end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(S_1=\sum X_i^2\)</span> and <span class="math inline">\(S_2=\sum X_i\)</span> are jointly complete and sufficient statistics of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. <strong>Refer to the normal example in the section of “Neyman factorization theorem, minimal sufficiency of MLEs”</strong></p>
<p><strong>Theorem 10.4.3</strong></p>
<p><em>P.349</em></p>
<p>If a CRLB estimator <span class="math inline">\(T\)</span> exists for <span class="math inline">\(\tau(\theta)\)</span>, then a single sufficient statistic exists, and <span class="math inline">\(T\)</span> is a function of the sufficient statistic. Conversely, if a single sufficient statistic exists and teh CRLB exists, then a CRLB estimator exists for some <span class="math inline">\(\tau(\theta)\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/0000_2_556.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
