<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Score, Gradient and Jacobian | Basic Stats</title>
  <meta name="description" content="The webpages are mainly about Bayesian." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Score, Gradient and Jacobian | Basic Stats" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The webpages are mainly about Bayesian." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Score, Gradient and Jacobian | Basic Stats" />
  
  <meta name="twitter:description" content="The webpages are mainly about Bayesian." />
  

<meta name="author" content="Bill Last Updated:" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="canonical-link-function.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://williamdingpullman.github.io/" target="blank">Bill's Stats Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface: Motivation</a></li>
<li class="chapter" data-level="1" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>1</b> 443</a><ul>
<li class="chapter" data-level="1.1" data-path="section.html"><a href="section.html#some-basic-concepts"><i class="fa fa-check"></i><b>1.1</b> Some basic concepts</a><ul>
<li class="chapter" data-level="1.1.1" data-path="section.html"><a href="section.html#random-variable"><i class="fa fa-check"></i><b>1.1.1</b> Random variable</a></li>
<li class="chapter" data-level="1.1.2" data-path="section.html"><a href="section.html#permutation"><i class="fa fa-check"></i><b>1.1.2</b> Permutation</a></li>
<li class="chapter" data-level="1.1.3" data-path="section.html"><a href="section.html#combinations"><i class="fa fa-check"></i><b>1.1.3</b> Combinations</a></li>
<li class="chapter" data-level="1.1.4" data-path="section.html"><a href="section.html#partitioning"><i class="fa fa-check"></i><b>1.1.4</b> Partitioning</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="section.html"><a href="section.html#discrete-random-variables"><i class="fa fa-check"></i><b>1.2</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="1.2.1" data-path="section.html"><a href="section.html#binomial"><i class="fa fa-check"></i><b>1.2.1</b> Binomial</a></li>
<li class="chapter" data-level="1.2.2" data-path="section.html"><a href="section.html#poisson"><i class="fa fa-check"></i><b>1.2.2</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="section.html"><a href="section.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.3</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="1.3.1" data-path="section.html"><a href="section.html#uniform"><i class="fa fa-check"></i><b>1.3.1</b> Uniform</a></li>
<li class="chapter" data-level="1.3.2" data-path="section.html"><a href="section.html#exponential"><i class="fa fa-check"></i><b>1.3.2</b> Exponential</a></li>
<li class="chapter" data-level="1.3.3" data-path="section.html"><a href="section.html#normal"><i class="fa fa-check"></i><b>1.3.3</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="section.html"><a href="section.html#large-sample-theory"><i class="fa fa-check"></i><b>1.4</b> Large Sample Theory</a><ul>
<li class="chapter" data-level="1.4.1" data-path="section.html"><a href="section.html#convergence-in-distribution"><i class="fa fa-check"></i><b>1.4.1</b> Convergence in distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="section.html"><a href="section.html#weak-law"><i class="fa fa-check"></i><b>1.4.2</b> Weak law</a></li>
<li class="chapter" data-level="1.4.3" data-path="section.html"><a href="section.html#strong-law"><i class="fa fa-check"></i><b>1.4.3</b> Strong law</a></li>
<li class="chapter" data-level="1.4.4" data-path="section.html"><a href="section.html#central-limit-theorem"><i class="fa fa-check"></i><b>1.4.4</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.4.5" data-path="section.html"><a href="section.html#poisson-approximation-to-binomial"><i class="fa fa-check"></i><b>1.4.5</b> Poisson approximation to binomial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>2</b> 556</a><ul>
<li class="chapter" data-level="2.1" data-path="section-1.html"><a href="section-1.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>2.1</b> Statistics and Sampling Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="section-1.html"><a href="section-1.html#statistics"><i class="fa fa-check"></i><b>2.1.1</b> Statistics</a></li>
<li class="chapter" data-level="2.1.2" data-path="section-1.html"><a href="section-1.html#chi2-t-f-beta"><i class="fa fa-check"></i><b>2.1.2</b> <span class="math inline">\(\chi^2, t, F, beta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="section-1.html"><a href="section-1.html#large-sample-approximations"><i class="fa fa-check"></i><b>2.1.3</b> Large-sample approximations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="section-1.html"><a href="section-1.html#point-estimation"><i class="fa fa-check"></i><b>2.2</b> Point Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="section-1.html"><a href="section-1.html#method-of-moments-estimators"><i class="fa fa-check"></i><b>2.2.1</b> Method of moments estimators</a></li>
<li class="chapter" data-level="2.2.2" data-path="section-1.html"><a href="section-1.html#least-squares-estimators"><i class="fa fa-check"></i><b>2.2.2</b> least squares estimators</a></li>
<li class="chapter" data-level="2.2.3" data-path="section-1.html"><a href="section-1.html#likelihood-function-and-maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.2.3</b> likelihood function and maximum likelihood estimators</a></li>
<li class="chapter" data-level="2.2.4" data-path="section-1.html"><a href="section-1.html#invariance-property-of-mles"><i class="fa fa-check"></i><b>2.2.4</b> Invariance property of MLEs</a></li>
<li class="chapter" data-level="2.2.5" data-path="section-1.html"><a href="section-1.html#unbiased-estimators"><i class="fa fa-check"></i><b>2.2.5</b> Unbiased estimators</a></li>
<li class="chapter" data-level="2.2.6" data-path="section-1.html"><a href="section-1.html#unbiased-estimators-vs.-invariance-property-of-mles"><i class="fa fa-check"></i><b>2.2.6</b> Unbiased estimators vs. Invariance property of MLEs</a></li>
<li class="chapter" data-level="2.2.7" data-path="section-1.html"><a href="section-1.html#umvue-and-cramer-rao-lower-bound"><i class="fa fa-check"></i><b>2.2.7</b> UMVUE and Cramer-Rao lower bound</a></li>
<li class="chapter" data-level="2.2.8" data-path="section-1.html"><a href="section-1.html#best-linear-unbiased-estimation-blue-or-mvlue"><i class="fa fa-check"></i><b>2.2.8</b> Best linear unbiased estimation (BLUE or MVLUE)</a></li>
<li class="chapter" data-level="2.2.9" data-path="section-1.html"><a href="section-1.html#consistency-asymptotic-unbiasedness"><i class="fa fa-check"></i><b>2.2.9</b> Consistency, asymptotic unbiasedness</a></li>
<li class="chapter" data-level="2.2.10" data-path="section-1.html"><a href="section-1.html#efficiency"><i class="fa fa-check"></i><b>2.2.10</b> Efficiency</a></li>
<li class="chapter" data-level="2.2.11" data-path="section-1.html"><a href="section-1.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>2.2.11</b> Asymptotic efficiency</a></li>
<li class="chapter" data-level="2.2.12" data-path="section-1.html"><a href="section-1.html#asymptotic-properties-of-mles"><i class="fa fa-check"></i><b>2.2.12</b> Asymptotic properties of MLEs</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="section-1.html"><a href="section-1.html#sufficient-and-completeness"><i class="fa fa-check"></i><b>2.3</b> Sufficient and completeness</a><ul>
<li class="chapter" data-level="2.3.1" data-path="section-1.html"><a href="section-1.html#sufficiency-and-minimal-sufficiency"><i class="fa fa-check"></i><b>2.3.1</b> Sufficiency and minimal sufficiency</a></li>
<li class="chapter" data-level="2.3.2" data-path="section-1.html"><a href="section-1.html#neyman-factorization-theorem-minimal-sufficiency-of-mles"><i class="fa fa-check"></i><b>2.3.2</b> Neyman factorization theorem, minimal sufficiency of MLEs</a></li>
<li class="chapter" data-level="2.3.3" data-path="section-1.html"><a href="section-1.html#rao-blackwell-theorem"><i class="fa fa-check"></i><b>2.3.3</b> Rao-Blackwell theorem</a></li>
<li class="chapter" data-level="2.3.4" data-path="section-1.html"><a href="section-1.html#completeness"><i class="fa fa-check"></i><b>2.3.4</b> completeness</a></li>
<li class="chapter" data-level="2.3.5" data-path="section-1.html"><a href="section-1.html#lehmann-scheffe-completeness-theorem"><i class="fa fa-check"></i><b>2.3.5</b> Lehmann-Scheffe completeness theorem</a></li>
<li class="chapter" data-level="2.3.6" data-path="section-1.html"><a href="section-1.html#exponential-class-complete-sufficient-statistics"><i class="fa fa-check"></i><b>2.3.6</b> Exponential class, complete sufficient statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>3</b> 530_533</a><ul>
<li class="chapter" data-level="3.1" data-path="section-2.html"><a href="section-2.html#definition-of-the-general-linear-model"><i class="fa fa-check"></i><b>3.1</b> Definition of the general linear model</a></li>
<li class="chapter" data-level="3.2" data-path="section-2.html"><a href="section-2.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="section-2.html"><a href="section-2.html#least-squares-and-propoerties-of-the-regression-parameters"><i class="fa fa-check"></i><b>3.2.1</b> Least squares and propoerties of the regression parameters</a></li>
<li class="chapter" data-level="3.2.2" data-path="section-2.html"><a href="section-2.html#mle"><i class="fa fa-check"></i><b>3.2.2</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="section-2.html"><a href="section-2.html#full-rank-less-than-full-rank"><i class="fa fa-check"></i><b>3.3</b> Full rank, less than full rank</a></li>
<li class="chapter" data-level="3.4" data-path="section-2.html"><a href="section-2.html#assumptions-checking-assumptions"><i class="fa fa-check"></i><b>3.4</b> Assumptions, checking assumptions</a></li>
<li class="chapter" data-level="3.5" data-path="section-2.html"><a href="section-2.html#bootstrapping-used-in-linear-models"><i class="fa fa-check"></i><b>3.5</b> Bootstrapping used in linear models</a></li>
<li class="chapter" data-level="3.6" data-path="section-2.html"><a href="section-2.html#generalized-linear-models"><i class="fa fa-check"></i><b>3.6</b> Generalized linear models</a><ul>
<li class="chapter" data-level="3.6.1" data-path="section-2.html"><a href="section-2.html#definition-similarities-and-differences-from-general-linear-models"><i class="fa fa-check"></i><b>3.6.1</b> Definition, similarities and differences from general linear models</a></li>
<li class="chapter" data-level="3.6.2" data-path="section-2.html"><a href="section-2.html#advantage-and-disadvantages"><i class="fa fa-check"></i><b>3.6.2</b> Advantage and disadvantages</a></li>
<li class="chapter" data-level="3.6.3" data-path="section-2.html"><a href="section-2.html#logistic-and-poisson-regression"><i class="fa fa-check"></i><b>3.6.3</b> logistic and poisson regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>4</b> 512</a><ul>
<li class="chapter" data-level="4.1" data-path="section-3.html"><a href="section-3.html#completely-randomized-designs"><i class="fa fa-check"></i><b>4.1</b> Completely randomized designs</a></li>
<li class="chapter" data-level="4.2" data-path="section-3.html"><a href="section-3.html#randomized-complete-block-designs"><i class="fa fa-check"></i><b>4.2</b> Randomized complete block designs</a></li>
<li class="chapter" data-level="4.3" data-path="section-3.html"><a href="section-3.html#incomplete-block-designs"><i class="fa fa-check"></i><b>4.3</b> incomplete block designs</a></li>
<li class="chapter" data-level="4.4" data-path="section-3.html"><a href="section-3.html#split-plot-designs"><i class="fa fa-check"></i><b>4.4</b> Split-plot designs</a></li>
<li class="chapter" data-level="4.5" data-path="section-3.html"><a href="section-3.html#row-column-design-latin-square-designs"><i class="fa fa-check"></i><b>4.5</b> Row-column design (Latin square designs)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logit-and-probit.html"><a href="logit-and-probit.html"><i class="fa fa-check"></i><b>5</b> Logit and Probit</a><ul>
<li class="chapter" data-level="5.1" data-path="logit-and-probit.html"><a href="logit-and-probit.html#logit"><i class="fa fa-check"></i><b>5.1</b> Logit</a></li>
<li class="chapter" data-level="5.2" data-path="logit-and-probit.html"><a href="logit-and-probit.html#probit"><i class="fa fa-check"></i><b>5.2</b> Probit</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="normal-distribution.html"><a href="normal-distribution.html"><i class="fa fa-check"></i><b>6</b> Normal distribution</a><ul>
<li class="chapter" data-level="6.1" data-path="normal-distribution.html"><a href="normal-distribution.html#basics"><i class="fa fa-check"></i><b>6.1</b> Basics</a></li>
<li class="chapter" data-level="6.2" data-path="normal-distribution.html"><a href="normal-distribution.html#confidence-intervals-for-normal-distributions"><i class="fa fa-check"></i><b>6.2</b> Confidence intervals for normal distributions</a></li>
<li class="chapter" data-level="6.3" data-path="normal-distribution.html"><a href="normal-distribution.html#percentile"><i class="fa fa-check"></i><b>6.3</b> Percentile</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>7</b> MLE</a><ul>
<li class="chapter" data-level="7.1" data-path="intro.html"><a href="intro.html#basic-idea-of-mle"><i class="fa fa-check"></i><b>7.1</b> Basic idea of MLE</a></li>
<li class="chapter" data-level="7.2" data-path="intro.html"><a href="intro.html#coin-flip-example-probit-and-logit"><i class="fa fa-check"></i><b>7.2</b> Coin flip example, probit, and logit</a><ul>
<li class="chapter" data-level="7.2.1" data-path="intro.html"><a href="intro.html#probit-1"><i class="fa fa-check"></i><b>7.2.1</b> Probit</a></li>
<li class="chapter" data-level="7.2.2" data-path="intro.html"><a href="intro.html#logit-1"><i class="fa fa-check"></i><b>7.2.2</b> Logit</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="intro.html"><a href="intro.html#further-on-logit"><i class="fa fa-check"></i><b>7.3</b> Further on logit</a></li>
<li class="chapter" data-level="7.4" data-path="intro.html"><a href="intro.html#references"><i class="fa fa-check"></i><b>7.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html"><i class="fa fa-check"></i><b>8</b> Score, Gradient and Jacobian</a><ul>
<li class="chapter" data-level="8.1" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#score"><i class="fa fa-check"></i><b>8.1</b> Score</a></li>
<li class="chapter" data-level="8.2" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#fisher-scoring"><i class="fa fa-check"></i><b>8.2</b> Fisher scoring</a></li>
<li class="chapter" data-level="8.3" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#gradient-and-jacobian"><i class="fa fa-check"></i><b>8.3</b> Gradient and Jacobian</a></li>
<li class="chapter" data-level="8.4" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#hessian-and-fisher-information"><i class="fa fa-check"></i><b>8.4</b> Hessian and Fisher Information</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="canonical-link-function.html"><a href="canonical-link-function.html"><i class="fa fa-check"></i><b>9</b> Canonical link function</a></li>
<li class="chapter" data-level="10" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>10</b> Ordinary Least Squares (OLS)</a><ul>
<li class="chapter" data-level="10.1" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#taylor-series"><i class="fa fa-check"></i><b>10.1</b> Taylor series</a></li>
<li class="chapter" data-level="10.2" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#references-1"><i class="fa fa-check"></i><b>10.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html"><i class="fa fa-check"></i><b>11</b> Cholesky decomposition</a><ul>
<li class="chapter" data-level="11.1" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-1"><i class="fa fa-check"></i><b>11.1</b> Example 1</a></li>
<li class="chapter" data-level="11.2" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-2"><i class="fa fa-check"></i><b>11.2</b> Example 2</a></li>
<li class="chapter" data-level="11.3" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-3"><i class="fa fa-check"></i><b>11.3</b> Example 3</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://www.williamsding.com/" target="blank">Bill's website</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basic Stats</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="score-gradient-and-jacobian" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Score, Gradient and Jacobian</h1>
<div id="score" class="section level2">
<h2><span class="header-section-number">8.1</span> Score</h2>
<p>The score is the gradient (the vector of partial derivatives) of <span class="math inline">\(log L(\theta)\)</span>, with respect to an m-dimensional parameter vector <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[S(\theta) = \frac{\partial\ell}{\partial \theta}\]</span>
Typically, they use <span class="math inline">\(\nabla\)</span> to denote the partical derivative.</p>
<p><span class="math display">\[\nabla \ell\]</span></p>
<p>Such differentiation will generate a <span class="math inline">\(m \times 1\)</span> row vector, which indicates the sensitivity of the likelihood.</p>
<p>Quote from Steffen Lauritzen’s slides: “Generally the solution to this equation must be calculated by iterative methods. One of the most common methods is the Newton–Raphson
method and this is based on successive approximations to the solution, using Taylor’s theorem to approximate the equation.”</p>
<p>For instance, using logit link, we can get the first derivative of log likelihood logistic regression as follows. We can not really find <span class="math inline">\(\beta\)</span> easily to make the equation to be 0.</p>
<p><span class="math display">\[\begin{aligned}
\frac{\partial \ell} {\partial \beta} 
&amp;= \sum_{i=1}^{n}x_i^T[y_i-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}] \\
&amp;=\sum_{i=1}^{n} x_i^T[y_i-\hat{y_i}]
\end{aligned}\]</span></p>
</div>
<div id="fisher-scoring" class="section level2">
<h2><span class="header-section-number">8.2</span> Fisher scoring</h2>
<p>[I will come back to this later.]</p>
<p><a href="https://www2.stat.duke.edu/courses/Fall00/sta216/handouts/diagnostics.pdf" class="uri">https://www2.stat.duke.edu/courses/Fall00/sta216/handouts/diagnostics.pdf</a></p>
<p><a href="https://stats.stackexchange.com/questions/176351/implement-fisher-scoring-for-linear-regression" class="uri">https://stats.stackexchange.com/questions/176351/implement-fisher-scoring-for-linear-regression</a></p>
</div>
<div id="gradient-and-jacobian" class="section level2">
<h2><span class="header-section-number">8.3</span> Gradient and Jacobian</h2>
<p><strong>Remarks</strong>: This part discusses gradient in a more general sense.</p>
<p>When <span class="math inline">\(f(x)\)</span> is only in a single dimension space:</p>
<p><span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}\)</span></p>
<p><span class="math display">\[\nabla f(x)=[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n}]\]</span>
When <span class="math inline">\(f(x)\)</span> is only in a m-dimension space (i.e., Jacobian):
<span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R^m}\)</span></p>
<p><span class="math display">\[Jac(f)=\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_1}{\partial x_3} &amp; ... &amp; \frac{\partial f_1}{\partial x_n}\\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_3} &amp; ... &amp; \frac{\partial f_2}{\partial x_n} \\
...\\
\frac{\partial f_m}{\partial x_1} &amp; \frac{\partial f_m}{\partial x_2} &amp; \frac{\partial f_n}{\partial x_3} &amp; ... &amp; \frac{\partial f_m}{\partial x_n}
\end{bmatrix}\]</span></p>
<p>For instance,</p>
<p><span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}\)</span>:</p>
<p><span class="math display">\[f(x,y)=x^2+2y\]</span>
<span class="math display">\[\nabla f(x,y)=[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]=[2x,2]\]</span>
<span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R^m}\)</span></p>
<p><span class="math display">\[f(x,y)=(x^2+2y,x^3)\]</span>
<span class="math display">\[Jac(f)=\begin{bmatrix}
2x &amp; 2\\
2x^2 &amp; 0 
\end{bmatrix}\]</span></p>
</div>
<div id="hessian-and-fisher-information" class="section level2">
<h2><span class="header-section-number">8.4</span> Hessian and Fisher Information</h2>
<p>Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field.</p>
<p><span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}\)</span></p>
<p><span class="math display">\[Hessian=\nabla ^2(f) =\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_3} &amp; ... &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \frac{\partial^2 f}{\partial x_2 \partial x_3} &amp; ... &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\frac{\partial^2 f}{\partial x_3 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_3 \partial x_2} &amp; \frac{\partial^2 f}{\partial x_3^2} &amp; ... &amp; \frac{\partial^2 f}{\partial x_3 \partial x_n} \\
...\\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \frac{\partial^2 f}{\partial x_n \partial x_3} &amp; ... &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}\]</span></p>
<p>As a special case, in the context of logit:</p>
<p>Suppose that the log likelihood function is <span class="math inline">\(\ell (\theta)\)</span>. <span class="math inline">\(\theta\)</span> is a <span class="math inline">\(m\)</span> demension vector.</p>
<p><span class="math display">\[ \theta = \begin{bmatrix}\theta_1 \\
\theta_2 \\
\theta_3 \\
\theta_4 \\
...\\
\theta_m \\
\end{bmatrix}\]</span></p>
<p><span class="math display">\[Hessian=\nabla ^2(\ell) =\begin{bmatrix}
\frac{\partial^2 \ell}{\partial \theta_1^2} &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_2} &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_3} &amp; ... &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_m}\\
\frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_1} &amp; \frac{\partial^2 \ell}{\partial \theta_2^2 } &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_3} &amp; ... &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_m} \\
\frac{\partial^2 \ell}{\partial \theta_3 \partial \theta_1} &amp; \frac{\partial^2 \ell}{\partial \theta_3 \theta_2 } &amp; \frac{\partial^2 \ell}{\partial \theta_3^2} &amp; ... &amp; \frac{\partial^2 \ell}{\partial \theta_3 \partial \theta_m} \\
...\\
\frac{\partial^2 \ell}{\partial \theta_m \partial \theta_1} &amp; \frac{\partial^2 \ell}{\partial \theta_m \theta_2 } &amp; \frac{\partial^2 \ell}{\partial \theta_m \partial \theta_3} &amp; ... &amp; \frac{\partial^2 \ell}{\partial \theta_m \partial \theta_m} 
\end{bmatrix}\]</span></p>
<p>“In statistics, the observed information, or observed Fisher information, is the negative of the second derivative (the Hessian matrix) of the”log-likelihood" (the logarithm of the likelihood function). It is a sample-based version of the Fisher information." (Direct quote from Wikipedia.)</p>
<p>Thus, the observed information matrix:</p>
<p><span class="math display">\[-Hessian=-\nabla ^2(\ell) \]</span></p>
<p>Expected (Fisher) information matrix:</p>
<p><span class="math display">\[E[-\nabla ^2(\ell)] \]</span></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="canonical-link-function.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/081-BasicStat.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
