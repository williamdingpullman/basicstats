[
["index.html", "Basic Stats Preface: Motivation", " Basic Stats Bill Last Updated: 19 January, 2020 Preface: Motivation All the notes I have done here are about basic stats. While I have tried my best, probably there are still some typos and errors. Please feel free to let me know in case you find one. Thank you! "],
["intro.html", "Chapter 1 MLE 1.1 Basic idea of MLE 1.2 Coin flip example, probit, and logit 1.3 Further on logit 1.4 References", " Chapter 1 MLE 1.1 Basic idea of MLE Suppose that we flip a coin, \\(y_i=0\\) for tails and \\(y_i=1\\) for heads. If we get \\(p\\) heads from \\(n\\) trials, we can get the proportion of heads is \\(p/n\\), which is the sample mean. If we do not do any further calculation, this is our best guess. Suppose that the true proablity is \\(\\rho\\), then we can get: \\[ \\mathbf{L}(y_i)=\\begin{cases} \\rho \\;\\;\\: y_i = 1 \\\\ 1-\\rho \\;\\;\\: y_i = 0 \\end{cases} \\] Thus, we can also write it as follows. \\[\\mathbf{L}(y_i) = \\rho^{y_i}(1-\\rho)^{1-y_i}\\] Thus, we can get: \\[\\prod \\mathbf{L}(y_i|\\rho)=\\rho^{\\sum y_i}(1-\\rho)^{\\sum(1-y_i)}\\] Further, we can get a log-transformed format. \\[log (\\prod \\mathbf{L}(y_i|\\rho))=\\sum y_i log \\rho + \\sum(1-y_i) log(1-\\rho)\\] To maximize the log-function above, we can calculate the derivative with respect to \\(\\rho\\). \\[\\frac{\\partial log (\\prod \\mathbf{L}(y_i|\\rho)) }{\\partial \\rho}=\\sum y_i \\frac{1}{\\rho}-\\sum(1-y_i) \\frac{1}{1-\\rho}\\] Set the derivative to zero and solve for \\(\\rho\\), we can get \\[\\sum y_i \\frac{1}{\\rho}-\\sum(1-y_i) \\frac{1}{1-\\rho}=0\\] \\[\\Rightarrow (1-\\rho)\\sum y_i - \\rho \\sum(1-y_i) =0\\] \\[\\Rightarrow \\sum y_i-\\rho\\sum y_i - n\\rho +\\rho\\sum y_i =0\\] \\[\\Rightarrow \\sum y_i - n\\rho =0\\] \\[\\Rightarrow \\rho = \\frac{\\sum y_i}{n}=\\frac{p}{n}\\] Thus, we can see that the \\(\\rho\\) maximizing the likelihood function is equal to the sample mean. 1.2 Coin flip example, probit, and logit In the example above, we are not really trying to estimate a lot of regression coefficients. What we are doing actually is to calculate the sample mean, or intercept in the regresion sense. What does it mean? Let’s use some data to explain it. Suppose that we flip a coin 20 times and observe 8 heads. We can use the R’s glm function to esimate the \\(\\rho\\). If the result is consistent with what we did above, we should observe that the \\(cdf\\) of the esimate of \\(\\beta_0\\) (i.e., intercept) should be equal to \\(8/20=0.4\\). coins&lt;-c(rep(1,times=8),rep(0,times=12)) table(coins) ## coins ## 0 1 ## 12 8 coins&lt;-as.data.frame(coins) 1.2.1 Probit probitresults &lt;- glm(coins ~ 1, family = binomial(link = &quot;probit&quot;), data = coins) probitresults ## ## Call: glm(formula = coins ~ 1, family = binomial(link = &quot;probit&quot;), ## data = coins) ## ## Coefficients: ## (Intercept) ## -0.2533 ## ## Degrees of Freedom: 19 Total (i.e. Null); 19 Residual ## Null Deviance: 26.92 ## Residual Deviance: 26.92 AIC: 28.92 pnorm(probitresults$coefficients) ## (Intercept) ## 0.4 As we can see the intercept is \\(-0.2533\\), and thus \\(\\Phi(-0.2533471)=0.4\\) 1.2.2 Logit We can also use logit link to calculate the intercept as well. Recall that \\[p(y=1)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)}}=\\frac{e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}{1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}\\] Thus, \\[p(y=1)=\\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\\] logitresults &lt;- glm(coins ~ 1, family = binomial(link = &quot;logit&quot;), data = coins) logitresults$coefficients ## (Intercept) ## -0.4054651 exp(logitresults$coefficients)/(1+exp(logitresults$coefficients)) ## (Intercept) ## 0.4 Note that, the defaul link for the binomial in the glm function in logit. 1.3 Further on logit The probablity of \\(y=1\\) is as follows: \\[p=p(y=1)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)}}=\\frac{e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}{1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}\\] Thus, the likelihood function is as follows: \\[L=\\prod p^{y_i}(1-p)^{1-y_i}=\\prod (\\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)}})^{y_i}(\\frac{1}{1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}})^{1-y_i}\\] \\[=\\prod (1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)})^{-y_i}(1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n})^{-(1-y_i)}\\] Thus, the log-likelihood is as follows: \\[logL=\\sum (-y_i \\cdot log(1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)})-(1-y_i)\\cdot log(1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}))\\] Typically, optimisers minimize a function, so we use negative log-likelihood as minimising that is equivalent to maximising the log-likelihood or the likelihood itself. #Source of R code: https://www.r-bloggers.com/logistic-regression/ mle.logreg = function(fmla, data) { # Define the negative log likelihood function logl &lt;- function(theta,x,y){ y &lt;- y x &lt;- as.matrix(x) beta &lt;- theta[1:ncol(x)] # Use the log-likelihood of the Bernouilli distribution, where p is # defined as the logistic transformation of a linear combination # of predictors, according to logit(p)=(x%*%beta) loglik &lt;- sum(-y*log(1 + exp(-(x%*%beta))) - (1-y)*log(1 + exp(x%*%beta))) return(-loglik) } # Prepare the data outcome = rownames(attr(terms(fmla),&quot;factors&quot;))[1] dfrTmp = model.frame(data) x = as.matrix(model.matrix(fmla, data=dfrTmp)) y = as.numeric(as.matrix(data[,match(outcome,colnames(data))])) # Define initial values for the parameters theta.start = rep(0,(dim(x)[2])) names(theta.start) = colnames(x) # Calculate the maximum likelihood mle = optim(theta.start,logl,x=x,y=y, method = &#39;BFGS&#39;, hessian=T) out = list(beta=mle$par,vcov=solve(mle$hessian),ll=2*mle$value) } mydata = read.csv(url(&#39;https://stats.idre.ucla.edu/stat/data/binary.csv&#39;)) mylogit1 = glm(admit~gre+gpa+as.factor(rank), family=binomial, data=mydata) mydata$rank = factor(mydata$rank) #Treat rank as a categorical variable fmla = as.formula(&quot;admit~gre+gpa+rank&quot;) #Create model formula mylogit2 = mle.logreg(fmla, mydata) #Estimate coefficients print(cbind(coef(mylogit1), mylogit2$beta)) ## [,1] [,2] ## (Intercept) -3.989979073 -3.772676422 ## gre 0.002264426 0.001375522 ## gpa 0.804037549 0.898201239 ## as.factor(rank)2 -0.675442928 -0.675543009 ## as.factor(rank)3 -1.340203916 -1.356554831 ## as.factor(rank)4 -1.551463677 -1.563396035 1.4 References http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf "],
["score-gradient-and-jacobian.html", "Chapter 2 Score, Gradient and Jacobian 2.1 Score 2.2 Fisher scoring 2.3 Gradient and Jacobian 2.4 Hessian and Fisher Information", " Chapter 2 Score, Gradient and Jacobian 2.1 Score The score is the gradient (the vector of partial derivatives) of \\(log L(\\theta)\\), with respect to an m-dimensional parameter vector \\(\\theta\\). \\[S(\\theta) = \\frac{\\partial\\ell}{\\partial \\theta}\\] Typically, they use \\(\\nabla\\) to denote the partical derivative. \\[\\nabla \\ell\\] Such differentiation will generate a \\(m \\times 1\\) row vector, which indicates the sensitivity of the likelihood. Quote from Steffen Lauritzen’s slides: “Generally the solution to this equation must be calculated by iterative methods. One of the most common methods is the Newton–Raphson method and this is based on successive approximations to the solution, using Taylor’s theorem to approximate the equation.” For instance, using logit link, we can get the first derivative of log likelihood logistic regression as follows. We can not really find \\(\\beta\\) easily to make the equation to be 0. \\[\\begin{aligned} \\frac{\\partial \\ell} {\\partial \\beta} &amp;= \\sum_{i=1}^{n}x_i^T[y_i-\\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}] \\\\ &amp;=\\sum_{i=1}^{n} x_i^T[y_i-\\hat{y_i}] \\end{aligned}\\] 2.2 Fisher scoring [I will come back to this later.] https://www2.stat.duke.edu/courses/Fall00/sta216/handouts/diagnostics.pdf https://stats.stackexchange.com/questions/176351/implement-fisher-scoring-for-linear-regression 2.3 Gradient and Jacobian Remarks: This part discusses gradient in a more general sense. When \\(f(x)\\) is only in a single dimension space: \\(\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) \\[\\nabla f(x)=[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...,\\frac{\\partial f}{\\partial x_n}]\\] When \\(f(x)\\) is only in a m-dimension space (i.e., Jacobian): \\(\\mathbb{R}^n \\rightarrow \\mathbb{R^m}\\) \\[Jac(f)=\\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\frac{\\partial f_1}{\\partial x_3} &amp; ... &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\frac{\\partial f_2}{\\partial x_3} &amp; ... &amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ ...\\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\frac{\\partial f_m}{\\partial x_2} &amp; \\frac{\\partial f_n}{\\partial x_3} &amp; ... &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\\] For instance, \\(\\mathbb{R}^n \\rightarrow \\mathbb{R}\\): \\[f(x,y)=x^2+2y\\] \\[\\nabla f(x,y)=[\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}]=[2x,2]\\] \\(\\mathbb{R}^n \\rightarrow \\mathbb{R^m}\\) \\[f(x,y)=(x^2+2y,x^3)\\] \\[Jac(f)=\\begin{bmatrix} 2x &amp; 2\\\\ 2x^2 &amp; 0 \\end{bmatrix}\\] 2.4 Hessian and Fisher Information Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. \\(\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) \\[Hessian=\\nabla ^2(f) =\\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_3} &amp; ... &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}\\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_3} &amp; ... &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_3 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_3 \\partial x_2} &amp; \\frac{\\partial^2 f}{\\partial x_3^2} &amp; ... &amp; \\frac{\\partial^2 f}{\\partial x_3 \\partial x_n} \\\\ ...\\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_3} &amp; ... &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\\] As a special case, in the context of logit: Suppose that the log likelihood function is \\(\\ell (\\theta)\\). \\(\\theta\\) is a \\(m\\) demension vector. \\[ \\theta = \\begin{bmatrix}\\theta_1 \\\\ \\theta_2 \\\\ \\theta_3 \\\\ \\theta_4 \\\\ ...\\\\ \\theta_m \\\\ \\end{bmatrix}\\] \\[Hessian=\\nabla ^2(\\ell) =\\begin{bmatrix} \\frac{\\partial^2 \\ell}{\\partial \\theta_1^2} &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_2} &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_3} &amp; ... &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_m}\\\\ \\frac{\\partial^2 \\ell}{\\partial \\theta_2 \\partial \\theta_1} &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_2^2 } &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_3} &amp; ... &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_m} \\\\ \\frac{\\partial^2 \\ell}{\\partial \\theta_3 \\partial \\theta_1} &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_3 \\theta_2 } &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_3^2} &amp; ... &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_3 \\partial \\theta_m} \\\\ ...\\\\ \\frac{\\partial^2 \\ell}{\\partial \\theta_m \\partial \\theta_1} &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_m \\theta_2 } &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_m \\partial \\theta_3} &amp; ... &amp; \\frac{\\partial^2 \\ell}{\\partial \\theta_m \\partial \\theta_m} \\end{bmatrix}\\] “In statistics, the observed information, or observed Fisher information, is the negative of the second derivative (the Hessian matrix) of the”log-likelihood&quot; (the logarithm of the likelihood function). It is a sample-based version of the Fisher information.&quot; (Direct quote from Wikipedia.) Thus, the observed information matrix: \\[-Hessian=-\\nabla ^2(\\ell) \\] Expected (Fisher) information matrix: \\[E[-\\nabla ^2(\\ell)] \\] "],
["canonical-link-function.html", "Chapter 3 Canonical link function", " Chapter 3 Canonical link function Inspired by a Stack Exchange post, I created the following figure: \\[ \\frac{Paramter}{\\theta} \\longrightarrow \\gamma^{&#39;}(\\theta) = \\mu \\longrightarrow \\frac{Mean}{\\mu} \\longrightarrow g(\\mu) = \\eta \\longrightarrow \\frac{ Linear predictor}{\\eta} \\] For the case of \\(n\\) time Bernoulli (i.e., Binomial), its canonical link function is logit. Specifically, \\[ \\frac{Paramter}{\\theta=\\beta^Tx_i} \\longrightarrow \\gamma^{&#39;}(\\theta)= \\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}\\longrightarrow \\frac{Mean}{\\mu=\\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}}\\longrightarrow g(\\mu) = log \\frac{\\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}}{1-\\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}}\\longrightarrow \\frac{ Linear predictor}{\\eta = \\beta^Tx_i}\\] Thus, we can see that, \\[\\theta \\equiv \\eta \\] The link function \\(g(\\mu)\\) relates the linear predictor \\(\\eta = \\beta^Tx_i\\) to the mean \\(\\mu\\). Remarks: Parameter is \\(\\theta = \\beta ^T x_i\\) (Not \\(\\mu\\)!). \\(\\mu=p(y=1)=\\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}\\) (Not logit!). Link function (i.e., \\(g(\\mu)\\)) = logit = logarithm of odds = log \\(\\frac{Event - Happened }{Event - Not - Happened}\\). \\(g(\\mu) = log \\frac{\\mu}{1-\\mu}=\\beta^T x_i\\). Thus, link function = linear predictor = log odds! Quote from the Stack Exchange post “Newton Method and Fisher scoring for finding the ML estimator coincide, these links simplify the derivation of the MLE.” (Recall, we know that \\(\\mu\\) or \\(p(y=1)\\) is the mean function. Recall that, \\(n\\) trails of coin flips, and get \\(p\\) heads. Thus \\(\\mu = \\frac{p}{n}\\).) "],
["ordinary-least-squares-ols.html", "Chapter 4 Ordinary Least Squares (OLS) 4.1 Taylor series 4.2 References", " Chapter 4 Ordinary Least Squares (OLS) Suppose we have \\(n\\) observation, and \\(m\\) variables. \\[\\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1m}\\\\ x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2m} \\\\ ...\\\\ x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nm} \\end{bmatrix}\\] Thus, we can write it as the following \\(n\\) equations. \\[y_1=\\beta_0+\\beta_1 x_{11}+\\beta_2 x_{12}+...+ \\beta_m x_{1m}\\] \\[y_2=\\beta_0+\\beta_1 x_{21}+\\beta_2 x_{22}+...+ \\beta_m x_{2m}\\] \\[y_3=\\beta_0+\\beta_1 x_{31}+\\beta_2 x_{32}+...+ \\beta_m x_{3m}\\] \\[...\\] \\[y_n=\\beta_0+\\beta_1 x_{n1}+\\beta_2 x_{n2}+...+ \\beta_m x_{nm}\\] We can combine all the \\(n\\) equations as the following one: \\[y_i=\\beta_0+\\beta_1 x_{i1}+\\beta_2 x_{i2}+...+ \\beta_m x_{im} (i \\in [1,n])\\] We can further rewrite it as a matrix format as follows. \\[y= X \\beta\\] Where, \\[y = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ ...\\\\ y_n \\\\ \\end{bmatrix}\\] \\[X=\\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1m}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2m} \\\\ ...\\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nm} \\end{bmatrix}\\] \\[\\beta = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ ...\\\\ \\beta_m \\\\ \\end{bmatrix}\\] Since later we need the inverse of \\(X\\), we need to make it into a square matrix. \\[X^Ty=X^TX \\hat{\\beta} \\Rightarrow \\hat{\\beta} = (X^TX)^{-1} X^Ty\\] We can use R to implement this calculation. As we can see, there is no need to do any iterations at all, but rather just pure matrix calculation. X&lt;-matrix(rnorm(1000),ncol=2) # we define a 2 column matrix, with 500 rows X&lt;-cbind(1,X) # add a 1 constant beta_true&lt;-c(2,1,2) # True regression coefficients beta_true&lt;-as.matrix(beta_true) y=X%*%beta_true+rnorm(500) transposed_X&lt;-t(X) beta_hat&lt;-solve(transposed_X%*%X)%*%transposed_X%*%y beta_hat ## [,1] ## [1,] 2.0766575 ## [2,] 0.9557739 ## [3,] 1.9399556 Side Notes The function of as.matrix will automatically make c(2,1,2) become the dimension of \\(3 \\times 1\\), you do not need to transpose the \\(\\beta\\). 4.1 Taylor series \\[\\begin{aligned} f(x)|_{a} &amp;=f(a)+\\frac{f^{&#39;}(a)}{1!}(x-a)+\\frac{f^{&#39;}(a)}{2!}(x-a)^2+\\frac{f^{&#39;&#39;}(a)}{3!}(x-a)^{3}+...\\\\&amp;=\\sum_{n=0}^{\\infty} \\frac{f^{n}(a)}{n!}(x-a)^n \\end{aligned}\\] For example: \\[\\begin{aligned} e^x |_{a=0} &amp;= e^a+ \\frac{e^a}{1!}(x-a)+\\frac{e^a}{2!}(x-a)^2+...+\\frac{e^a}{n!}(x-a)^n \\\\ &amp;= 1+ \\frac{1}{1!}x+\\frac{1}{2!}x^2+...+\\frac{1}{n!}x^n \\end{aligned}\\] if \\(x=2\\) \\(e^2 = 7.389056\\) \\(e^2 \\approx 1+\\frac{1}{1!}x =1+\\frac{1}{1!}2=3\\) \\(e^2 \\approx 1+\\frac{1}{1!}x+\\frac{1}{2!}x^2 =1+\\frac{1}{1!}2 + \\frac{1}{2!}2 =5\\) … \\(e^2 \\approx 1+\\frac{1}{1!}x+\\frac{1}{2!}x^2 +\\frac{1}{3!}x^2+\\frac{1}{4!}x^2+\\frac{1}{5!}x^2=7.2666...\\) 4.2 References Steffen Lauritzen’s slides: http://www.stats.ox.ac.uk/~steffen/teaching/bs2HT9/scoring.pdf The Stack Exchange post: https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function Wilipedia for OLS https://en.wikipedia.org/wiki/Ordinary_least_squares Gradient and Jacobian https://math.stackexchange.com/questions/1519367/difference-between-gradient-and-jacobian https://www.youtube.com/watch?v=3xVMVT-2_t4 https://math.stackexchange.com/questions/661195/what-is-the-difference-between-the-gradient-and-the-directional-derivative Hessian https://en.wikipedia.org/wiki/Hessian_matrix Observed information https://en.wikipedia.org/wiki/Observed_information Fisher information https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf Link function https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function "]
]
