<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 530_533 | Basic Stats</title>
  <meta name="description" content="The webpages are mainly about Bayesian." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 530_533 | Basic Stats" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The webpages are mainly about Bayesian." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 530_533 | Basic Stats" />
  
  <meta name="twitter:description" content="The webpages are mainly about Bayesian." />
  

<meta name="author" content="Bill Last Updated:" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="section-1.html"/>
<link rel="next" href="section-3.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://williamdingpullman.github.io/" target="blank">Bill's Stats Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface: Motivation</a></li>
<li class="chapter" data-level="1" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>1</b> 443</a><ul>
<li class="chapter" data-level="1.1" data-path="section.html"><a href="section.html#some-basic-concepts"><i class="fa fa-check"></i><b>1.1</b> Some basic concepts</a><ul>
<li class="chapter" data-level="1.1.1" data-path="section.html"><a href="section.html#random-variable"><i class="fa fa-check"></i><b>1.1.1</b> Random variable</a></li>
<li class="chapter" data-level="1.1.2" data-path="section.html"><a href="section.html#permutation"><i class="fa fa-check"></i><b>1.1.2</b> Permutation</a></li>
<li class="chapter" data-level="1.1.3" data-path="section.html"><a href="section.html#combinations"><i class="fa fa-check"></i><b>1.1.3</b> Combinations</a></li>
<li class="chapter" data-level="1.1.4" data-path="section.html"><a href="section.html#partitioning"><i class="fa fa-check"></i><b>1.1.4</b> Partitioning</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="section.html"><a href="section.html#discrete-random-variables"><i class="fa fa-check"></i><b>1.2</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="1.2.1" data-path="section.html"><a href="section.html#binomial"><i class="fa fa-check"></i><b>1.2.1</b> Binomial</a></li>
<li class="chapter" data-level="1.2.2" data-path="section.html"><a href="section.html#poisson"><i class="fa fa-check"></i><b>1.2.2</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="section.html"><a href="section.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.3</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="1.3.1" data-path="section.html"><a href="section.html#uniform"><i class="fa fa-check"></i><b>1.3.1</b> Uniform</a></li>
<li class="chapter" data-level="1.3.2" data-path="section.html"><a href="section.html#exponential"><i class="fa fa-check"></i><b>1.3.2</b> Exponential</a></li>
<li class="chapter" data-level="1.3.3" data-path="section.html"><a href="section.html#normal"><i class="fa fa-check"></i><b>1.3.3</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="section.html"><a href="section.html#large-sample-theory"><i class="fa fa-check"></i><b>1.4</b> Large Sample Theory</a><ul>
<li class="chapter" data-level="1.4.1" data-path="section.html"><a href="section.html#convergence-in-distribution"><i class="fa fa-check"></i><b>1.4.1</b> Convergence in distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="section.html"><a href="section.html#weak-law"><i class="fa fa-check"></i><b>1.4.2</b> Weak law</a></li>
<li class="chapter" data-level="1.4.3" data-path="section.html"><a href="section.html#strong-law"><i class="fa fa-check"></i><b>1.4.3</b> Strong law</a></li>
<li class="chapter" data-level="1.4.4" data-path="section.html"><a href="section.html#central-limit-theorem"><i class="fa fa-check"></i><b>1.4.4</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.4.5" data-path="section.html"><a href="section.html#poisson-approximation-to-binomial"><i class="fa fa-check"></i><b>1.4.5</b> Poisson approximation to binomial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>2</b> 556</a><ul>
<li class="chapter" data-level="2.1" data-path="section-1.html"><a href="section-1.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>2.1</b> Statistics and Sampling Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="section-1.html"><a href="section-1.html#statistics"><i class="fa fa-check"></i><b>2.1.1</b> Statistics</a></li>
<li class="chapter" data-level="2.1.2" data-path="section-1.html"><a href="section-1.html#chi2-t-f-beta"><i class="fa fa-check"></i><b>2.1.2</b> <span class="math inline">\(\chi^2, t, F, beta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="section-1.html"><a href="section-1.html#large-sample-approximations"><i class="fa fa-check"></i><b>2.1.3</b> Large-sample approximations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="section-1.html"><a href="section-1.html#point-estimation"><i class="fa fa-check"></i><b>2.2</b> Point Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="section-1.html"><a href="section-1.html#method-of-moments-estimators"><i class="fa fa-check"></i><b>2.2.1</b> Method of moments estimators</a></li>
<li class="chapter" data-level="2.2.2" data-path="section-1.html"><a href="section-1.html#least-squares-estimators"><i class="fa fa-check"></i><b>2.2.2</b> least squares estimators</a></li>
<li class="chapter" data-level="2.2.3" data-path="section-1.html"><a href="section-1.html#likelihood-function-and-maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.2.3</b> likelihood function and maximum likelihood estimators</a></li>
<li class="chapter" data-level="2.2.4" data-path="section-1.html"><a href="section-1.html#invariance-property-of-mles"><i class="fa fa-check"></i><b>2.2.4</b> Invariance property of MLEs</a></li>
<li class="chapter" data-level="2.2.5" data-path="section-1.html"><a href="section-1.html#unbiased-estimators"><i class="fa fa-check"></i><b>2.2.5</b> Unbiased estimators</a></li>
<li class="chapter" data-level="2.2.6" data-path="section-1.html"><a href="section-1.html#unbiased-estimators-vs.-invariance-property-of-mles"><i class="fa fa-check"></i><b>2.2.6</b> Unbiased estimators vs. Invariance property of MLEs</a></li>
<li class="chapter" data-level="2.2.7" data-path="section-1.html"><a href="section-1.html#umvue-and-cramer-rao-lower-bound"><i class="fa fa-check"></i><b>2.2.7</b> UMVUE and Cramer-Rao lower bound</a></li>
<li class="chapter" data-level="2.2.8" data-path="section-1.html"><a href="section-1.html#best-linear-unbiased-estimation-blue-or-mvlue"><i class="fa fa-check"></i><b>2.2.8</b> Best linear unbiased estimation (BLUE or MVLUE)</a></li>
<li class="chapter" data-level="2.2.9" data-path="section-1.html"><a href="section-1.html#consistency-asymptotic-unbiasedness"><i class="fa fa-check"></i><b>2.2.9</b> Consistency, asymptotic unbiasedness</a></li>
<li class="chapter" data-level="2.2.10" data-path="section-1.html"><a href="section-1.html#efficiency"><i class="fa fa-check"></i><b>2.2.10</b> Efficiency</a></li>
<li class="chapter" data-level="2.2.11" data-path="section-1.html"><a href="section-1.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>2.2.11</b> Asymptotic efficiency</a></li>
<li class="chapter" data-level="2.2.12" data-path="section-1.html"><a href="section-1.html#asymptotic-properties-of-mles"><i class="fa fa-check"></i><b>2.2.12</b> Asymptotic properties of MLEs</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="section-1.html"><a href="section-1.html#sufficient-and-completeness"><i class="fa fa-check"></i><b>2.3</b> Sufficient and completeness</a><ul>
<li class="chapter" data-level="2.3.1" data-path="section-1.html"><a href="section-1.html#sufficiency-and-minimal-sufficiency"><i class="fa fa-check"></i><b>2.3.1</b> Sufficiency and minimal sufficiency</a></li>
<li class="chapter" data-level="2.3.2" data-path="section-1.html"><a href="section-1.html#neyman-factorization-theorem-minimal-sufficiency-of-mles"><i class="fa fa-check"></i><b>2.3.2</b> Neyman factorization theorem, minimal sufficiency of MLEs</a></li>
<li class="chapter" data-level="2.3.3" data-path="section-1.html"><a href="section-1.html#rao-blackwell-theorem"><i class="fa fa-check"></i><b>2.3.3</b> Rao-Blackwell theorem</a></li>
<li class="chapter" data-level="2.3.4" data-path="section-1.html"><a href="section-1.html#completeness"><i class="fa fa-check"></i><b>2.3.4</b> completeness</a></li>
<li class="chapter" data-level="2.3.5" data-path="section-1.html"><a href="section-1.html#lehmann-scheffe-completeness-theorem"><i class="fa fa-check"></i><b>2.3.5</b> Lehmann-Scheffe completeness theorem</a></li>
<li class="chapter" data-level="2.3.6" data-path="section-1.html"><a href="section-1.html#exponential-class-complete-sufficient-statistics"><i class="fa fa-check"></i><b>2.3.6</b> Exponential class, complete sufficient statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>3</b> 530_533</a><ul>
<li class="chapter" data-level="3.1" data-path="section-2.html"><a href="section-2.html#definition-of-the-general-linear-model"><i class="fa fa-check"></i><b>3.1</b> Definition of the general linear model</a></li>
<li class="chapter" data-level="3.2" data-path="section-2.html"><a href="section-2.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="section-2.html"><a href="section-2.html#least-squares-and-propoerties-of-the-regression-parameters"><i class="fa fa-check"></i><b>3.2.1</b> Least squares and propoerties of the regression parameters</a></li>
<li class="chapter" data-level="3.2.2" data-path="section-2.html"><a href="section-2.html#mle"><i class="fa fa-check"></i><b>3.2.2</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="section-2.html"><a href="section-2.html#full-rank-less-than-full-rank"><i class="fa fa-check"></i><b>3.3</b> Full rank, less than full rank</a></li>
<li class="chapter" data-level="3.4" data-path="section-2.html"><a href="section-2.html#assumptions-checking-assumptions"><i class="fa fa-check"></i><b>3.4</b> Assumptions, checking assumptions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="section-2.html"><a href="section-2.html#regression-function-is-nonlinear"><i class="fa fa-check"></i><b>3.4.1</b> Regression function is nonlinear</a></li>
<li class="chapter" data-level="3.4.2" data-path="section-2.html"><a href="section-2.html#non-constant-variance"><i class="fa fa-check"></i><b>3.4.2</b> Non-constant variance</a></li>
<li class="chapter" data-level="3.4.3" data-path="section-2.html"><a href="section-2.html#error-terms-not-independent"><i class="fa fa-check"></i><b>3.4.3</b> Error terms not independent</a></li>
<li class="chapter" data-level="3.4.4" data-path="section-2.html"><a href="section-2.html#possibility-of-outliers"><i class="fa fa-check"></i><b>3.4.4</b> Possibility of outliers</a></li>
<li class="chapter" data-level="3.4.5" data-path="section-2.html"><a href="section-2.html#non-normal-distribution-of-error"><i class="fa fa-check"></i><b>3.4.5</b> Non normal distribution of error</a></li>
<li class="chapter" data-level="3.4.6" data-path="section-2.html"><a href="section-2.html#omission-of-some-of-the-important-predictors"><i class="fa fa-check"></i><b>3.4.6</b> Omission of some of the important predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="section-2.html"><a href="section-2.html#bootstrapping-used-in-linear-models"><i class="fa fa-check"></i><b>3.5</b> Bootstrapping used in linear models</a></li>
<li class="chapter" data-level="3.6" data-path="section-2.html"><a href="section-2.html#generalized-linear-models"><i class="fa fa-check"></i><b>3.6</b> Generalized linear models</a><ul>
<li class="chapter" data-level="3.6.1" data-path="section-2.html"><a href="section-2.html#definition-similarities-and-differences-from-general-linear-models"><i class="fa fa-check"></i><b>3.6.1</b> Definition, similarities and differences from general linear models</a></li>
<li class="chapter" data-level="3.6.2" data-path="section-2.html"><a href="section-2.html#advantage-and-disadvantages"><i class="fa fa-check"></i><b>3.6.2</b> Advantage and disadvantages</a></li>
<li class="chapter" data-level="3.6.3" data-path="section-2.html"><a href="section-2.html#logistic-and-poisson-regression"><i class="fa fa-check"></i><b>3.6.3</b> logistic and poisson regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>4</b> 512</a><ul>
<li class="chapter" data-level="4.1" data-path="section-3.html"><a href="section-3.html#basics"><i class="fa fa-check"></i><b>4.1</b> Basics</a><ul>
<li class="chapter" data-level="4.1.1" data-path="section-3.html"><a href="section-3.html#random-vs.-fixed"><i class="fa fa-check"></i><b>4.1.1</b> Random vs. Fixed</a></li>
<li class="chapter" data-level="4.1.2" data-path="section-3.html"><a href="section-3.html#crossed-vs.-nested"><i class="fa fa-check"></i><b>4.1.2</b> Crossed vs. Nested</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="section-3.html"><a href="section-3.html#completely-randomized-designs"><i class="fa fa-check"></i><b>4.2</b> Completely randomized designs</a></li>
<li class="chapter" data-level="4.3" data-path="section-3.html"><a href="section-3.html#randomized-complete-block-designs"><i class="fa fa-check"></i><b>4.3</b> Randomized complete block designs</a></li>
<li class="chapter" data-level="4.4" data-path="section-3.html"><a href="section-3.html#row-column-design-latin-square-designs"><i class="fa fa-check"></i><b>4.4</b> Row-column design (Latin square designs)</a></li>
<li class="chapter" data-level="4.5" data-path="section-3.html"><a href="section-3.html#comparing-rcbd-versus-crd"><i class="fa fa-check"></i><b>4.5</b> Comparing RCBD versus CRD</a></li>
<li class="chapter" data-level="4.6" data-path="section-3.html"><a href="section-3.html#incomplete-block-designs"><i class="fa fa-check"></i><b>4.6</b> Incomplete block designs</a></li>
<li class="chapter" data-level="4.7" data-path="section-3.html"><a href="section-3.html#split-plot-designs"><i class="fa fa-check"></i><b>4.7</b> Split-plot designs</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter-12.html"><a href="chapter-12.html"><i class="fa fa-check"></i><b>5</b> 556 Chapter 12</a></li>
<li class="chapter" data-level="6" data-path="logit-and-probit.html"><a href="logit-and-probit.html"><i class="fa fa-check"></i><b>6</b> Logit and Probit</a><ul>
<li class="chapter" data-level="6.1" data-path="logit-and-probit.html"><a href="logit-and-probit.html#logit"><i class="fa fa-check"></i><b>6.1</b> Logit</a></li>
<li class="chapter" data-level="6.2" data-path="logit-and-probit.html"><a href="logit-and-probit.html#probit"><i class="fa fa-check"></i><b>6.2</b> Probit</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="normal-distribution.html"><a href="normal-distribution.html"><i class="fa fa-check"></i><b>7</b> Normal distribution</a><ul>
<li class="chapter" data-level="7.1" data-path="normal-distribution.html"><a href="normal-distribution.html#basics-1"><i class="fa fa-check"></i><b>7.1</b> Basics</a></li>
<li class="chapter" data-level="7.2" data-path="normal-distribution.html"><a href="normal-distribution.html#confidence-intervals-for-normal-distributions"><i class="fa fa-check"></i><b>7.2</b> Confidence intervals for normal distributions</a></li>
<li class="chapter" data-level="7.3" data-path="normal-distribution.html"><a href="normal-distribution.html#percentile"><i class="fa fa-check"></i><b>7.3</b> Percentile</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>8</b> MLE</a><ul>
<li class="chapter" data-level="8.1" data-path="intro.html"><a href="intro.html#basic-idea-of-mle"><i class="fa fa-check"></i><b>8.1</b> Basic idea of MLE</a></li>
<li class="chapter" data-level="8.2" data-path="intro.html"><a href="intro.html#coin-flip-example-probit-and-logit"><i class="fa fa-check"></i><b>8.2</b> Coin flip example, probit, and logit</a><ul>
<li class="chapter" data-level="8.2.1" data-path="intro.html"><a href="intro.html#probit-1"><i class="fa fa-check"></i><b>8.2.1</b> Probit</a></li>
<li class="chapter" data-level="8.2.2" data-path="intro.html"><a href="intro.html#logit-1"><i class="fa fa-check"></i><b>8.2.2</b> Logit</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="intro.html"><a href="intro.html#further-on-logit"><i class="fa fa-check"></i><b>8.3</b> Further on logit</a></li>
<li class="chapter" data-level="8.4" data-path="intro.html"><a href="intro.html#references"><i class="fa fa-check"></i><b>8.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html"><i class="fa fa-check"></i><b>9</b> Score, Gradient and Jacobian</a><ul>
<li class="chapter" data-level="9.1" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#score"><i class="fa fa-check"></i><b>9.1</b> Score</a></li>
<li class="chapter" data-level="9.2" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#fisher-scoring"><i class="fa fa-check"></i><b>9.2</b> Fisher scoring</a></li>
<li class="chapter" data-level="9.3" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#gradient-and-jacobian"><i class="fa fa-check"></i><b>9.3</b> Gradient and Jacobian</a></li>
<li class="chapter" data-level="9.4" data-path="score-gradient-and-jacobian.html"><a href="score-gradient-and-jacobian.html#hessian-and-fisher-information"><i class="fa fa-check"></i><b>9.4</b> Hessian and Fisher Information</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="canonical-link-function.html"><a href="canonical-link-function.html"><i class="fa fa-check"></i><b>10</b> Canonical link function</a></li>
<li class="chapter" data-level="11" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>11</b> Ordinary Least Squares (OLS)</a><ul>
<li class="chapter" data-level="11.1" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#taylor-series"><i class="fa fa-check"></i><b>11.1</b> Taylor series</a></li>
<li class="chapter" data-level="11.2" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#references-1"><i class="fa fa-check"></i><b>11.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html"><i class="fa fa-check"></i><b>12</b> Cholesky decomposition</a><ul>
<li class="chapter" data-level="12.1" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-1"><i class="fa fa-check"></i><b>12.1</b> Example 1</a></li>
<li class="chapter" data-level="12.2" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-2"><i class="fa fa-check"></i><b>12.2</b> Example 2</a></li>
<li class="chapter" data-level="12.3" data-path="cholesky-decomposition.html"><a href="cholesky-decomposition.html#example-3"><i class="fa fa-check"></i><b>12.3</b> Example 3</a></li>
</ul></li>
<li class="divider"></li>
<li></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basic Stats</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-2" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> 530_533</h1>
<p><a href="https://www.ssc.wisc.edu/sscc/pubs/RegressionDiagnostics.html" class="uri">https://www.ssc.wisc.edu/sscc/pubs/RegressionDiagnostics.html</a></p>
<p><a href="http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/" class="uri">http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/</a></p>
<div id="definition-of-the-general-linear-model" class="section level2">
<h2><span class="header-section-number">3.1</span> Definition of the general linear model</h2>
<p><span class="math display">\[Y=X\beta+\varepsilon\]</span></p>
<p><span class="math display">\[\begin{bmatrix}
Y_1 \\
Y_2  \\
...\\
Y_n \end{bmatrix}=\begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1m}\\
1 &amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2m} \\
...\\
1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nm}
\end{bmatrix} \begin{bmatrix}
\beta_0 \\
\beta_1  \\
...\\
\beta_m \end{bmatrix}+\begin{bmatrix}
\varepsilon_1 \\ \varepsilon_2 \\
...\\
\varepsilon_n  \end{bmatrix}\]</span></p>
<p>Where,</p>
<p><span class="math inline">\(Y\)</span>: Response vector</p>
<p><span class="math inline">\(X\)</span>: Design matrix</p>
<p><span class="math inline">\(\beta\)</span>: parameter vector</p>
<p><span class="math inline">\(\varepsilon\)</span>: error vector</p>
<p>If <span class="math inline">\(\varepsilon\)</span> follows a mutivariate normal distribution then we will be under the General Linear Model (GLM) framework.</p>
<p><span class="math display">\[\varepsilon \sim N(0, \sigma^2I_{x \times n})\]</span></p>
<p>If <span class="math inline">\(X\)</span> is continuous we have regression.</p>
<p>If <span class="math inline">\(X\)</span> is categorical we have ANOVA.</p>
<p>If <span class="math inline">\(X\)</span> is a mix of both, we have ANCOVA.</p>
</div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Simple Linear Regression</h2>
<p>Simple linear regression is a linear regression model with a single explanatory variable. In addition, we typically assume that this is under the GLM framework and thus we also assume that the residuals follow normal distribution.</p>
<div id="least-squares-and-propoerties-of-the-regression-parameters" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Least squares and propoerties of the regression parameters</h3>
<div id="basic-idea" class="section level4">
<h4><span class="header-section-number">3.2.1.1</span> Basic idea</h4>
<p><strong>Of note</strong> The following method can always calculate the vector of <span class="math inline">\(\beta\)</span>, not related to any specific methods.</p>
<p><span class="math display">\[Y_{n \times 1}=X_{n\times m}\beta_{m \times 1}\]</span></p>
<p><span class="math inline">\(\rightarrow\)</span></p>
<p><span class="math display">\[[X^T]_{m \times n}Y_{n \times 1}=[X^T]_{m \times n}X_{n\times m}\beta_{m \times 1}\]</span>
<span class="math inline">\(\rightarrow\)</span></p>
<p><span class="math display">\[[X^TX]^{-1}_{m\times m}[X^TY]_{m \times 1}=\beta_{m \times 1}\]</span></p>
<p><span class="math inline">\(\rightarrow\)</span></p>
<p><span class="math display">\[\beta_{m \times 1}=[(X^TX)^{-1} \cdot (X^TY)]_{m \times 1}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[Var(\hat{\beta})=(X^TX)^{-1}X^T \cdot \sigma^2 \cdot [(X^TX)^{-1}X^T]^T\]</span></p>
<p><strong>Residual is given by <span class="math inline">\(e_i\)</span></strong>, which is an estimate of <span class="math inline">\(\varepsilon_i\)</span>.</p>
<p>Properties of the residuals <span class="math inline">\(e_i\)</span> :</p>
<ol style="list-style-type: decimal">
<li><p>The mean of resituals should be <span class="math inline">\(0\)</span>.</p></li>
<li><p>The variance of <span class="math inline">\(e_i\)</span>:</p></li>
</ol>
<p><span class="math display">\[\begin{aligned} Var(e) &amp;=Var(Y-X\hat{\beta}) \\ &amp;=Var(Y-X(X^TX)^{-1}X^TY) \\ &amp;=Var(I-X(X^TX)^{-1}X^T)Y \\&amp;=(I-X(X^TX)^{-1}X^{T})\sigma^2 \end{aligned}\]</span>
Thus,</p>
<p><span class="math display">\[Var(e) \neq \sigma^2\]</span></p>
</div>
<div id="least-squares" class="section level4">
<h4><span class="header-section-number">3.2.1.2</span> Least Squares</h4>
<p>Assume the following model:</p>
<p><span class="math display">\[Y=X\beta +\varepsilon\]</span></p>
<p>When <span class="math inline">\(\beta\)</span> only has a dimension of <span class="math inline">\(2 \times 1\)</span>, we can write it as follows.</p>
<p><span class="math display">\[Y_i=\beta_0+\beta_1X_i +\varepsilon\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[Q=\sum_{i=1}^n (Y_i-\beta_0-\beta_1X_i)^2\]</span>
We can calculate the partial derivates as follows.</p>
<p><span class="math display">\[\frac{\partial}{\partial \beta_0} Q \rightarrow \sum_{i=1}^n [Y_i-\beta_0-\beta_1X_i]=0\]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial \beta_1} Q \rightarrow \sum_{i=1}^n [Y_i-\beta_0-\beta_1X_i]X_i=0\]</span></p>
<p><strong>Combining the two pieces of information, we can get</strong></p>
<p><span class="math display">\[b_1=\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}\]</span></p>
<p><span class="math display">\[b_0=\frac{1}{n}(\sum Y_i-b_1\sum X_i)=\bar{Y}-b_1\bar{X}\]</span></p>
<p><strong>Properties of Least Squares Estimators (Gauss- Markov theorem):</strong></p>
<p><em>P.18</em></p>
<p>Under the conditions of regression model shown above, the least squares estimator <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are unbiased and have minimum variance among all unbiased linear estimators.</p>
<p>That is,</p>
<p><span class="math display">\[E(b_0)=\beta_0\]</span></p>
<p><span class="math display">\[E(b_1)=\beta_1\]</span></p>
<p><strong>Point Estimation of Mean Response:</strong></p>
<p>Given sample estimators <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> in the regression function</p>
<p><span class="math display">\[E\{Y\}=\beta_0+\beta_1X\]</span>
We estimate the regression function as follows:</p>
<p><span class="math display">\[\hat{Y}=b_0+b_1X\]</span></p>
<p>We call a value of the response variable a response and <span class="math inline">\(E\{Y\}\)</span> the <em>mean response</em>.</p>
<p><strong>Propoerties of fitted regression line</strong></p>
<p><em>P.22</em></p>
<p>The resitual:</p>
<p><span class="math display">\[e_i=Y_i-\hat{Y_i}\]</span>
The sum of the residuals is zero</p>
<p><span class="math display">\[\sum_{i=1}^n e_i=0\]</span></p>
</div>
<div id="point-estimator-of-sigma2" class="section level4">
<h4><span class="header-section-number">3.2.1.3</span> Point estimator of <span class="math inline">\(\sigma^2\)</span></h4>
<p>Note that, there are two <span class="math inline">\(S^2\)</span> below, and they have different formulus, even though the only difference is the degree of freedom and the logic of calculating df is the same across these two cases (i.e., <strong>Single Population vs. Regression Model</strong> ).</p>
<p><strong>1. Single Pupulation</strong></p>
<p><strong>Sum of squares:</strong></p>
<p><em>P.25</em></p>
<p><span class="math display">\[\sum_{i=1}^ne_i^2=\sum_{i=1}^n (Y_i-\bar{Y})^2\]</span></p>
<p>The sum of squares is then divided by the degrees of freedom associated with it.</p>
<p><span class="math display">\[S^2= \frac{\sum_{i=1}^n (Y_i-\bar{Y})^2}{n-1}\]</span></p>
<p><span class="math inline">\(S^2\)</span> is an unbiased estimator of the variance <span class="math inline">\(\sigma^2\)</span>. The sample variance <span class="math inline">\(S^2\)</span> is often called a mean square, because a sum of squares has been divided by the appropriate number of degrees of freedom.</p>
<p><span class="math display">\[E(S^2)=\sigma^2\]</span></p>
<p><strong>2. Regression model</strong></p>
<p><strong>Sum of Squares Error (SSE) (or, Sum of Squares Residual): </strong></p>
<p><span class="math display">\[SSE=\sum_{i=1}^n (Y_i-\hat{Y_i})^2=\sum_{i=1}^n e_i^2\]</span></p>
<p><strong>Mean Square Error (MSE) (or, Mean Square Residual):</strong></p>
<p><span class="math display">\[S^2=MSE=\frac{SSE}{n-2}=\frac{\sum(Y_i-\hat{Y_i})^2}{n-2}=\frac{\sum e_i^2}{n-2}\]</span>
<span class="math inline">\(MSE\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span> for regression model.</p>
<p><span class="math display">\[E(MSE=S^2)=\sigma^2\]</span></p>
</div>
</div>
<div id="mle" class="section level3">
<h3><span class="header-section-number">3.2.2</span> MLE</h3>
<p><em>P.30</em></p>
<p>Note that the <strong>Normal Error Regression Model</strong> is as follows.</p>
<p><span class="math display">\[Y_i=\beta_0+\beta_1X_i +\varepsilon\]</span>
(This is a special case of the definition provided earlier, i.e., <span class="math inline">\(Y=X\beta +\varepsilon\)</span>)</p>
<p>For this model, each <span class="math inline">\(Y_i\)</span> observation is normally distributed with mean <span class="math inline">\(\beta_0+\beta_1x_i\)</span> and a standard devision <span class="math inline">\(\sigma\)</span>.</p>
<p>Givern <span class="math inline">\(Y_i\)</span> follows normal distribution, we can use <span class="math inline">\(pdf\)</span> of normal distributions and MLE to estimate parameters.</p>
<p><span class="math display">\[\begin{aligned} L(\beta_0,\beta_1,\sigma^2) &amp;=\prod_{i=1}^n \frac{1}{(2\pi \sigma^2)^{1/2}}exp[-\frac{1}{2 \sigma^2}(Y_i-\beta_0-\beta_1X_i)^2] \\ &amp;=\frac{1}{(2\pi \sigma^2)^{n/2}}exp[-\frac{1}{2 \sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2]  \end{aligned}\]</span>
Since the variance <span class="math inline">\(\sigma^2\)</span>is usually unknown, the likelihood fucntion is a function of three parameters, <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Thus, we can calculate <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma^2\)</span> analytically, and the results of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the same as least squares estimators (see above).</p>
<p>The variance <span class="math inline">\(\sigma^2\)</span> is as follows.</p>
<p><span class="math display">\[\hat{\sigma}^2=\frac{\sum(Y_i-\hat{Y_i})^2}{n}\]</span></p>
<p>As noted in the least square estimation, we know that <span class="math inline">\(\hat{\sigma}^2\)</span> is biased. Thus, we know that:</p>
<p><span class="math display">\[S^2=MSE=\frac{n}{n-2}\hat{\sigma}^2\]</span></p>
</div>
</div>
<div id="full-rank-less-than-full-rank" class="section level2">
<h2><span class="header-section-number">3.3</span> Full rank, less than full rank</h2>
<p><a href="http://www.biostat.jhsph.edu/~iruczins/teaching/140.751/notes/ch7.pdf" class="uri">http://www.biostat.jhsph.edu/~iruczins/teaching/140.751/notes/ch7.pdf</a></p>
<p><span class="math display">\[Y_{n \times 1}=X_{n\times m}\beta_{m \times 1}\]</span></p>
<p>If the rank <span class="math inline">\(r\)</span> of <span class="math inline">\(X_{n \times m}\)</span> is smaller than <span class="math inline">\(m\)</span>, i.e., <span class="math inline">\(r&lt;m\)</span>, there is not a unique solution <span class="math inline">\(\hat{\beta}\)</span>. We have three ways to find a solution <span class="math inline">\(\hat{\beta}\)</span> and the orthogonal projection <span class="math inline">\(\hat{Y}\)</span>:</p>
<p><strong>1. Reducing the model to the full rank.</strong></p>
<p>Let <span class="math inline">\(X_1\)</span> consist of <span class="math inline">\(r\)</span> linear independent columns from <span class="math inline">\(X\)</span> and let <span class="math inline">\(X_2\)</span> consistn of the remaining columns. Then, <span class="math inline">\(X_2=X_1F\)</span> because the columns of <span class="math inline">\(X_2\)</span> are linearly dependent on the columns of <span class="math inline">\(X_1\)</span>.</p>
<p><span class="math display">\[X=(X_1, X_2)=(X_1, X_1F)=X_1[I_{r\times r}, F]\]</span>
This is a special case of the factorization <span class="math inline">\(X=KL\)</span>, where rank <span class="math inline">\((K_{n \times r})=r\)</span> and rank <span class="math inline">\((L_{r\times p})=r\)</span>.</p>
<p><span class="math display">\[E[Y]=x\beta=KL\beta=k\alpha\]</span>
Since <span class="math inline">\(K\)</span> has full rank, the Least Squares Estimate of <span class="math inline">\(\alpha\)</span> is <span class="math inline">\(\hat{\alpha}=(K^T K)^{-1} \cdot K^TY\)</span>.</p>
<p>The orthogonal project,</p>
<p><span class="math display">\[\hat{Y}=K\cdot \alpha=K \cdot(K^T K)^{-1} \cdot K^TY=X_1 \cdot(X_1^TX_1)^{-1}\cdot X_1^TY\]</span></p>
<p><span class="math display">\[\begin{bmatrix}
Y_{11} \\ ... \\ Y_{1n_1} \\
Y_{21}  \\ ... \\ Y_{2n_2} \end{bmatrix}=
\begin{bmatrix}
1 &amp; 1 &amp; 0\\
... &amp; ... &amp; ... \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\ 
... &amp; ... &amp; ... \\
1 &amp; 0 &amp;1 \end{bmatrix} 
\begin{bmatrix}
\mu \\
\beta_1  \\
\beta_2\\
\end{bmatrix}+\begin{bmatrix}
\varepsilon_{11} \\ ... \\ \varepsilon_{1n_1} \\
\varepsilon_{21}  \\ ... \\ \varepsilon_{2n_2}  \end{bmatrix}\]</span></p>
<p>Let <span class="math inline">\(X_1\)</span> consist of the first 2 columns of <span class="math inline">\(X\)</span>, then</p>
<p><span class="math display">\[X=X_1 \begin{bmatrix}
1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; -1 \end{bmatrix}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[X_1=K=\begin{bmatrix}
1 &amp; 1 \\
... &amp; ...  \\
1 &amp; 1 \\
1 &amp; 0  \\ 
... &amp; ... \\
1 &amp; 0  \end{bmatrix} \]</span></p>
<p><span class="math display">\[\hat{\alpha}=(K^T K)^{-1} \cdot K^TY=\begin{bmatrix}
n &amp; n_1 \\
n_1 &amp; n_1 \end{bmatrix}^{-1} \begin{bmatrix}
\sum Y_{1j}+\sum y_{2j} \\
\sum y_{1j} \end{bmatrix} =\begin{bmatrix}
\bar{Y_2}\\ \bar{Y_1}-\bar{Y_2} \end{bmatrix}\]</span></p>
<p><span class="math display">\[\hat{Y}=X_1 \hat{\alpha}=\begin{bmatrix}
1 &amp; 1 \\
... &amp; ...  \\
1 &amp; 1 \\
1 &amp; 0  \\ 
... &amp; ... \\
1 &amp; 0  \end{bmatrix} \begin{bmatrix}
\bar{Y_2}\\ \bar{Y_1}-\bar{Y_2} \end{bmatrix}=\begin{bmatrix}
\bar{Y_1}  \\
...   \\
\bar{Y_1} \\
\bar{Y_2}   \\ 
... \\
\bar{Y_2} \end{bmatrix}\]</span></p>
<p><strong>2. Find the generalized inverse <span class="math inline">\((X^TX)^{-1}\)</span>. </strong></p>
<p>As noted above, when <span class="math inline">\(X^TX\)</span> has a full rank, we can directly calculate the inverse of <span class="math inline">\(X^TX\)</span>. That is,</p>
<p><span class="math display">\[\beta_{m \times 1}=[(X^TX)^{-1} \cdot (X^TY)]_{m \times 1}\]</span></p>
<p>We can just find <strong>some columns</strong> within <span class="math inline">\(X\)</span> that are independent, and then calculate the inverse of it.</p>
<p>This is because if a matrix <span class="math inline">\(W\)</span> with a rank <span class="math inline">\(r\)</span> and can be partitioned as follows.</p>
<p><span class="math display">\[W=\begin{bmatrix}
A &amp; B  \\
C &amp; D \end{bmatrix}\]</span></p>
<p>Assume <span class="math inline">\(A\)</span> has rank <span class="math inline">\(r\)</span>, then</p>
<p><span class="math display">\[W^{-1}=\begin{bmatrix}
A^{-1} &amp;0  \\
0 &amp; 0 \end{bmatrix}\]</span></p>
<p>Thus, let <span class="math inline">\(X=(X_1, X_2)\)</span>, where <span class="math inline">\(X_1\)</span> consists of <span class="math inline">\(r\)</span> linearly independent columns from <span class="math inline">\(X\)</span>. Then a generalized inverse of <span class="math inline">\(X^TX\)</span> is</p>
<p><span class="math display">\[[X^TX]^{-1}=\begin{bmatrix}
[X_1^TX_1]^{-1}&amp;0  \\
0 &amp; 0 \end{bmatrix}\]</span></p>
<p>Thus, all other steps are similar to the full rank case.</p>
<p><strong>3. Impose identififiability constraints</strong></p>
<p>(Not very sure this one.)</p>
</div>
<div id="assumptions-checking-assumptions" class="section level2">
<h2><span class="header-section-number">3.4</span> Assumptions, checking assumptions</h2>
<p><strong>The following are some of the possible violations of assumptions:</strong></p>
<div id="regression-function-is-nonlinear" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Regression function is nonlinear</h3>
<p>Plot explanatory and response variables to see whether their relationship is linear, it is not, suggesting that a linear regression function is not appropriate.</p>
<p><strong>Stat Methods:</strong></p>
<p>F-test for lack of fit.</p>
</div>
<div id="non-constant-variance" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Non-constant variance</h3>
<p>Plots of the residuals against the predictor variable (i.e., <span class="math inline">\(X\)</span>) or against the fitted values (i.e., <span class="math inline">\(\hat{Y}\)</span>), to study whether the linear model is appropriate and whether the <strong>variance of the error terms</strong> is constant.</p>
<p><strong>Stat Methods:</strong></p>
<p><strong>(1) Modified Levene’s test (Brown-Forsythe test):</strong></p>
<p>To check and see if the variability for <span class="math inline">\(Y\)</span> for the smaller <span class="math inline">\(X&#39;s\)</span> are different than the vairability of <span class="math inline">\(Y\)</span> for the larger <span class="math inline">\(X&#39;s\)</span>.</p>
<p>Thus, we can break up the data into two groups based <span class="math inline">\(X&#39;s\)</span> values. Then, we test to see if the vairability of these two groups significantly differ.</p>
<p><strong>Note that</strong> Modefied Levene’s test does not depend on normality of the error terms. That is, this test is robust against serious departures from normality.</p>
<p><strong>(2) Breusch-Pagan test:</strong></p>
<p>Want to see if there is any relationship between <span class="math inline">\(\sigma_i\)</span> and the <span class="math inline">\(X_i\)</span>’s. To do this, we can fit a regression of <span class="math inline">\(log(\sigma_i)\)</span> on <span class="math inline">\(X_i\)</span>’s:</p>
<p><span class="math display">\[log(\sigma_i)=b_0+b_1X_i\]</span></p>
</div>
<div id="error-terms-not-independent" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Error terms not independent</h3>
<p>We want a random pattern. Any type of pattern indicates time orderred problems indicates that the model is not appropriate.</p>
<p><strong>Stat Methods:</strong></p>
<p><strong>Durbin Watson Test</strong> Test for presence of autocorrelation amongst observations.</p>
</div>
<div id="possibility-of-outliers" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Possibility of outliers</h3>
<p>Scatter plot the dependent variable and independent variables (each <span class="math inline">\(X_i\)</span> is plotted seprately.)</p>
</div>
<div id="non-normal-distribution-of-error" class="section level3">
<h3><span class="header-section-number">3.4.5</span> Non normal distribution of error</h3>
<p>Use histogram to check whether the resitual follows normal distribution.</p>
<p><strong>Stat Methods:</strong></p>
<p><em>P.115</em></p>
<p>Correlation between ordered residuals and their expected values under normality.</p>
<p><em>P.70 512 note</em></p>
<p>You can use Shapiro-Wilk, Kolmogorov-Smirnov, Cramer-von Mises, Anderson-Darling in SAS to assess the normal errors.</p>
</div>
<div id="omission-of-some-of-the-important-predictors" class="section level3">
<h3><span class="header-section-number">3.4.6</span> Omission of some of the important predictors</h3>
<p>Plots should be made betweeen variables omitted from the model and the dependent variable. Such omitted variables may have important effects on the response.</p>
</div>
</div>
<div id="bootstrapping-used-in-linear-models" class="section level2">
<h2><span class="header-section-number">3.5</span> Bootstrapping used in linear models</h2>
<p><em>P.458</em></p>
<p><strong>Intro</strong></p>
<p>For standard fitted regression models, methods described earlier chapters are available for evaluating the precision of estimated regression coefficients, fitted values, and predictions of new observations.</p>
<p>However, in many nonstandard situations, such as when nonconstant error variance are estimated by iteratively reweighted least sqaure or when robust regression estimation is used, standard methods for evaluating the precision may not be available or may only approximately applicable when the sample size is large.</p>
<p>Bootstrapping was developed by Efron to provide estimates of the precision of sample estimates for these complex cases.</p>
<p><strong>Conceptual</strong></p>
<p>Suppose that we have fitted a regression model (simple or multiple) by some procedure and get the coefficient <span class="math inline">\(b_1\)</span>. We now wish to evaluate the precision of this esimtate by bootsrap method.</p>
<ol style="list-style-type: decimal">
<li><p>In essensce, the bootstrap method call for the selection from the observed sample data of a random sample of size <span class="math inline">\(n\)</span> with replacement.</p></li>
<li><p>Next, the bootstrap method calculates the estimated regression coefficient from the boostrap sample, using the same fitting procedure as employed for the original fitting. This leads to the first bootstrap estimate <span class="math inline">\(b_1^*\)</span>.</p></li>
</ol>
<p>This procedure repeated a large number of times, each time a bootstrap sample of size <span class="math inline">\(n\)</span> is selected with replacement from the original sample and the estimated coefficient is obtained.</p>
<ol start="3" style="list-style-type: decimal">
<li>The estimated standard deviation of all the bootstrap estimates <span class="math inline">\(b_1^*\)</span> denoted by <span class="math inline">\(S^*\{b_1^*\}\)</span>, is an estimate of the variability of the sampling distribution of <span class="math inline">\(b_1\)</span> and therefore a measure of the prevision of <span class="math inline">\(b_1\)</span>.</li>
</ol>
<p><strong>Some Math</strong></p>
<p>Bootstrapping sampling for regression can be done in two basic ways.</p>
<ol style="list-style-type: decimal">
<li>Whne the regression fucntion being fitted is a good model for the data, the error terms have constant variance, and the predictor variables can be regarded as fixed, fixed X sampling is appropriate.</li>
</ol>
<p>Here, the resitudals <span class="math inline">\(e_i\)</span> from the original fitting are regarded as the sample data to be sampled with replacement. After bootstrap sample of the resituals of size n have beeb obtained, denoted by <span class="math inline">\(e_1^*, ..., e_n^*\)</span>. The bootstrap sample residuals are added to the fitted values from the original fitting to obtain nuew bootstrap Y values, denoted by <span class="math inline">\(Y_1^*,...,Y_n^*\)</span>:</p>
<p><span class="math display">\[Y_i^*=\hat{Y_i}+e_i^*\]</span>
These bootstrap <span class="math inline">\(Y^*\)</span> values are then regressed on the original <span class="math inline">\(X\)</span> variables by the same procedure used initially to obtain the bootstrap estimate <span class="math inline">\(b_1^*\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>When there is some doubt about the adequacy of the regressio nmodel, or the rror variances are not constant, or the predictor variables can not be regarded as fixed, random X sampling is appropriate.</li>
</ol>
<p>For simple regression, the pair <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> data in the oringal sample are considered to be the data to be sampled with replacedment. This this second procedure samples cases with replacement $n
times, yelding a bootstrap sample of <span class="math inline">\(n\)</span> pairs of (<span class="math inline">\(X^*,Y^*\)</span>) values. This bootstrap sample is then used for obtaining the bootstrap estimate <span class="math inline">\(b_1\)</span>, as with fixed <span class="math inline">\(X\)</span> sampling.</p>
<p><strong>Bootstrap confidence intervals</strong></p>
<p><em>P.460</em></p>
</div>
<div id="generalized-linear-models" class="section level2">
<h2><span class="header-section-number">3.6</span> Generalized linear models</h2>
<div id="definition-similarities-and-differences-from-general-linear-models" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Definition, similarities and differences from general linear models</h3>
<p>Generalized Linear Model (GLiM) loosens this assumption that <span class="math inline">\(\varepsilon\)</span> follows a mutivariate normal distribution, and allows for a variety of other distributions from the exponential family for the residuals.</p>
<p>Of note, the GLM is a special case of the GLiM in which the distribution of the residuals follow a conditionally normal distribution.</p>
</div>
<div id="advantage-and-disadvantages" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Advantage and disadvantages</h3>
</div>
<div id="logistic-and-poisson-regression" class="section level3">
<h3><span class="header-section-number">3.6.3</span> logistic and poisson regression</h3>
<p><strong>Logistic regression</strong></p>
<p>Logistic regression is a special case of GLiM with binomial distribution on the <span class="math inline">\(Y&#39;s\)</span> and the link function</p>
<p>The basic idea of logistic regression:
<span class="math display">\[p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span></p>
<p>Thus, <span class="math inline">\(\beta_0+\beta_1x_1+...+\beta_nx_n\)</span> can be from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>, and <span class="math inline">\(p(y=1)\)</span> will be always within the range of <span class="math inline">\((0,1)\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" title="1">f&lt;-<span class="cf">function</span>(x){<span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x))}</a>
<a class="sourceLine" id="cb1-2" title="2">data&lt;-<span class="kw">seq</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1-3" title="3"><span class="kw">plot</span>(data,<span class="kw">f</span>(data),<span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</a></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>We can also write the function into another format as follows:
<span class="math display">\[log \frac{p(y=1)}{1-p(y=1)}= \beta_0+\beta_1x_1+...+\beta_nx_n\]</span>
Thus, we know that the regression coeficients of <span class="math inline">\(\beta_i\)</span> actually change the “log-odds” of the event. Of course, note that the magnitude of <span class="math inline">\(\beta_i\)</span> is dependent upon the units of <span class="math inline">\(x_i\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/0000_3_530533.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
