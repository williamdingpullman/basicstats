# 556 Chapter 12

## 12.1

### THeory

If $X \sim f(x;\theta)$, a __statistical hypothesis__ is a statement about the distribution of $X$. If the hypothesis completely specifies $f(x;\theta)$, then it is referred to as a __simple hypothesis__; otherwise it is called __composite__.

Quite often the distribution in question has __a known parametric form__ with a single unknown parameter $\theta$, and the hypothesis consists of a statement about $\theta$. 

In this framework, a statistical hypothesis corresponds to a subset of the parameter space, and the objective of a test would be to decide whether the true value of the parameter is in the subset. Thus, a null hypothesis would correspond to a subset $\Omega_0$ of $\Omega$, and the alternative hypothesis would correspond to its complement, $\Omega-\Omega_0$

In the case of simple hypothesis, these sets consist of only one element each, $\Omega_0=\{ \theta_0\}$ and $\Omega -\Omega_0=\{ \theta_1\}$, where $\theta_0 \neq \theta_1$.

We now must consider sample data, and decide on the basis of the data whether we have sufficient statistical evidence to reject $H_0$ in favor of the alternative $H_a$, or whether we do not have sufficient evidence.

That is, our philosophy will be to divide the sample space into two regions, the "critical region" or "rejection region" $C$, and the nonrejection region $S-C$. If the observed sample data fall in $C$, then we will reject $H_0$.

$\bar{X}$ is a sufficent statistic for $\mu$, so we may conveniently express the critical region directly in terms of the univariate variable $\bar{X}$, and we will refer $\bar{X}$ as the test statstic. Because $\mu_1 > \mu_0$, a natural form for the critical region in this problem is to let $C=\{ (x_1,..., x_n) | \bar{x} \geq c\}$, for some appropriate constant $c$. That is, we will reject $H_0$ if $\bar{x} \geq c$, and we not reject $H_0$ if $\bar{x} < c$.

There are two possible error:

1. Type I error: reject a true $H_0$. $P[Type I error]=\alpha$.

2. Type II error: fail to reject a false $H_0$. $P[Type II error]=\beta$.


Note that, $P[Type I error]=\alpha$ is also referred to as the __significance level__ of the test. 


### Example

For instance, if $n=25$, then $\alpha=0.05$ gives $c=\mu_0+Z_{1-\alpha}\sigma/\sqrt{n}=10+1.645 \cdot 4/5=11.316$.

We can verify as follows.

$$\begin{aligned} P[\bar{X}\geq c | \mu=\mu_0=10] &=P[\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}} \geq \frac{c-\mu_0}{\sigma/\sqrt{n}}] \\ &=P[Z \geq \frac{11.316-10}{4/5}] \\&=P[Z\geq 1.645] \\&=0.05 \end{aligned}$$

Thus, a size $0.05$ test of $H_0: \mu=10$ against the alternative $H_a:\mu=11$ is to reject $H_0$ if the observed value $\bar{X} \geq 11.316$.


Note that, the calculation of $C$ has nothing to do with the alternative. However, $c$ impacts type II error:

$$\begin{aligned} \beta=P[TII] &=P[\bar{X}<11.316 |\mu=\mu_1=11] \\ &=P[\frac{\bar{X}-11}{4/5}<\frac{11.316-11}{4/5}] \\ &=P[Z <0.395] \\ &=0.645 \end{aligned}$$

__My personal note__:

(1) $C$ is only determined by $H_0$, but the choose of $C$ can impact type II error. The basic idea of the calculation a region where the probability there will be $0.05$. And if the observed mean falls into that region, we can reject $H_0$. 

(2) Type II error is to calculate the __probability__ that __NOT__ in the __critical region__ defined by __null hypothesis__. 


Thus, we can set another critical area to illustrate this.

$$P[10 < \bar{X}<10.1006]=P[0 <\frac{\bar{X}-10}{4/5} <0.1257]=0.05$$

However, $P[Type II error]$ for this critical regiion is:

$$\begin{aligned} P[TII] &=1-P[10<\bar{X}<10.1006|\mu=11] \\ &=1-P[\frac{10-11}{4/5}<Z<\frac{10.1006-11}{4/5}] \\ &=1-P[-1.25 <Z<-1.12425] \\ &=0.9752 \end{aligned}$$

If we increase the sample size from 25 to 100
