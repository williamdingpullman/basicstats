                                                                            # 556
# 556

## Statistics and Sampling Distributions

### Statistics 

#### Definition of Statistic

_P.264_

A fucntion of observable random variables, $T=t(X_1, ... X_n)$, which does not depend on any unknown parameters is called statistic.

For example, let $X_1, ..., X_n$ represent a random sample from a population with $pdf \;f(x)$. The sample mean provides an example of a statistic with the function 

$$t(x_1,...,x_n)=(x_1+...+x_n)/n$$

This statistic usually is denoted by 

$$\bar{X}=\sum_{i=1}^n \frac{X_i}{n}$$

When a random sample is observed, the value of $\bar{X}$, computed from the data, usually is denoted by lower case $\bar{x}$.

$$\bar{x}=\sum_{i=1}^n \frac{x_i}{n}$$


#### Sample and parameters

_P.265_

If $X_1,..., X_n$ denotes a random sample from $f(x)$ with $E(X)=\mu$ and $var(X)=\sigma^2$, then 

$$E(\bar{X})=\mu$$

$$Var(\bar{X})=\frac{\sigma^2}{n}$$

For example, a random sample of size $n$ from a Bernoulli distribution $X_i \sim BIN(1,p)$. We know Bernoulli has $\mu=p$ and $\sigma^2 =pq$. In this case, the sample mean is 

$$\bar{X}=Y/n=\hat{p}$$

Thus, 

$$E(\hat{p})=p$$

$$Var (\hat{p})=\frac{pq}{n}$$

Thus, sample mean is the unbiased estimate for the population mean. However, you can not use sample mean's variance to estimate pupulation variance. That lead to definition of sample variance. 

_P.266_

Sample variance:

$$S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$$

$$E(S^2)=\sigma^2$$

### $\chi^2, t, F, beta$

#### $\chi^2$

Always squre from standard normal, and the standardization can be using $\mu$ or $\bar{X}$.


$$\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2(n-1)$$
(Thus, we can see this is a bit weired, as the numerator is $\bar{X}$ is from the sample, whereas $\sigma^2$ is from the population.)

Thus, we can 

$$\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sigma^2}\sim \chi^2(n-1)$$
(You can compare $\bar{X}$ with $\mu$, we can see the only difference is that the $\chi^2$ has one less degree of freedom because we use this degree of freedom to calculate the mean.)


For the mean and variance of $\chi^2$:

Assume that

$$X \sim x^2(v)$$

$$mean:v$$
$$variance: 2v$$


#### $t$

Definition

$$t(k)=\frac{N(0,1)}{\sqrt{\frac{\chi^2(k)}{k}}}$$


__Property 1__

__t__ distribution is symmetrical


Given that _t_ distribution is symmetrical, we can get

$$H(-c)=1-H(c)$$


__Property 2__

_t_ distribution has heavier tails than the normal.


My note: _t_ distribution only has a parameter of $k$, which is determined by the $\chi^2$'s degree of freedom. Of course, $\chi^2$ also only has one parameter, namely the degree of freedom.


#### $F$

If $V_1 \sim \chi^2(v_1)$ and $V_2 \sim \chi^2(v_2)$ are independent, then the random variable

$$\frac{V_1/v_1}{V_2/v_2}\sim F(v_1,v_2)$$


#### Beta

If $X \sim F(v_1, v_2)$

$$Y=\frac{(v_1/v_2)X}{1+(v_1/v_2)X} \sim Beta(\alpha, \beta)$$


### Large-sample approximations


_P.280_

If $Y_v \sim x^2(x)$, then


$$Z_v =\frac{Y_v-v}{\sqrt{2v}} \xrightarrow{d} Z \sim N(0,1)$$

(The proof is based on CLT.)


## Point Estimation

### Method of moments estimators

#### Definition about moments (chapter 2)

_P.73_

THe __kth moment about the origin__ of a random variable $X$ is 

$$\mu^{'}_k=E(X^k)$$

and the __kth moment about the mean__ is 

$$\mu_k=E[X-E(X)]^k=E(X-\mu)^k$$

Thus,$k=E(X^k)$ may be considered as the $k$th moment of $X$ or the first moment of $X^k$. 

The first moment about the mean is zero,

$$\mu_1=E[X-E(X)]=E(X)-E(X)=0$$

The second moment about the mean is the variance,

$$\mu_2=E[(X-\mu)^2]=\sigma^2$$

Note that the definition of variance:

_P.73_

$$Var(X)=E[(X-\mu)^2]$$

#### Definition

Based on the last chapater (i.e., Chapter 8), sample mean $\bar{X}$ is an estimator of the population mean $\mu$. A more general approach, which produced estimators known as the __method of moments estimators(MMEs)__ , can be developed. 


If $X_1,...,X_n$ is a random sample from $f(x; \theta_1,...,\theta_k)$, the first $k$ sample moments are given by 


$$M_j^{'}=\frac{\sum_{i=1}^n X_i^j}{n}$$


where,

$$j=1,2,...k$$



__Example 1__


_P.291_

Consider a random sample from a distribution with two unknown parameters, the mean $\mu$ and the variance $\sigma^2$. We know from earlier considerations that $\mu=\mu^{'}_1$ and $\sigma^2=E(X^2)-\mu^2=\mu_2^{'}-(\mu^{'}_1)^2$.

Thus,

$$\hat{\sigma}^2=\mu_2^{'}-(\mu^{'}_1)^2=\frac{\sum_{i=1}^n X_i^2}{n}-\frac{\sum_{i=1}^n X_i}{n}=\frac{\sum_{i=1}^n X_i^2}{n}-\bar{X}=\sum_{i=1}^n \frac{(X_i-\bar{X})^2}{n}$$

(Thus, we can see that the estimation of $\sigma^2$ is not the same as the definition of sample variance $S^2$. $\hat{\sigma}^2=\frac{n-1}{n}S^2$.)


__Example 2__

_P.292_

If a sample is from a Gamma distribution $X_i \sim GAM(\theta,k)$, and we want to estimate the $\theta$ and $k$.

We know that for Gamma distribution, the mean is $k\theta$, and the variance is $k\theta^2$.

We also know that $\mu_1^{'}=\mu=k\theta$ and $\mu_2^{'}= \sigma^2+\mu^2= k\theta^2+k^2\theta^2=k\theta^2(1+k)$.

Thus, we can get

$$\bar{X}=k\theta$$

$$\sum \frac{X_i^2}{n}=k\theta^2(1+k)$$

Thus, we can get

$$\hat{\theta}=\sum_{i=1}^n \frac{(X_i-\bar{X})^2}{n \bar{X}}=\frac{(n-1)/n S^2}{\bar{X}}$$


$$\hat{k}=\frac{\bar{X}}{\hat{\theta}}$$

#### Property

The joint MGF of ($X_1, ..., X_n$) is defined as $M(t_1,...,t_n)=E(e^{\sum_{i=1}^nt_iX_i})$

When $X_1, ..., X_n$ are independent if and only if 

$$M(t_1,...,t_n)=\prod_{i=1}^n M_{X_i}(t_i)$$
where $M_{X_i}(t_i)$ is the MGF of $X_i$

#### Well-known MGF

(1) Bernoulli with success probability p: $1-p+pe^t$

(2) Binomial Bin(n,p): $(1-p+pe^t)^n$

(3) Poisson $POI(\lambda)$: $e^{\lambda(e^t-1)}$

(4) Normal $N(\mu,\sigma^2)$: $e^{\mu t+\frac{1}{2}\sigma^2t^2}$

(5) Gamma $GAM(\theta,k)$: $(1-\theta t)^{-k}$


Two special cases:

(6) Chi-square $\chi^2(v) =GAM(2,\frac{v}{2})$: $(1-2t)^{-\frac{v}{2}}$

(7) Exponential $EXP(\theta)=GAM(\theta,1)$: $(1-\theta t)^{-1}$


### least squares estimators


### likelihood function and maximum likelihood estimators

#### Likelihood function

_P.293_

The joint density function of $n$ random variables $X_1,...X_n$ evaluated at $x_1, ...x_n$, say $f(x_1,...,x_n; \theta)$, is referered to as a _likelihood function_.

#### Maximum likelihood estimators

_P.294_

Let $L(\theta)=f(x_1,...x_n; \theta), \theta \in \Omega$, be the joint pdf of $X_1, ..., X_n$. For a given set of observations, $(x_1,...x_n)$, a value $\hat{\theta}$ in $\Omega$ at which $L(\theta)$ is a maximum is called a _maximum likelihood estimate (MLE)_ of $\theta$. That is $\hat{\theta}$ is a value of $\theta$ that satisfies

$$f(x_1,...x_n; \theta)=MAX_{\theta \in \Omega} f(x_1, ..., x_n; \theta)$$

### Invariance property of MLEs

_P.296_

If $\hat{\theta}$ is the MLE of $\theta$ and if $u(\theta)$ is a function of $\theta$, then $u(\hat{\theta})$ is an MLE of $u(\theta)$.


__Example 1__

We know that the $pdf$ of exponential distribution ($X \sim EXP (\theta)$) is as follows:

$$\frac{1}{\theta} e^{-\frac{X}{\theta}}$$

Thus, its likelihood function is as follows

$$L(\theta)=\frac{1}{\theta^n}e^{-\frac{\sum X_i}{\theta}}$$
Thus, log-likelihood is as follows.

$$lnL(\theta)=-n ln(\theta)-\frac{\sum X_i}{\theta}$$
Thus,

$$\frac{d}{d\theta} lnL(\theta)=-n \frac{1}{\theta}+\frac{\sum X_i}{\theta^2}$$

Thus, we can get the $MLE$ for $\theta$ is $\hat{\theta}=\bar{x}$.

If we want to estimate $\tau(\theta)=P(X \geq 1)$:

$$\tau(\theta)=P(X\geq 1)=\int_1^{\infty} \frac{1}{\theta} e^{-\frac{X}{\theta}} dx=-\int_1^{\infty}  e^{-\frac{X}{\theta}} d(-\frac{x}{\theta})=-[e^{-\frac{X}{\theta}}]_1^{\infty}=-[0-e^{-\frac{1}{\theta}}]=e^{-\frac{1}{\theta}}$$

Thus, based on the invariance property, we know that the $MLE$ for $\tau(\theta)$ is as follows.

$$e^{-\frac{1}{\bar{x}}}$$

__Example 2: MLE vs. MME__

_P.296_

Assume a random sample from a two-parameter exponential distribution, $X_i \sim EXP(1)$. Thus, the $pdf$ is $e^{-(x-\eta)}$. Thus, the likelihood function is 

$$L(\eta)=e^{-\sum(x_i-\eta)}$$

Thus, the log likelihood,

$$lnL(\eta)=-\sum(x_i-\eta)=n\eta-n\bar{X}$$
Thus, we know that as $\eta$ increases, the log likelihood increases accordingly. Thus, we want to find the maximum $\eta$. Note that a two-parameter exponential distribution has the support of $x_i \geq \eta$. Thus, all $\eta$ are smaller than any $X_i$. Thus, we can get the ML estimator is the first order statistic 

$$\hat{\eta}=X_{1:n}$$

__Note that__ the estimators above is based on ML. What would be the answer if using MME?

We know that for a two-parameter exponential distribution, its mean is $\mu=1+\eta$. And, we know that based on MME, $\mu=\bar{X}$. Thus, we can get the following,

$$\hat{\eta}=\bar{X}-1$$

__Conclusion__

We can see that ML and MME have didferent estimators for the same $\eta$ for a two-parameter exponential distribution.


### Unbiased estimators

An estimator $T$ is said to be an unbiased estimator of $\tau(\theta)$ if 

$$E(T)=\tau(\theta)$$

for all $\theta \in \Omega$. Otherwise, we said that $T$ is biased  stimator of $\tau(\theta)$.


For instance, if we want to estimate a percentile, say the 95th percentile of $N(\mu,9)$. Note that the percentiles that we know are about standardized noraml (i.e., $N(0,1)$). Thus, we need to have some calculation to get the non-standard one.

$$\frac{X_{95 \; percentile}-\mu}{\sigma}=1.645$$
Thus, we can get 

$$X_{95 \; percentile}=1.645 \times \sigma +\mu$$

We know that $\bar{X}$ is the unbiased estimate for $\mu$. Thus, we can get

$$X_{95 \; percentile}=1.645 \times \sigma +\mu=4.94+\mu$$

We know that 

$$E(T)=E(\bar{X}+4.94)=\mu+4.94$$
Thus, $T=\bar{X}+4.94$ is the unbiased estimator of $\tau(\mu)=\mu+4.94$.


### Unbiased estimators vs. Invariance property of MLEs

__Do not apply "Invariance property of MLEs" to the Unbiased estimators.__

( __Note that:__ You can apply the Invariance property to "Unbiased estimators" when it is a linear combination. In that case, $E(a\theta+b)=aE(\theta)+b$. Thus, if you find a $T$ that is a unbiased estimator for $\theta$, it should be unbiased estimator for $a\theta+b$ as well. Thus, note that $\frac{1}{\theta}$ is not a linear combination of $\theta$, thus $\frac{1}{\theta}$ has a very different estimator, compared to $\theta$.)

_P.303_

For example, consider a random sample of size $n$ from an exponential distribution, $X_i \sim EXP(\theta)$, with parameter $\theta$. We know that, $\bar{X}$ is unbiased for $\theta$ (which is the mean of an exponential distribution).

If we want to estimate $\tau(\theta)=\frac{1}{\theta}$, then by the invariance property of MLE is $T_1=\frac{1}{\bar{X}}$.

However, $T_1$ is a biased estimators of $\frac{1}{\theta}$. Specifically,

We know that if $X \sim Gam(\theta,k)$, then $Y=\frac{2X}{\theta} \sim x^2(2k)$. We know that exponential distributions are a specicial case of Gamma distribution, $EXP(\theta)=Gam(\theta,1)$, thus we get the following,


$$Y=\frac{2n\bar{X}}{\theta}=\frac{2n}{\theta} \frac{\sum_{i=1}^n X_i}{n}=\frac{\sum_{i=1}^n 2X_i}{\theta}\sim\sum_{i=1}^n x^2(2\cdot1)=\sum_{i=1}^n x^2(2)=x^2(2n)$$

We further know that if $Y \sim x^2(v)$, $E(Y^r)=2^r\frac{\Gamma(v/2+r)}{\Gamma(v/2)}$. Thus, we know that,

$$E(Y^{-1})=2^{-1} \frac{\Gamma(2n/2-1)}{\Gamma(2n/2)}=\frac{1}{2}\cdot \frac{1}{n-1}$$

Thus,

$$E(Y^{-1})=E(\frac{\theta}{2n \bar{X}})=\frac{\theta}{2n}E(\frac{1}{\bar{X}})=\frac{1}{2}\cdot \frac{1}{n-1}$$

Thus.

$$E(\frac{1}{\bar{X}})=\frac{1}{n-1}\frac{n}{\theta}=\frac{n}{n-1}\frac{1}{\theta}$$


Thus,

$$E(\frac{n-1}{n} \frac{1}{\bar{X}})=\frac{1}{\theta}$$

__Conclusion:__

$\frac{1}{\bar{X}}$ is not the unbiased estimator for $\frac{1}{\theta}$. However, we can adjust it to $\frac{n-1}{n} \frac{1}{\bar{X}}$, which is the unbiased estimator for $\frac{1}{\theta}$. When the sample size is big enough, we know that $\frac{n-1}{n}$ will be close to 1. 


### UMVUE and Cramer-Rao lower bound

#### UMVUE

Let $X_1, X_2,...,X_n$ be a random sample of size $n$ from $f(x; \theta)$. An estimator $T^*$ of $\tau (\theta)$ is called a _uniformly minimum variance ubiased estimator_ (UMVUE) of $\tau(\theta)$ if 

1. $T^*$ is unbiased for $\tau{\theta}$

2. For any other unbiased estimator $T$ of $\tau(\theta)$, $Var(T^*) \leq Var(T)$ for all $\theta \in \Omega$.

#### Cramer-Rao lower bound 

If $T$ is an unbiased estimator of $\tau(\theta)$, then the Cramer-Rao lower bound (CRLB), based on a random sample, is

$$Var(T)=\frac{[\tau^{'}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}$$

__Example:__

Consider a random sample from an exponential distribution, $X_i \sim Exp(\theta)$. Because

$$f(x; \theta)=\frac{1}{\theta} e^{-\frac{x}{\theta}}$$

Thus,

$$ln(f(x;\theta))=-\frac{x}{\theta}-ln \theta$$

$$\frac{\partial}{\partial \theta} ln(f(X; \theta))=\frac{X}{\theta^2}-\frac{1}{\theta}=\frac{X-\theta}{\theta^2}$$

Thus,

$$E[\frac{\partial}{\partial \theta} ln(f(X; \theta))]^2 =E[\frac{(X-\theta)^2}{\theta^4}]=\frac{1}{\theta^4}E(X-\theta)^2=\frac{1}{\theta^4} Var(X)=\frac{\theta^2}{\theta^4}=\frac{1}{\theta^2}$$

Thus, the CRLB for $\tau(\theta)=\theta$ is as follows.

$$Var(T) \geq \frac{[\tau^{'}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}=\frac{[\frac{\partial }{\partial \theta}\theta]^2}{n \frac{1}{\theta^2}}=\frac{1^2}{\frac{n}{\theta^2}}=\frac{\theta^2}{n}$$

We know that the variance for exponential distribution

$$Var(\bar{X})=\frac{Var(X)}{n}=\frac{\theta^2}{n}$$

We know that $\theta$ is the mean for exponential distribution. We also know that sample mean $\bar{X}$ is the unbiased estimator for population mean.

Thus,

$\bar{X}$ is the UMVUE of $\theta$.



### Best linear unbiased estimation (BLUE or MVLUE)


### Consistency, asymptotic unbiasedness

__Simple consistency:__


_P.311_

Let $\{T_n\}$ be a sequence of estimators of $\tau(\theta)$. These estimators are said to be __consistent__ estimators of $\tau(\theta)$ if for every $\varepsilon >0$,

$$lim_{n\rightarrow \infty}P[|T_n-\tau(\theta)|<\varepsilon]=1$$

for every $\theta \in \Omega$.


In the terminology of Chapter 7, $T_n$ converges stochastically to $\tau(\theta)$,$T_n \xrightarrow{P} \tau(\theta)$ as $n \rightarrow \infty$. Sometimes this also is referred to as __simple__ consistency.

One interpretation of consistency is that for large sample size the estimator tends to be more concentrated about $\tau(\theta)$, and by making $n$ sufficiently large $T_n$ can be made as concentrated as desired.

__MSE consistency:__

If $\{ T_n \}$ is a sequence of estimator of $\tau(\theta)$, then they are called __mean squared error consistent__ if 

$$lim_{n\rightarrow \infty} E[T_n - \tau(\theta)]^2=0$$

for every $\theta \in \Omega$.

__Asymptotic Unbiased__

A sequence $\{ T_n \}$ is said to be __asymptotically unbiased__ for $\tau(\theta)$ if

$$lim_{n\rightarrow \infty} E(T_n)=\tau(\theta)$$

for every $\theta \in \Omega$.

__Example:__


_P.313_

For a sample from $X_i \sim EXP(\theta)$, we know that $T_n=1/ \bar{X}$ is an MLE estimator for $\tau(\theta)=1/\theta$. Howwever, $T_n$ is not a unbiased estimator for $\tau(\theta)$, as 

$$E(T_n)=\frac{n}{n-1}\cdot \frac{1}{\theta}$$

We also know that 

$$Var(T_n)=\frac{(\frac{n}{n-1})^2}{(n-2)\theta^2}$$

Thus, while $T_n$ is not unbiased, it is asymptotically unbiased and MSE consistent for $\tau(\theta)=\frac{1}{\theta}$.

__Note that__, MES consistency is a stronger property than simple consistency. Thus, if a sequenc $\{T_n\}$ is mean squared error consistent, it is also simply consistent.




### Efficiency

_P.308_

The relative efficiency of an unbiased estimator $T$ of $\tau(\theta)$ to another unbiased estimator $T^*$ of $\tau(\theta)$ is given by 

$$re(T, T^*)=\frac{Var(T^*)}{Var(T)}$$

An unbiased estimator $T^*$ of $\tau(\theta)$ is said to be efficient if $re(T, T^*) \leq 1$ for all unbiased estimators $T$ of $\tau(\theta)$, and all $\theta \in \Omega$. The efficiency of an unbiased estimator $T$ of $\tau(\theta)$ is given by 

$$e(T)=re(T, T^*)$$

if $T^*$ is an efficient estimator of $\tau(\theta)$.


Thus, __an effecient estimator is just a UMVUE.__


__Example:__

_P.233 Example 7.2.2_

_P.303 Example 9.3.2_

_P.309 Example 9.3.7_

Let $X_1,X_2,...,X_n$ sample from $X_i \sim EXP(\theta)$. We know that 

$$X_{1:n}=EXP(\theta/n)$$

Thus,

$$E(nX_{1:n})=nE(X_{1:n})=n\frac{\theta}{n}=\theta$$
Thus,

$$nX_{1:n}$$
is the unbiased estimator for $\theta$.


Thus,

$$re(T,T^*)=\frac{Var(T^*)}{Var(T)}=\frac{Var(\bar{X})}{Var(nX_{1:n})}=\frac{\theta^2/n}{n^2Var(X_{1:n})}=\frac{\theta^2/n}{n^2(\theta/n)^2}=\frac{\theta^2/n}{\theta^2}=\frac{1}{n}$$
Thus, $T^*=\bar{X}$ is a more efficient estimator for $\theta$ than $T=nX_{1:n}$.


### Asymptotic efficiency

Let $\{T_n\}$ and $\{T_n^*\}$ be the two asymptotically unbiased sequences of estimators for $\tau(\theta)$. The __asymptotic relative efficiency__ of $T_n$relative to $T_n^*$ is given by

$$are(T_n,T_n^*)=lim_{n \rightarrow \infty} \frac{Var(T_n^*)}{T_n}$$
The sequence $\{T_n^*\}$ is said to asymptotically efficient if $are\{T_n, T_n^*\} \leq 1$ for all other asymptotically unbiased sequences $\{T_n\}$, and all $\theta \in \Omega$.


### Asymptotic properties of MLEs

_P.316_

If certain regularity conditions are satisfied, then the solutions, $\hat{\theta}$, of the MLE have the following properties:

(1) $\hat{\theta_n}$ exists and is unique.

(2) $\hat{\theta_n}$ is a consistent estimator of $\theta$.

(3) $\hat{\theta_n}$ is asymptotically normal with asymptotic mean $\theta$ and variance 

$$\frac{1}{2}E[\frac{\partial}{\partial\theta}ln \; f(X;\theta)]^2$$

(4) $\hat{\theta}$ is asymptotically efficient.

Note that the asymptotic efficiency of $\hat{\theta}$ follows from the fact that the asymptotic variance is the same as the __CRLB__ for unbiased estimators of $\theta$. Thus, for large $n$, approximately

$$\hat{\theta_n} \sim N(\theta, CRLB)$$

__Example__

_P.317_

From Example 9.2.7, we know the MLE of the mean $\theta$ of an exponential distribution is the sample mean, $\hat{\theta_n}=\bar{X}$. 

We can infer the same asymptotic properties either from the discussion above or from the Central Limit Theorem. That is, $\hat{\theta_n}$ is asymptotically normal with asymptotic mean $\theta$ and variance $\theta^2/n$. From example 9.3.4, we know that $CRLB=\theta^2/n$.

Thus, 

$$\sqrt{n}\frac{\hat{\theta_n}-\theta}{\theta}\sim N(0,1)$$

## Sufficient and completeness

### Sufficiency and minimal sufficiency

_P.335_

__Sufficient statistic:__

A statistic $S$ will be consiered a "sufficient" statistic for a parameter $\theta$ if the conditional distribution of any other statistic $T$ given the value of $S$ does not involve $\theta$.



__Jointly sufficient statistics:__

Let $X=(X_1,...,X_n)$ have joint pdf $f(x,\theta)$, and let $S=(S_1,...,S_k)$ be a k-dimensional statistic. 

Then, $S_1,...,S_k$ is a set of __jointly sufficient statistics__ for $\theta$ if for any other vector of statistics, $T$, the conditional pdf of $T$ given $S=s$, denoted by $f_{T|s}(t)$, does not depend on $\theta$. 

In the one-dimension case, we simply say that $S$ is a sufficient statistic for $\theta$.

If $k$ unknown parameters are present in the model, then quite often there will exist a set of $k$ sufficient statistics. In some cases, the number of sufficient statistics will exceed the number of parameters.



__Minimal sufficient:__

A set of statistics is called a __minimal sufficient__ set if the members of the set are jointly sufficient for the parameters and if they are a function of every other set of jointly sufficient statistics. 


__In other words, once the value of a sufficient statistic is known, the observed value of any other statistic does not contain any further inforamtion about the parameter.__

__Example:__

_P.339_

Assume a random sample from an exponential distribution, $X_i \sim EXP(\theta)$. It follows that

$$f(x_1,...,x_n; \theta)=\frac{1}{\theta^n} e^{-\frac{\sum X_i}{\theta}}$$
which suggests checking the stastic $S=\sum X_i$. We know that $S \sim GAM(\theta,n)$, thus

$$f_S(s; \theta)=\frac{1}{\theta^n \Gamma(n)}s^{n-1}e^{-\frac{s}{n}}$$

If $s=\sum x_i$, then,

$$\frac{f(x_1,...,x_n; \theta)}{f_S(s;\theta)}=\frac{\Gamma(n)}{s^{n-1}}$$

which is free of $\theta$, and thus by definition S is sufficient for $\theta$.


### Neyman factorization theorem, minimal sufficiency of MLEs

__Factorization criterion:__

If $X_1,...,X_n$ have joint pdf $f(x_1,...,x_n; \theta)$, and if $S=(S_1,...,S_n)$, then $S_1,...,S_k$ are joinly sufficient for $\theta$ if and only if 

$$f(x_1,...x_n; \theta)=g(s;\theta)h(x_1,...,x_n)$$
where $g(s;\theta)$ does not depend on $x_1,...,x_n$, except through $s$, and $h(x_1,...,x_n)$ does not involve $\theta$.

__Example 1:__

_P.340_

$X_i \sim Bin(1,\theta)$. We can use factorization criterion to check its sufficient statistic.

$$f(x_1,...,x_n;\theta)=\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}=\theta^s(1-\theta)^{n-s}=g(s;\theta)h(x_1,...,x_n)$$

where $s=\sum X_i$ and $h(x_1,...x_n)=1$. Thus, $s$ is sufficient for $\theta$.


__Example 2:__

__It is important to specify the regions of zero probability. The following shows that care must be exercised in this matter.__

_P.340_

$X_i \sim UNIF(0,\theta)$, where $\theta$ is unknown. We get the joint $pdf$ of $X_1,...,X_n$ is

$$f(x_1,...x_n; \theta)=\frac{1}{\theta^n}$$
where,

$$0<x_i<\theta$$

It is easier to specify this $pdf$ in terms of the minimum, $x_{1:n}$, and maximum, $x_{n:n}$, of $x_1,...,x_n$. In particular,

$$0< x_{1:n}  \; \; x_{n:n} <\theta$$

Thus, $x_{n:n}$ is sufficient for $\theta$.


__Example 3:__

_P.341_

Consider a random sample from a normal distribution, $X_i \sim N(\mu, \sigma^2)$.

We know the $pdf$ for normal is 

$$f=\frac{1}{\sqrt{2\pi \sigma^2 }} e^{-\frac{1}{2\sigma^2}(x-\mu)^2}$$

Thus, the joint $pdf$ is as follows.

$$f(x_1,...,x_n;\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}\sum (x_i-\mu)^2}=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}\sum (x_i^2+\mu^2-2x_i\mu)}=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}(\sum x_i^2+n\mu^2-2\mu\sum x_i)}$$

Thus, $S_1=\sum X_i$ and $S_2=\sum X_i^2$ are jointly sufficient for $\theta={\mu, \sigma^2}$.


__Note that,__

_P.341_

based on _P.298_ , Example 9.2.10, ML estimation results in

$$\hat{\mu}=\bar{x}$$
$$\hat{\sigma^2}=\frac{\sum(x_i-\bar{x})^2}{n}$$


Further, note that

$$\hat{\mu}=\bar{x}=\frac{S_1}{n}$$

$$\begin{aligned} \hat{\sigma^2}&=\frac{\sum(x_i-\bar{x})^2}{n}=\frac{\sum (x_i^2+\bar{x}^2-2x_i\bar{x})}{n} \\ &=\frac{\sum x_i^2+n\bar{x}^2-2\bar{x} \sum x_i}{n} \\ &=\frac{\sum x_i^2+n\bar{x}^2-2n\bar{x}^2}{n} \\&=\frac{\sum x_i^2-n\bar{x}^2}{n} \\&=\frac{\sum x_i^2}{n}-\bar{x}^2 \\ &=\frac{\sum x_i^2}{n}- (\frac{\sum x_i}{n})^2 \\ &=\frac{S_2}{n}-(\frac{S_1}{n})^2 \end{aligned}$$

Thus, we can see that $\hat{\mu}$ and $\hat{\sigma^2}$ correspond to a one-to-one stransformation of $S_1$ and $S_2$, thus $\hat{\mu}$ and $\hat{\sigma^2}$ also are jontly sufficient for $\mu$ and $\sigma^2$.


### Rao-Blackwell theorem

Let $X_1,...,X_n$ have joint $pdf \; f(x_1,...,x_n; \theta)$, and let $S=(S_1,...,S_k)$ be a vector of jointly sufficient statistics for $\theta$. If $T$ is any unbiased estimator of $\tau(\theta)$, and if $T^*=E(T|S)$, then

(1) $T^*$ is an unbiased estimator of $\tau(\theta)$,

(2) $T^*$ is a function of $S$, and

(3) $Var(T^*) \leq Var(T)$ for every $\theta$, and  $Var(T^*) < Var(T)$ for some $\theta$ unless $T^*=T$ with probability 1. 


__Discussion:__

_P.345_

"It is clear from the Rao-Blackwell theorem that if we are searching for an unbiased estimator with small variance, we may as well restrict attention to function of sufficient statistics. If any ubiased estimator exists, then there will be one that is a function of sufficient statistics, namely $E(T|S)$, which also is unbiased and has variance at least as small or smaller. In particular, we still are interested in knowing how to find a UMVUE for a parameter, and the above theorem narrows our problem down somewhat. 

For instance, consider a one-parameter model $f(x;\theta)$, and assume that a single sufficient statistic, $S$, exists. We know that we must consider only unbiased functions of $S$ in searching for a UMVUE. in some cases it may be possible to show that only one function of $S$ is unbiased, and in that case we would know that it is a UMVUE. 

The concept of "completeness" is helpful in determinning unique unbiased estimators, and this concept is defined in the next section."



### completeness

__Completeness:__

_P.345_

A family of density functions $\{ f_T(t,\theta); \theta \in \Omega\}$, is called __complete__ if $E[u(T)]=0$ for all $\theta \in \Omega$ implies $u(T)=0$ with probability 1 for all $\theta \in \Omega$.


_P.345_

"This sometimes is expressed by saying that there are no nontrivial unbiased estimators of zero. In particular, it means that two different functions of $T$ cannot have the same expected value."

For example, if $E[u_1(T)]=\tau(\theta)$ and $E[u_2(T)]=\tau(\theta)$, which implies $u_1(T)-u_2(T)=0$, or $u_1(T)=u_2(T)$ with probablity 1, if the family of density fucntions is complete. That is, any unbiased estimator is unique in this case. 

We mainly are interested in knowing that the family of density functions of a sufficient statistic is complete, because in that case an unbiased function of the sufficient statistic will be unique, and it must be a UMVUE by the Rao-Blackwell theorem.


### Lehmann-Scheffe completeness theorem

Let $X_1,...,X_n$ have joint $pdf \; f(x_1,...,x_n; \theta)$ and let $S$ be a vector of jointly complete sufficient statistics for $\theta$. If $T^*=t^*(S)$ is a statistic that is unbiased for $\tau(\theta)$ and a function of $S$, then $T^*$ is a UMVUE of $tau(\theta)$.

__Example:__

_P.346_


### Exponential class, complete sufficient statistics


A density fucntion is said to be a member of the __regular exponential class__ if it can be expressed in the form

$$f(x; \theta)=c(\theta)h(x)e^{\sum_{j=1}^k q_j(\theta)t_j(x)}$$
where,

$$x \in A$$
And zero otherwise, where $\theta=(\theta_1,...,\theta_k)$ is a vector of $k$ unknown parameters, if the parameter space has the form

$$\Omega=\{\theta | a_i \leq \theta_i \leq b_i, i=1,...k\}$$

(note that $a_i=-\infty$ and $b_i=\infty$ are permissible values), and if it satisfies regularity conditions 1, 2, and 3a or 3b given by:

(1) The set $A=\{x:f(x; \theta)>0\}$ does not depend on $\theta$.

(2) The functions $q_j(\theta)$ are nontrivial, functionally independent, continuous fucntion of the $\theta_i$.

(3a) For a continuous random variable, the derivatives $t_k^{'}(x)$ are linearly independent continous functions of $x$ over $A$.

(3b) For a discrete random variable, the $t_k(x)$ are nontrivial functions of $x$ on $A$, and none is a linear function of the others. 

__Example__

_P.348_

Consider Bernoulli distribution $X \sim Bin(1,p)$. It follows that 

$$\begin{aligned} f(x; p) &=p^x(1-p)^{1-x} \\ &=P^x(1-p)(1-p)^{-x} \\ &=(1-p)(1-p)^{-x}P^x \\&=(1-p)e^{ln(1-p)^{-x}P^x} \\ &=(1-p)e^{xln(\frac{p}{1-p})} \end{aligned}$$


Compared to the definition of __Exponential Class__ defined above, we can get the following. 
$$c(\theta)=(1-p)$$
$$h(x)=1$$
$$q_1(\theta)=ln(\frac{p}{1-p})$$
$$t_1(x)=x$$

__Theorem 10.4.2__

_P.348_

If $X_1,...,X_n$ is a random sample from a member of the regular exponential class $REC(q_1,...,q_k)$, then the statistics 

$$S_1=\sum_{i=1}^n t_1(X_i),...,S_k=\sum_{i=1}^n t_k(X_i)$$

are a minimal set of complete sufficient statistics for $\theta_1,...\theta_k$.


__Example 1:__

_P.348_

Again, consider Bernoulli distribution $X \sim Bin(1,p)$. For a random sample of size $n$, $t(x_i)=x_i$ and thus based on __Theorem 10.4.2 (see above)__ $S=\sum_{i=1}^n X_i$ is a complete sufficient statistic for $p$.

__How about we want to find UMVUE for $Var(X)=p(1-p)$?__

We might try $\bar{X}(1-\bar{X})$.

$$\begin{aligned} E[\bar{X}(1-\bar{X})] &=E(\bar{X})-E(\bar{X}^2) \\ &=p-(p^2+var(\bar{X})) \\ &=p-p^2-\frac{p(1-p)}{n} \\ &=p(1-p)(1-\frac{1}{n}) \end{aligned}$$


Thus, 

$$E[\frac{1}{1-\frac{1}{n}}\bar{X}(1-\bar{X})]=p(1-p)$$


Thus, $\frac{1}{1-\frac{1}{n}}\bar{X}(1-\bar{X})$ is the UMVUE of the $p(1-p)$.

but __why?__


__Example 2:__

_P.349_

If $X \sim N(\mu,\sigma^2)$, then

$$\begin{aligned} f(x;\mu,\sigma)&=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\ &=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{1}{2\sigma^2}(x^2+\mu^2-2x\mu)} \\ &=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{x^2}{2\sigma^2}-\frac{\mu^2}{2\sigma^2}+\frac{x\mu}{\sigma^2}} \end{aligned}$$


Thus, $S_1=\sum X_i^2$ and $S_2=\sum X_i$ are jointly complete and sufficient statistics of $\mu$ and $\sigma^2$. __Refer to the normal example in the section of "Neyman factorization theorem, minimal sufficiency of MLEs"__

__Theorem 10.4.3__

_P.349_

If a CRLB estimator $T$ exists for $\tau(\theta)$, then a single sufficient statistic exists, and $T$ is a function of the sufficient statistic. Conversely, if a single sufficient statistic exists and teh CRLB exists, then a CRLB estimator exists for some $\tau(\theta)$.





