# 556

## Statistics and Sampling Distributions

### Statistics 

#### Definition of Statistic

_P.264_

A fucntion of observable random variables, $T=t(X_1, ... X_n)$, which does not depend on any unknown parameters is called statistic.

For example, Let $X_1, ..., X_n$ represent a random sample from a population with $pdf \;f(x)$. The sample mean provides an example of a statistic with the function 

$$t(x_1,...,x_n)=(x_1+...+x_n)/n$$

This statistic usually is denoted by 

$$\bar{X}=\sum_{i=1}^n \frac{X_i}{n}$$

When a random sample is observed, the value of $\bar{X}$, computed from the data, usually is denoted by lower case $\bar{x}$.

$$\bar{x}=\sum_{i=1}^n \frac{x_i}{n}$$


#### Sample and parameters

_P.265_

If $X_1,..., X_n$ denotes a random sample from $f(x)$ with $E(X)=\mu$ and $var(X)=\sigma^2$, then 

$$E(\bar{X})=\mu$$

$$Var(\bar{X})=\frac{\sigma^2}{n}$$

For example, a random sample of size $n$ from a Bernoulli distribution $X_i \sim BIN(1,p)$. We know Bernoulli has $\mu=p$ and $\sigma^2 =pq$. In this case, the sample mean is 

$$\bar{X}=Y/n=\hat{p}$$

Thus, 

$$E(\hat{p})=p$$

$$Var (\hat{p})=\frac{pq}{n}$$

Thus, sample mean is the unbiased estimate for the population mean. However, you can not use sample mean's variance to estimate pupulation variance. That lead to definition of sample variance. 

_P.266_

Sample variance:

$$S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$$

$$E(S^2)=\sigma^2$$

### $\chi^2, t, F, beta$

#### $\chi^2$

Always squre from standard normal, and the standardization can be using $\mu$ or $\bar{X}$.


$$\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2(n-1)$$
(Thus, we can see this is a bit weired, as the numerator is $\bar{X}$ is from the sample, whereas $\sigma^2$ is from the population.)

Thus, we can 

$$\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sigma^2}\sim \chi^2(n-1)$$
(You can compare $\bar{X}$ with $\mu$, we can see the only difference is that the $\chi^2$ has one less degree of freedom because we use this degree of freedom to calculate the mean.)


We need to know the mean and variance of $\chi^2$.

Assume that

$$X \sim x^2(v)$$

$$mean:v$$
$$variance: 2v$$


#### $t$

Definition

$$t(k)=\frac{N(0,1)}{\sqrt{\frac{\chi^2(k)}{k}}}$$


Property 1: t distribution is symmetrical


Given that t-distribution is symmetrical, we can get

$$H(-c)=1-H(c)$$


Property 2: t distribution has heavier tails than the normal

My note: t-distribution only has a parameter of $k$, which is determined by the $\chi^2$'s degree of freedom. Of course, $\chi^2$ also only has one parameter, namely the degree of freedom.


#### $F$

If $V_1 \sim \chi^2(v_1)$ and $V_2 \sim \chi^2(v_2)$ are independent, then the random variable

$$\frac{V_1/v_1}{V_2/v_2}\sim F(v_1,v_2)$$


#### Beta

If $X \sim F(v_1, v_2)$

$$Y=\frac{(v_1/v_2)X}{1+(v_1/v_2)X} \sim Beta(\alpha, \beta)$$


### Large-sample approximations


_P.280_

If $Y_v \sim x^2(x)$, then


$$Z_v =\frac{Y_v-v}{\sqrt{2v}} \xrightarrow{d} Z \sim N(0,1)$$

(The proof is based on CLT.)


## Point Estimation

### Method of moments estimators

#### Definition

The joint MGF of ($X_1, ..., X_n$) is defined as $M(t_1,...,t_n)=E(e^{\sum_{i=1}^nt_iX_i})$

When $X_1, ..., X_n$ are independent if and only if 

$$M(t_1,t_n)=\prod_{i=1}^n M_{X_i}(t_i)$$
where $M_{X_i}(t_i)$ is the MGF of $X_i$

#### Well-known MGF

(1) Bernoulli with success probability p: $1-p+pe^t$

(2) Binomial Bin(n,p): $(1-p+pe^t)^n$

(3) Poisson $POI(\lambda)$: $e^{\lambda(e^t-1)}$

(4) Normal $N(\mu,\sigma^2)$: $e^{\mu t+\frac{1}{2}\sigma^2t^2}$

(5) Gamma $GAM(\theta,k)$: $(1-\theta t)^{-k}$


Two special cases:

(6) Chi-square $\chi^2(v) =GAM(2,\frac{v}{2})$: $(1-2t)^{-\frac{v}{2}}$

(7) Exponential $EXP(\theta)=GAM(\theta,1)$: $(1-\theta t)^{-1}$


### least squares estimators


### likelihood function and maximum likelihood estimators

#### Likelihood function

_P.293_

The joint density function of $n$ random variables $X_1,...X_n$ evaluated at $x_1, ...x_n$, say $f(x_1,...,x_n; \theta)$, is referered to as a _likelihood function_.

#### Maximum likelihood estimators

_P.294_

Let $L(\theta)=f(x_1,...x_n; \theta), \theta \in \Omega$, be the joint pdf of $X_1, ..., X_n$. For a given set of observations, $(x_1,...x_n)$, a value $\hat{\theta}$ in $\Omega$ at which $L(\theta)$ is a maximum is called a _maximum likelihood estimate (MLE)_ of $\theta$. That is $\hat{\theta}$ is a value of $\theta$ that satisfies

$$f(x_1,...x_n; \theta)=max_{\theta \in \Omega} f(x_1, ..., x_n; \theta)$$

### Invariance property of MLEs

_p296_

If $\hat{\theta}$ is the MLE of $\theta$ and if $u(\theta)$ is a function of $\theta$, then $u(\hat{\theta})$ is an MLE of $u(\theta)$.

For instance,

### Unbiased estimators

An estimator T is said to be an unbiased estimator of $\tau(\theta)$ if 

$$E(T)=\tau(\theta)$$

for all $\theta \in \Omega$. Otherwise, we said that $T$ is biased  stimator of $\tau(\theta)$.


For instance, if we want to estimate a percentile, say the 95th percentile of $N(\mu,9)$. Note that the percentiles that we know are about standardized noraml (i.e., $N(0,1)$). Thus, we need to have some calculation to get the non-standard one.

$$\frac{X_{95 \; percentile}-\mu}{\sigma}=1.645$$
Thus, we can get 

$$X_{95 \; percentile}=1.645 \times \sigma +\mu$$

We know that $\bar{X}$ is the unbiased estimate for $\mu$. Thus, we can get

$$X_{95 \; percentile}=1.645 \times \sigma +\mu=4.94+\mu$$

We know that 

$$E(T)=E(\bar{X}+4.94)=\mu+4.94$$
Thus, $T=\bar{X}+4.94$ is the unbiased estimator of $\tau(\mu)=\mu+4.94$.

### Cramer-Rao lower bound

### Best linear unbiased estimation (BLUE or MVLUE)

### Rao-Blackwell theorem,  UMVUEs


### Consistency, asymptotic unbiasedness


### Efficiency, asymptotic efficiency


### Asymptotic properties of MLEs



## Sufficient and completeness

### Sufficiency and minimal sufficiency

### Neyman factorization theorem, minimal sufficiency of MLEs

### completeness, Lehmann-Scheffe completeness theorem

### Exponential class, complete sufficient statistics




