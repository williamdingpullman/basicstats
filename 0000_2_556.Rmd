# 556

## Statistics and Sampling Distributions

### Statistics 

#### Definition of Statistic

_P.264_

A fucntion of observable random variables, $T=t(X_1, ... X_n)$, which does not depend on any unknown parameters is called statistic.

For example, let $X_1, ..., X_n$ represent a random sample from a population with $pdf \;f(x)$. The sample mean provides an example of a statistic with the function 

$$t(x_1,...,x_n)=(x_1+...+x_n)/n$$

This statistic usually is denoted by 

$$\bar{X}=\sum_{i=1}^n \frac{X_i}{n}$$

When a random sample is observed, the value of $\bar{X}$, computed from the data, usually is denoted by lower case $\bar{x}$.

$$\bar{x}=\sum_{i=1}^n \frac{x_i}{n}$$


#### Sample and parameters

_P.265_

If $X_1,..., X_n$ denotes a random sample from $f(x)$ with $E(X)=\mu$ and $var(X)=\sigma^2$, then 

$$E(\bar{X})=\mu$$

$$Var(\bar{X})=\frac{\sigma^2}{n}$$

For example, a random sample of size $n$ from a Bernoulli distribution $X_i \sim BIN(1,p)$. We know Bernoulli has $\mu=p$ and $\sigma^2 =pq$. In this case, the sample mean is 

$$\bar{X}=Y/n=\hat{p}$$

Thus, 

$$E(\hat{p})=p$$

$$Var (\hat{p})=\frac{pq}{n}$$

Thus, sample mean is the unbiased estimate for the population mean. However, you can not use sample mean's variance to estimate pupulation variance. That lead to definition of sample variance. 

_P.266_

Sample variance:

$$S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$$

$$E(S^2)=\sigma^2$$

### $\chi^2, t, F, beta$

#### $\chi^2$

Always squre from standard normal, and the standardization can be using $\mu$ or $\bar{X}$.


$$\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2(n-1)$$
(Thus, we can see this is a bit weired, as the numerator is $\bar{X}$ is from the sample, whereas $\sigma^2$ is from the population.)

Thus, we can 

$$\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sigma^2}\sim \chi^2(n-1)$$
(You can compare $\bar{X}$ with $\mu$, we can see the only difference is that the $\chi^2$ has one less degree of freedom because we use this degree of freedom to calculate the mean.)


For the mean and variance of $\chi^2$:

Assume that

$$X \sim x^2(v)$$

$$mean:v$$
$$variance: 2v$$


#### $t$

Definition

$$t(k)=\frac{N(0,1)}{\sqrt{\frac{\chi^2(k)}{k}}}$$


__Property 1__

__t__ distribution is symmetrical


Given that _t_ distribution is symmetrical, we can get

$$H(-c)=1-H(c)$$


__Property 2__

_t_ distribution has heavier tails than the normal.


My note: _t_ distribution only has a parameter of $k$, which is determined by the $\chi^2$'s degree of freedom. Of course, $\chi^2$ also only has one parameter, namely the degree of freedom.


#### $F$

If $V_1 \sim \chi^2(v_1)$ and $V_2 \sim \chi^2(v_2)$ are independent, then the random variable

$$\frac{V_1/v_1}{V_2/v_2}\sim F(v_1,v_2)$$


#### Beta

If $X \sim F(v_1, v_2)$

$$Y=\frac{(v_1/v_2)X}{1+(v_1/v_2)X} \sim Beta(\alpha, \beta)$$


### Large-sample approximations


_P.280_

If $Y_v \sim x^2(x)$, then


$$Z_v =\frac{Y_v-v}{\sqrt{2v}} \xrightarrow{d} Z \sim N(0,1)$$

(The proof is based on CLT.)


## Point Estimation

### Method of moments estimators

#### Definition

The joint MGF of ($X_1, ..., X_n$) is defined as $M(t_1,...,t_n)=E(e^{\sum_{i=1}^nt_iX_i})$

When $X_1, ..., X_n$ are independent if and only if 

$$M(t_1,...,t_n)=\prod_{i=1}^n M_{X_i}(t_i)$$
where $M_{X_i}(t_i)$ is the MGF of $X_i$

#### Well-known MGF

(1) Bernoulli with success probability p: $1-p+pe^t$

(2) Binomial Bin(n,p): $(1-p+pe^t)^n$

(3) Poisson $POI(\lambda)$: $e^{\lambda(e^t-1)}$

(4) Normal $N(\mu,\sigma^2)$: $e^{\mu t+\frac{1}{2}\sigma^2t^2}$

(5) Gamma $GAM(\theta,k)$: $(1-\theta t)^{-k}$


Two special cases:

(6) Chi-square $\chi^2(v) =GAM(2,\frac{v}{2})$: $(1-2t)^{-\frac{v}{2}}$

(7) Exponential $EXP(\theta)=GAM(\theta,1)$: $(1-\theta t)^{-1}$


### least squares estimators


### likelihood function and maximum likelihood estimators

#### Likelihood function

_P.293_

The joint density function of $n$ random variables $X_1,...X_n$ evaluated at $x_1, ...x_n$, say $f(x_1,...,x_n; \theta)$, is referered to as a _likelihood function_.

#### Maximum likelihood estimators

_P.294_

Let $L(\theta)=f(x_1,...x_n; \theta), \theta \in \Omega$, be the joint pdf of $X_1, ..., X_n$. For a given set of observations, $(x_1,...x_n)$, a value $\hat{\theta}$ in $\Omega$ at which $L(\theta)$ is a maximum is called a _maximum likelihood estimate (MLE)_ of $\theta$. That is $\hat{\theta}$ is a value of $\theta$ that satisfies

$$f(x_1,...x_n; \theta)=MAX_{\theta \in \Omega} f(x_1, ..., x_n; \theta)$$

### Invariance property of MLEs

_p296_

If $\hat{\theta}$ is the MLE of $\theta$ and if $u(\theta)$ is a function of $\theta$, then $u(\hat{\theta})$ is an MLE of $u(\theta)$.


__For instance__ ,

We know that the $pdf$ of exponential distribution ($X \sim EXP (\theta)$) is as follows:

$$\frac{1}{\theta} e^{-\frac{X}{\theta}}$$

Thus, its likelihood function is as follows

$$L(\theta)=\frac{1}{\theta^n}e^{-\frac{\sum X_i}{\theta}}$$
Thus, log-likelihood is as follows.

$$lnL(\theta)=-n ln(\theta)-\frac{\sum X_i}{\theta}$$
Thus,

$$\frac{d}{d\theta} lnL(\theta)=-n \frac{1}{\theta}+\frac{\sum X_i}{\theta^2}$$

Thus, we can get the $MLE$ for $\theta$ is $\hat{\theta}=\bar{x}$.

If we want to estimate $\tau(\theta)=P(X \geq 1)$:

$$\tau(\theta)=P(X\geq 1)=\int_1^{\infty} \frac{1}{\theta} e^{-\frac{X}{\theta}} dx=-\int_1^{\infty}  e^{-\frac{X}{\theta}} d(-\frac{x}{\theta})=-[e^{-\frac{X}{\theta}}]_1^{\infty}=-[0-e^{-\frac{1}{\theta}}]=e^{-\frac{1}{\theta}}$$

Thus, based on the invariance property, we know that the $MLE$ for $\tau(\theta)$ is as follows.

$$e^{-\frac{1}{\bar{x}}}$$

### Unbiased estimators

An estimator T is said to be an unbiased estimator of $\tau(\theta)$ if 

$$E(T)=\tau(\theta)$$

for all $\theta \in \Omega$. Otherwise, we said that $T$ is biased  stimator of $\tau(\theta)$.


For instance, if we want to estimate a percentile, say the 95th percentile of $N(\mu,9)$. Note that the percentiles that we know are about standardized noraml (i.e., $N(0,1)$). Thus, we need to have some calculation to get the non-standard one.

$$\frac{X_{95 \; percentile}-\mu}{\sigma}=1.645$$
Thus, we can get 

$$X_{95 \; percentile}=1.645 \times \sigma +\mu$$

We know that $\bar{X}$ is the unbiased estimate for $\mu$. Thus, we can get

$$X_{95 \; percentile}=1.645 \times \sigma +\mu=4.94+\mu$$

We know that 

$$E(T)=E(\bar{X}+4.94)=\mu+4.94$$
Thus, $T=\bar{X}+4.94$ is the unbiased estimator of $\tau(\mu)=\mu+4.94$.


### Unbiased estimators vs. Invariance property of MLEs

__Do not apply "Invariance property of MLEs" to the Unbiased estimators.

_P.303_

For example, consider a random sample of size $n$ from an exponential distribution, $X_i \sim EXP(\theta)$, with parameter $\theta$. We know that, $\bar{X}$ is unbiased for $\theta$ (which is the mean of an exponential distribution).

If we want to estimate $\tau(\theta)=\frac{1}{\theta}$, then by the invariance property of MLE is $T_1=\frac{1}{\bar{X}}$.

However, $T_1$ is a biased estimators of $\frac{1}{\theta}$. Specifically,

We know that if $X \sim Gam(\theta,k)$, then $Y=\frac{2X}{\theta} \sim x^2(2k)$. We know that exponential distributions are a specicial case of Gamma distribution, $EXP(\theta)=Gam(\theta,1)$, thus we get the following,


$$Y=\frac{2n\bar{X}}{\theta}=\frac{2n}{\theta} \frac{\sum_{i=1}^n X_i}{n}=\frac{\sum_{i=1}^n 2X_i}{\theta}\sim\sum_{i=1}^n x^2(2\cdot1)=\sum_{i=1}^n x^2(2)=x^2(2n)$$

We further know that if $Y \sim x^2(v)$, $E(Y^r)=2^r\frac{\Gamma(v/2+r)}{\Gamma(v/2)}$. Thus, we know that,

$$E(Y^{-1})=2^{-1} \frac{\Gamma(2n/2-1)}{\Gamma(2n/2)}=\frac{1}{2}\cdot \frac{1}{n-1}$$

Thus,

$$E(Y^{-1})=E(\frac{\theta}{2n \bar{X}})=\frac{\theta}{2n}E(\frac{1}{\bar{X}})=\frac{1}{2}\cdot \frac{1}{n-1}$$

Thus.

$$E(\frac{1}{\bar{X}})=\frac{1}{n-1}\frac{n}{\theta}=\frac{n}{n-1}\frac{1}{\theta}$$


Thus,

$$E(\frac{n-1}{n} \frac{1}{\bar{X}})=\frac{1}{\theta}$$

__Conclusion__

$\frac{1}{\bar{X}}$ is not the unbiased estimator for $\frac{1}{\theta}$. However, we can adjust it to $\frac{n-1}{n} \frac{1}{\bar{X}}$, which is the unbiased estimator for $\frac{1}{\theta}$. When the sample size is big enough, we know that $\frac{n-1}{n}$ will be close to 1. 


### UMVUE and Cramer-Rao lower bound

#### UMVUE

Let $X_1, X_2,...,X_n$ be a random sample of size $n$ from $f(x; \theta)$. An estimator $T^*$ of $\tau (\theta)$ is called a _uniformly minimum variance ubiased estimator_ (UMVUE) of $\tau(\theta)$ if 

1. $T^*$ is unbiased for $\tau{\theta}$

2. For any other unbiased estimator $T$ of $\tau(\theta)$, $Var(T^*) \leq Var(T)$ for all $\theta \in \Omega$.

#### Cramer-Rao lower bound 

If $T$ is an unbiased estimator of $\tau(\theta)$, then the Cramer-Rao lower bound (CRLB), based on a random sample, is

$$Var(T)=\frac{[\tau^{'}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}$$

For instance, consider a random sample from an exponential distribution, $X_i \sim Exp(\theta)$. Because

$$f(x; \theta)=\frac{1}{\theta} e^{-\frac{x}{\theta}}$$

Thus,

$$ln(f(x;\theta))=-\frac{x}{\theta}-ln \theta$$

$$\frac{\partial}{\partial \theta} ln(f(X; \theta))=\frac{X}{\theta^2}-\frac{1}{\theta}=\frac{X-\theta}{\theta^2}$$

Thus,

$$E[\frac{\partial}{\partial \theta} ln(f(X; \theta))]^2 =E[\frac{(X-\theta)^2}{\theta^4}]=\frac{1}{\theta^4}E(X-\theta)^2=\frac{1}{\theta^4} Var(X)=\frac{\theta^2}{\theta^4}=\frac{1}{\theta^2}$$

Thus, the CRLB for $\tau(\theta)=\theta$ is as follows.

$$Var(T) \geq \frac{[\tau^{'}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}=\frac{[\frac{\partial }{\partial \theta}\theta]^2}{n \frac{1}{\theta^2}}=\frac{1^2}{\frac{n}{\theta^2}}=\frac{\theta^2}{n}$$

We know that the variance for exponential distribution

$$Var(\bar{X})=\frac{Var(X)}{n}=\frac{\theta^2}{n}$$

We know that $\theta$ is the mean for exponential distribution. We also know that sample mean $\bar(X)$ is the unbiased estimator for population mean.

Thus,

$\bar{X}$ is the UMVUE of $\theta$.



### Best linear unbiased estimation (BLUE or MVLUE)

### Rao-Blackwell theorem


### Consistency, asymptotic unbiasedness


### Efficiency, asymptotic efficiency

_P.308_

The relative efficiency of an unbiased estimator $T$ of $\tau(\theta)$ to another unbiased estimator $T^*$ of $\tau(\theta)$ is given by 

$$re(T, T^*)=\frac{Var(T^*)}{Var(T)}$$

An unbiased estimator $T^*$ of $\tau(\theta)$ is said to be efficient if $re(T, T^*) \leq 1$ for all unbiased estimators $T$ of $\tau(\theta)$, and all $\theta \in \Omega$. The efficiency of an unbiased estimator $T$ of $\tau(\theta)$ is given by 

$$e(T)=re(T, T^*)$$

if $T^*$ is an efficient estimator of $\tau(\theta)$.


Thus, __an effecient estimator is just a UMVUE.__




### Asymptotic properties of MLEs



## Sufficient and completeness

### Sufficiency and minimal sufficiency

### Neyman factorization theorem, minimal sufficiency of MLEs

### completeness, Lehmann-Scheffe completeness theorem

### Exponential class, complete sufficient statistics




