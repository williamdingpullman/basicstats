# 556

## Statistics and Sampling Distributions

### Statistics 

#### Definition of Statistic

_P.264_

A fucntion of observable random variables, $T=t(X_1, ... X_n)$, which does not depend on any unknown parameters is called statistic.

For example, let $X_1, ..., X_n$ represent a random sample from a population with $pdf \;f(x)$. The sample mean provides an example of a statistic with the function 

$$t(x_1,...,x_n)=(x_1+...+x_n)/n$$

This statistic usually is denoted by 

$$\bar{X}=\sum_{i=1}^n \frac{X_i}{n}$$

When a random sample is observed, the value of $\bar{X}$, computed from the data, usually is denoted by lower case $\bar{x}$.

$$\bar{x}=\sum_{i=1}^n \frac{x_i}{n}$$


#### Sample and parameters

_P.265_

If $X_1,..., X_n$ denotes a random sample from $f(x)$ with $E(X)=\mu$ and $var(X)=\sigma^2$, then 

$$E(\bar{X})=\mu$$

$$Var(\bar{X})=\frac{\sigma^2}{n}$$

For example, a random sample of size $n$ from a Bernoulli distribution $X_i \sim BIN(1,p)$. We know Bernoulli has $\mu=p$ and $\sigma^2 =pq$. In this case, the sample mean is 

$$\bar{X}=Y/n=\hat{p}$$

Thus, 

$$E(\hat{p})=p$$

$$Var (\hat{p})=\frac{pq}{n}$$

Thus, sample mean is the unbiased estimate for the population mean. However, you can not use sample mean's variance to estimate pupulation variance. That lead to definition of sample variance. 

_P.266_

Sample variance:

$$S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$$

$$E(S^2)=\sigma^2$$

### $\chi^2, t, F, beta$

#### $\chi^2$

Always squre from standard normal, and the standardization can be using $\mu$ or $\bar{X}$.


$$\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2(n-1)$$
(Thus, we can see this is a bit weired, as the numerator is $\bar{X}$ is from the sample, whereas $\sigma^2$ is from the population.)

Thus, we can 

$$\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sigma^2}\sim \chi^2(n-1)$$
(You can compare $\bar{X}$ with $\mu$, we can see the only difference is that the $\chi^2$ has one less degree of freedom because we use this degree of freedom to calculate the mean.)


For the mean and variance of $\chi^2$:

Assume that

$$X \sim x^2(v)$$

$$mean:v$$
$$variance: 2v$$


#### $t$

Definition

$$t(k)=\frac{N(0,1)}{\sqrt{\frac{\chi^2(k)}{k}}}$$


__Property 1__

__t__ distribution is symmetrical


Given that _t_ distribution is symmetrical, we can get

$$H(-c)=1-H(c)$$


__Property 2__

_t_ distribution has heavier tails than the normal.


My note: _t_ distribution only has a parameter of $k$, which is determined by the $\chi^2$'s degree of freedom. Of course, $\chi^2$ also only has one parameter, namely the degree of freedom.


#### $F$

If $V_1 \sim \chi^2(v_1)$ and $V_2 \sim \chi^2(v_2)$ are independent, then the random variable

$$\frac{V_1/v_1}{V_2/v_2}\sim F(v_1,v_2)$$


#### Beta

If $X \sim F(v_1, v_2)$

$$Y=\frac{(v_1/v_2)X}{1+(v_1/v_2)X} \sim Beta(\alpha, \beta)$$


### Large-sample approximations


_P.280_

If $Y_v \sim x^2(x)$, then


$$Z_v =\frac{Y_v-v}{\sqrt{2v}} \xrightarrow{d} Z \sim N(0,1)$$

(The proof is based on CLT.)


## Point Estimation

### Method of moments estimators

#### Definition about moments (chapter 2)

_P.73_

THe __kth moment about the origin__ of a random variable $X$ is 

$$\mu^{'}_k=E(X^k)$$

and the __kth moment about the mean__ is 

$$\mu_k=E[X-E(X)]^k=E(X-\mu)^k$$

Thus,$k=E(X^k)$ may be considered as the $k$th moment of $X$ or the first moment of $X^k$. 

The first moment about the mean is zero,

$$\mu_1=E[X-E(X)]=E(X)-E(X)=0$$

The second moment about the mean is the variance,

$$\mu_2=E[(X-\mu)^2]=\sigma^2$$

Note that the definition of variance:

_P.73_

$$Var(X)=E[(X-\mu)^2]$$

#### Definition

Based on the last chapater (i.e., Chapter 8), sample mean $\bar{X}$ is an estimator of the population mean $\mu$. A more general approach, which produced estimators known as the __method of moments estimators(MMEs)__ , can be developed. 


If $X_1,...,X_n$ is a random sample from $f(x; \theta_1,...,\theta_k)$, the first $k$ sample moments are given by 


$$M_j^{'}=\frac{\sum_{i=1}^n X_i^j}{n}$$


where,

$$j=1,2,...k$$



__Example 1__


_P.291_

Consider a random sample from a distribution with two unknown parameters, the mean $\mu$ and the variance $\sigma^2$. We know from earlier considerations that $\mu=\mu^{'}_1$ and $\sigma^2=E(X^2)-\mu^2=\mu_2^{'}-(\mu^{'}_1)^2$.

Thus,

$$\hat{\sigma}^2=\mu_2^{'}-(\mu^{'}_1)^2=\frac{\sum_{i=1}^n X_i^2}{n}-\frac{\sum_{i=1}^n X_i}{n}=\frac{\sum_{i=1}^n X_i^2}{n}-\bar{X}=\sum_{i=1}^n \frac{(X_i-\bar{X})^2}{n}$$

(Thus, we can see that the estimation of $\sigma^2$ is not the same as the definition of sample variance $S^2$. $\hat{\sigma}^2=\frac{n-1}{n}S^2$.)


__Example 2__

_P.292_

If a sample is from a Gamma distribution $X_i \sim GAM(\theta,k)$, and we want to estimate the $\theta$ and $k$.

We know that for Gamma distribution, the mean is $k\theta$, and the variance is $k\theta^2$.

We also know that $\mu_1^{'}=\mu=k\theta$ and $\mu_2^{'}= \sigma^2+\mu^2= k\theta^2+k^2\theta^2=k\theta^2(1+k)$.

Thus, we can get

$$\bar{X}=k\theta$$

$$\sum \frac{X_i^2}{n}=k\theta^2(1+k)$$

Thus, we can get

$$\hat{\theta}=\sum_{i=1}^n \frac{(X_i-\bar{X})^2}{n \bar{X}}=\frac{(n-1)/n S^2}{\bar{X}}$$


$$\hat{k}=\frac{\bar{X}}{\hat{\theta}}$$

#### Property

The joint MGF of ($X_1, ..., X_n$) is defined as $M(t_1,...,t_n)=E(e^{\sum_{i=1}^nt_iX_i})$

When $X_1, ..., X_n$ are independent if and only if 

$$M(t_1,...,t_n)=\prod_{i=1}^n M_{X_i}(t_i)$$
where $M_{X_i}(t_i)$ is the MGF of $X_i$

#### Well-known MGF

(1) Bernoulli with success probability p: $1-p+pe^t$

(2) Binomial Bin(n,p): $(1-p+pe^t)^n$

(3) Poisson $POI(\lambda)$: $e^{\lambda(e^t-1)}$

(4) Normal $N(\mu,\sigma^2)$: $e^{\mu t+\frac{1}{2}\sigma^2t^2}$

(5) Gamma $GAM(\theta,k)$: $(1-\theta t)^{-k}$


Two special cases:

(6) Chi-square $\chi^2(v) =GAM(2,\frac{v}{2})$: $(1-2t)^{-\frac{v}{2}}$

(7) Exponential $EXP(\theta)=GAM(\theta,1)$: $(1-\theta t)^{-1}$


### least squares estimators


### likelihood function and maximum likelihood estimators

#### Likelihood function

_P.293_

The joint density function of $n$ random variables $X_1,...X_n$ evaluated at $x_1, ...x_n$, say $f(x_1,...,x_n; \theta)$, is referered to as a _likelihood function_.

#### Maximum likelihood estimators

_P.294_

Let $L(\theta)=f(x_1,...x_n; \theta), \theta \in \Omega$, be the joint pdf of $X_1, ..., X_n$. For a given set of observations, $(x_1,...x_n)$, a value $\hat{\theta}$ in $\Omega$ at which $L(\theta)$ is a maximum is called a _maximum likelihood estimate (MLE)_ of $\theta$. That is $\hat{\theta}$ is a value of $\theta$ that satisfies

$$f(x_1,...x_n; \theta)=MAX_{\theta \in \Omega} f(x_1, ..., x_n; \theta)$$

### Invariance property of MLEs

_P.296_

If $\hat{\theta}$ is the MLE of $\theta$ and if $u(\theta)$ is a function of $\theta$, then $u(\hat{\theta})$ is an MLE of $u(\theta)$.


__Example 1__

We know that the $pdf$ of exponential distribution ($X \sim EXP (\theta)$) is as follows:

$$\frac{1}{\theta} e^{-\frac{X}{\theta}}$$

Thus, its likelihood function is as follows

$$L(\theta)=\frac{1}{\theta^n}e^{-\frac{\sum X_i}{\theta}}$$
Thus, log-likelihood is as follows.

$$lnL(\theta)=-n ln(\theta)-\frac{\sum X_i}{\theta}$$
Thus,

$$\frac{d}{d\theta} lnL(\theta)=-n \frac{1}{\theta}+\frac{\sum X_i}{\theta^2}$$

Thus, we can get the $MLE$ for $\theta$ is $\hat{\theta}=\bar{x}$.

If we want to estimate $\tau(\theta)=P(X \geq 1)$:

$$\tau(\theta)=P(X\geq 1)=\int_1^{\infty} \frac{1}{\theta} e^{-\frac{X}{\theta}} dx=-\int_1^{\infty}  e^{-\frac{X}{\theta}} d(-\frac{x}{\theta})=-[e^{-\frac{X}{\theta}}]_1^{\infty}=-[0-e^{-\frac{1}{\theta}}]=e^{-\frac{1}{\theta}}$$

Thus, based on the invariance property, we know that the $MLE$ for $\tau(\theta)$ is as follows.

$$e^{-\frac{1}{\bar{x}}}$$

__Example 2: MLE vs. MME__

_P.296_

Assume a random sample from a two-parameter exponential distribution, $X_i \sim EXP(1)$. Thus, the $pdf$ is $e^{-(x-\eta)}$. Thus, the likelihood function is 

$$L(\eta)=e^{-\sum(x_i-\eta)}$$

Thus, the log likelihood,

$$lnL(\eta)=-\sum(x_i-\eta)=n\eta-n\bar{X}$$
Thus, we know that as $\eta$ increases, the log likelihood increases accordingly. Thus, we want to find the maximum $\eta$. Note that a two-parameter exponential distribution has the support of $x_i \geq \eta$. Thus, all $\eta$ are smaller than any $X_i$. Thus, we can get the ML estimator is the first order statistic 

$$\hat{\eta}=X_{1:n}$$

__Note that__ the estimators above is based on ML. What would be the answer if using MME?

We know that for a two-parameter exponential distribution, its mean is $\mu=1+\eta$. And, we know that based on MME, $\mu=\bar{X}$. Thus, we can get the following,

$$\hat{\eta}=\bar{X}-1$$

__Conclusion__

We can see that ML and MME have didferent estimators for the same $\eta$ for a two-parameter exponential distribution.


### Unbiased estimators

An estimator $T$ is said to be an unbiased estimator of $\tau(\theta)$ if 

$$E(T)=\tau(\theta)$$

for all $\theta \in \Omega$. Otherwise, we said that $T$ is biased  stimator of $\tau(\theta)$.


For instance, if we want to estimate a percentile, say the 95th percentile of $N(\mu,9)$. Note that the percentiles that we know are about standardized noraml (i.e., $N(0,1)$). Thus, we need to have some calculation to get the non-standard one.

$$\frac{X_{95 \; percentile}-\mu}{\sigma}=1.645$$
Thus, we can get 

$$X_{95 \; percentile}=1.645 \times \sigma +\mu$$

We know that $\bar{X}$ is the unbiased estimate for $\mu$. Thus, we can get

$$X_{95 \; percentile}=1.645 \times \sigma +\mu=4.94+\mu$$

We know that 

$$E(T)=E(\bar{X}+4.94)=\mu+4.94$$
Thus, $T=\bar{X}+4.94$ is the unbiased estimator of $\tau(\mu)=\mu+4.94$.


### Unbiased estimators vs. Invariance property of MLEs

__Do not apply "Invariance property of MLEs" to the Unbiased estimators.__

( __Note that:__ YOu can apply the Invariance property to "Unbiased estimators" when it is a linear combination. In that case, $E(a\theta+b)=aE(\theta)+b$. Thus, if you find a $T$ that is a unbiased estimator for $\theta$, it should be unbiased estimator for $a\theta+b$ as well. Thus, note that $\frac{1}{\theta}$ is not a linear combination of $\theta$, thus $\frac{1}{\theta}$ has a very different estimator, compared to $\theta$.)

_P.303_

For example, consider a random sample of size $n$ from an exponential distribution, $X_i \sim EXP(\theta)$, with parameter $\theta$. We know that, $\bar{X}$ is unbiased for $\theta$ (which is the mean of an exponential distribution).

If we want to estimate $\tau(\theta)=\frac{1}{\theta}$, then by the invariance property of MLE is $T_1=\frac{1}{\bar{X}}$.

However, $T_1$ is a biased estimators of $\frac{1}{\theta}$. Specifically,

We know that if $X \sim Gam(\theta,k)$, then $Y=\frac{2X}{\theta} \sim x^2(2k)$. We know that exponential distributions are a specicial case of Gamma distribution, $EXP(\theta)=Gam(\theta,1)$, thus we get the following,


$$Y=\frac{2n\bar{X}}{\theta}=\frac{2n}{\theta} \frac{\sum_{i=1}^n X_i}{n}=\frac{\sum_{i=1}^n 2X_i}{\theta}\sim\sum_{i=1}^n x^2(2\cdot1)=\sum_{i=1}^n x^2(2)=x^2(2n)$$

We further know that if $Y \sim x^2(v)$, $E(Y^r)=2^r\frac{\Gamma(v/2+r)}{\Gamma(v/2)}$. Thus, we know that,

$$E(Y^{-1})=2^{-1} \frac{\Gamma(2n/2-1)}{\Gamma(2n/2)}=\frac{1}{2}\cdot \frac{1}{n-1}$$

Thus,

$$E(Y^{-1})=E(\frac{\theta}{2n \bar{X}})=\frac{\theta}{2n}E(\frac{1}{\bar{X}})=\frac{1}{2}\cdot \frac{1}{n-1}$$

Thus.

$$E(\frac{1}{\bar{X}})=\frac{1}{n-1}\frac{n}{\theta}=\frac{n}{n-1}\frac{1}{\theta}$$


Thus,

$$E(\frac{n-1}{n} \frac{1}{\bar{X}})=\frac{1}{\theta}$$

__Conclusion:__

$\frac{1}{\bar{X}}$ is not the unbiased estimator for $\frac{1}{\theta}$. However, we can adjust it to $\frac{n-1}{n} \frac{1}{\bar{X}}$, which is the unbiased estimator for $\frac{1}{\theta}$. When the sample size is big enough, we know that $\frac{n-1}{n}$ will be close to 1. 


### UMVUE and Cramer-Rao lower bound

#### UMVUE

Let $X_1, X_2,...,X_n$ be a random sample of size $n$ from $f(x; \theta)$. An estimator $T^*$ of $\tau (\theta)$ is called a _uniformly minimum variance ubiased estimator_ (UMVUE) of $\tau(\theta)$ if 

1. $T^*$ is unbiased for $\tau{\theta}$

2. For any other unbiased estimator $T$ of $\tau(\theta)$, $Var(T^*) \leq Var(T)$ for all $\theta \in \Omega$.

#### Cramer-Rao lower bound 

If $T$ is an unbiased estimator of $\tau(\theta)$, then the Cramer-Rao lower bound (CRLB), based on a random sample, is

$$Var(T)=\frac{[\tau^{'}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}$$

__Example:__

Consider a random sample from an exponential distribution, $X_i \sim Exp(\theta)$. Because

$$f(x; \theta)=\frac{1}{\theta} e^{-\frac{x}{\theta}}$$

Thus,

$$ln(f(x;\theta))=-\frac{x}{\theta}-ln \theta$$

$$\frac{\partial}{\partial \theta} ln(f(X; \theta))=\frac{X}{\theta^2}-\frac{1}{\theta}=\frac{X-\theta}{\theta^2}$$

Thus,

$$E[\frac{\partial}{\partial \theta} ln(f(X; \theta))]^2 =E[\frac{(X-\theta)^2}{\theta^4}]=\frac{1}{\theta^4}E(X-\theta)^2=\frac{1}{\theta^4} Var(X)=\frac{\theta^2}{\theta^4}=\frac{1}{\theta^2}$$

Thus, the CRLB for $\tau(\theta)=\theta$ is as follows.

$$Var(T) \geq \frac{[\tau^{'}(\theta)]^2}{nE[\frac{\partial}{\partial \theta} ln f(X; \theta)]^2}=\frac{[\frac{\partial }{\partial \theta}\theta]^2}{n \frac{1}{\theta^2}}=\frac{1^2}{\frac{n}{\theta^2}}=\frac{\theta^2}{n}$$

We know that the variance for exponential distribution

$$Var(\bar{X})=\frac{Var(X)}{n}=\frac{\theta^2}{n}$$

We know that $\theta$ is the mean for exponential distribution. We also know that sample mean $\bar{X}$ is the unbiased estimator for population mean.

Thus,

$\bar{X}$ is the UMVUE of $\theta$.



### Best linear unbiased estimation (BLUE or MVLUE)

### Rao-Blackwell theorem


### Consistency, asymptotic unbiasedness

__Simple consistency:__


_P.311_

Let $\{T_n\}$ be a sequence of estimators of $\tau(\theta)$. These estimators are said to be __consistent__ estimators of $\tau(\theta)$ if for every $\varepsilon >0$,

$$lim_{n\rightarrow \infty}P[|T_n-\tau(\theta)|<\varepsilon]=1$$

for every $\theta \in \Omega$.


In the terminology of Chapter 7, $T_n$ converges stochastically to $\tau(\theta)$,$T_n \xrightarrow{P} \tau(\theta)$ as $n \rightarrow \infty$. Sometimes this also is referred to as __simple__ consistency.

One interpretation of consistency is that for large sample size the estimator tends to be more concentrated about $\tau(\theta)$, and by making $n$ sufficiently large $T_n$ can be made as concentrated as desired.

__MSE consistency:__

If $\{ T_n \}$ is a sequence of estimator of $\tau(\theta)$, then they are called __mean squared error consistent__ if 

$$lim_{n\rightarrow \infty} E[T_n - \tau(\theta)]^2=0$$

for every $\theta \in \Omega$.

__Asymptotic Unbiased__

A sequence $\{ T_n \}$ is said to be __asymptotically unbiased__ for $\tau(\theta)$ if

$$lim_{n\rightarrow \infty} E(T_n)=\tau(\theta)$$

for every $\theta \in \Omega$.

__Example:__


_P.313_

For a sample from $X_i \sim EXP(\theta)$, we know that $T_n=1/ \bar{X}$ is an MLE estimator for $\tau(\theta)=1/\theta$. Howwever, $T_n$ is not a unbiased estimator for $\tau(\theta)$, as 

$$E(T_n)=\frac{n}{n-1}\cdot \frac{1}{\theta}$$

We also know that 

$$Var(T_n)=\frac{(\frac{n}{n-1})^2}{(n-2)\theta^2}$$

Thus, while $T_n$ is not unbiased, it is asymptotically unbiased and MSE consistent for $\tau(\theta)=\frac{1}{\theta}$.

__Note that__, MES consistency is a stronger property than simple consistency. Thus, if a sequenc $\{T_n\}$ is mean squared error consistent, it is also simply consistent.




### Efficiency

_P.308_

The relative efficiency of an unbiased estimator $T$ of $\tau(\theta)$ to another unbiased estimator $T^*$ of $\tau(\theta)$ is given by 

$$re(T, T^*)=\frac{Var(T^*)}{Var(T)}$$

An unbiased estimator $T^*$ of $\tau(\theta)$ is said to be efficient if $re(T, T^*) \leq 1$ for all unbiased estimators $T$ of $\tau(\theta)$, and all $\theta \in \Omega$. The efficiency of an unbiased estimator $T$ of $\tau(\theta)$ is given by 

$$e(T)=re(T, T^*)$$

if $T^*$ is an efficient estimator of $\tau(\theta)$.


Thus, __an effecient estimator is just a UMVUE.__


__Example:__

_P.233 Example 7.2.2_

_P.303 Example 9.3.2_

_P.309 Example 9.3.7_

Let $X_1,X_2,...,X_n$ sample from $X_i \sim EXP(\theta)$. We know that 

$$X_{1:n}=EXP(\theta/n)$$

Thus,

$$E(nX_{1:n})=nE(X_{1:n})=n\frac{\theta}{n}=\theta$$
Thus,

$$nX_{1:n}$$
is the unbiased estimator for $\theta$.


Thus,

$$re(T,T^*)=\frac{Var(T^*)}{Var(T)}=\frac{Var(\bar{X})}{Var(nX_{1:n})}=\frac{\theta^2/n}{n^2Var(X_{1:n})}=\frac{\theta^2/n}{n^2(\theta/n)^2}=\frac{\theta^2/n}{\theta^2}=\frac{1}{n}$$
Thus, $T^*=\bar{X}$ is a more efficient estimator for $\theta$ than $T=nX_{1:n}$.


### Asymptotic efficiency

Let $\{T_n\}$ and $\{T_n^*\}$ be the two asymptotically unbiased sequences of estimators for $\tau(\theta)$. The __asymptotic relative efficiency__ of $T_n$relative to $T_n^*$ is given by

$$are(T_n,T_n^*)=lim_{n \rightarrow \infty} \frac{Var(T_n^*)}{T_n}$$
The sequence $\{T_n^*\}$ is said to asymptotically efficient if $are\{T_n, T_n^*\} \leq 1$ for all other asymptotically unbiased sequences $\{T_n\}$, and all $\theta \in \Omega$.


### Asymptotic properties of MLEs



## Sufficient and completeness

### Sufficiency and minimal sufficiency

### Neyman factorization theorem, minimal sufficiency of MLEs

### completeness, Lehmann-Scheffe completeness theorem

### Exponential class, complete sufficient statistics




